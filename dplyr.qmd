# Manipulating Data {#sec-dplyr}

```{r}
#| echo: false
#| output: false
library(tidyverse)
```

Once you have data loaded, it's time to begin what is variously called "wrangling", "munging", cleaning, or "pre-processing".  These various terms refer to the activities that are almost always necessary between the point at which you acquire your "raw" data and the point at which you are ready to begin your statistical analyses.  For example, you might wish to add together several different variables to get a new variable that is the sum. Or you might need to normalize a variable's measured values so that it has a mean of zero and standard deviation of zero.

## What is dplyr

In the tidyverse, the package responsible for such activities is called `dplyr`. This package contains a set of functions, each of which manipulates data in a particular way.  Each of these functions is very useful on its own. However, the real power of dplyr comes from the fact that you can repeatedly applying dplyr functions, each operating on the result from the previous. These "chains" of functions allow you to **compose** complex data manipulation operations that ultimately transform your data into whatever you need.  For this reason, dplyr is sometimes said to instantiate a "grammar" of data manipulation, a grammar which is made up of several "verbs" (functions).  These function include:

1. `filter()`: select a subset of rows from a data frame
1. `arrange()`: sort a data frame's rows
1. `mutate()` create new columns/variables based on existing columns/variables
1. `summarize()`: aggregate one or more columns/variables with a *summary statistic* (e.g., the mean)
1. `group_by()`: assign rows to *groups*, such that each group shares some values in common

Because dplyr is part of the tidyverse, these all work similarly.  Specifically, each dplyr function:

1. Takes a dataframe as its first argument
1. Takes additional arguments that often indicate which columns are to be operated on
1. Returns a modified dataframe

Let's see some of these characteristics in action.  To do so, we'll first load the data we downloaded back in @sec-dataactivities.  Run the following:

```{r}
#| output: false
nba <- read_csv("./data/nba_all_seasons.csv", na = c("Undrafted"))
```

Note that we have utilized the `na` argument here. This tells `read_csv` that we would like any values found in the file that match `"Undrafted"` to be treated as "missing".  R uses `NA` to represent missing values and so any `"Undrafted"` values will be converted in to `NA`.

## filter

The `filter()` function allows you to specify criteria about the values of a variable in your data set and then filters out only the rows that match that criteria.

The `team_abbreviation` for the New York Knicks is `"NYK"`. Run the following and look at the results in RStudio's spreadsheet viewer to ensure that only players on the Knicks heading to Portland are chosen here:

```{r}
filter(nba, team_abbreviation=="NYK")
```

If you prefer a more thorough inspection, you can open the data in RStudio's spreadsheet viewer:

```{r}
#| eval: false
View(filter(nba, team_abbreviation=="NYK"))
```

In either case, we are asking for a test of equality, keeping any rows where `team_abbreviation=="NYK"` is true and removing (filtering) any rows where `team_abbreviation=="NYK"` is false.  To do so, we use the double equal sign `==`, not a single equal sign `=`. In other words `filter(nba, team_abbreviation = "NYK")` will yield an error. This is a convention across many programming languages. If you are new to coding, you'll probably forget to use the double equal sign `==` a few times before you get the hang of it.

The equality operator is not the only operator available to us.  Others include:

- `>` corresponds to "greater than"
- `<` corresponds to "less than"
- `>=` corresponds to "greater than or equal to"
- `<=` corresponds to "less than or equal to"
- `!=` corresponds to "not equal to." The `!` is used in many programming languages to indicate "not."

Furthermore, you can combine multiple criteria using operators that make comparisons:

- `|` corresponds to "or"
- `&` corresponds to "and"

Let's look at an example that uses the \index{operators!not} `!` "not" operator to pick rows that *don't* match a criteria. As mentioned earlier, the `!` can be read as "not." Here we are filtering rows corresponding to players thare are **not** from the United States:

```{r}
#| eval: false
filter(nba, country!="USA")
```

Let's combine two different requirements using the `|`:

```{r}
#| eval: false
filter(nba, country=="Jamaica" | college=="Michigan")
```

This will select players that are **either** from Jamica **or** went to Michigan (or both).  We can also request the inverse of this.

```{r}
#| eval: false
filter(nba, !(country=="Jamaica" | college=="Michigan"))
```

This will select players that are **not** from Jamica **and** those players that did **not** go to Michigan.  Note the use of parentheses around ``(country=="Jamaica" | college=="Michigan")`.  If we used the `!`, but did not use the parentheses, we would only applying the "not" to the first test (country=="Jamaica"), not to the combination of `(country=="Jamaica" | college=="Michigan")`.  You can try it and compare the results:

```{r}
#| eval: false
filter(nba, !country=="Jamaica" | college=="Michigan")
```

This request, in contrast to the one above, is for players that are **not** from Jamaica **or** went to Michigan (or both).  So be very careful about the order of operations and use parentheses liberally.  It helps to minimize errors and makes your code more explicit and therefore more readable.

Let's see a slightly more complicated request that combines several different operators:

```{r}
#| eval: false
filter(nba, team_abbreviation=="NYK" & (country!="USA" | college=="Michigan") & age >= 25)
```

Here we have filtered the data to retain all rows corresponding to players from the NY Knicks, who are age 25 or older, and either went to Michigan or are not from the United States.  You can this this yourself and verify that the output matches these requirements (remember that you can use `View()` if you wish to inspect the entire result).

Let's request players that went to either Michigan, Duke, or Georgetown.

```{r}
#| eval: false
filter(nba, college=="Michigan" | college=="Duke" | college=="Georgetown")
```

This works, but as we progressively include more collegs, this will get unwieldy to write. A slightly shorter approach uses the `%in%` operator along with the `c()` function. Recall from Subsection @sec-vectors that the `c()` function "combines" or "concatenates" values into a single *vector* of values.

```{r}
#| eval: false
filter(nba, college %in% c("Michigan", "Duke", "Georgetown", "UCLA", "Kentucky"))
```

What this code is doing is filtering our for all flights where `college` is in the vector of airports `c("Michigan", "Duke", "Georgetown", "UCLA", "Kentucky")`. This approach produces results that are similar to a sequence of `|` operators, but takes much less energy to write (and read). The `%in%` operator is useful for looking for matches commonly in one vector/variable compared to another.

As a final note, we recommend that `filter()` should often be among the first tidyverse "verbs" you consider applying to your data. This cleans your dataset to only those rows you care about, or to put it another way, it narrows down the scope of your data to just the observations you care about.

## pipe

Before we go any further, let's first introduce a nifty tool that gets loaded with the `dplyr` package: the pipe operator `%>%`. The pipe operator allows us to combine multiple tidyverse operations into a single sequential *chain* of actions.

Let's start with a hypothetical example. Say you would like to perform a hypothetical sequence of operations on a hypothetical data frame `x` using hypothetical functions `f()`, `g()`, and `h()`:

1. Take `x` *then*
1. Use `x` as an input to a function `f()` *then*
1. Use the output of `f(x)` as an input to a function `g()` *then*
1. Use the output of `g(f(x))` as an input to a function `h()`

One way to achieve this sequence of operations is by using nesting parentheses as follows:

```{r}
#| eval: false
h(g(f(x)))
```

This code isn't so hard to read since we are applying only three functions: `f()`, then `g()`, then `h()` and each of the functions has a short name. Further, each of these functions also only has one argument. However, you can imagine that this will get progressively more difficult to read as the number of functions applied in your sequence increases and the arguments in each function grow more numerous. This is where the pipe operator `%>%` comes in handy. The pipe takes the output of one function and then "pipes" it to be the input of the next function. Furthermore, a helpful trick is to read `%>%` as "then" or "and then". For example, you can obtain the same output as the hypothetical sequence of functions as follows:

```{r}
#| eval: false
x %>% 
  f() %>% 
  g() %>% 
  h()
```

You would read this sequence as:

1. Take `x` *then*
1. Use this output as the input to the next function `f()` *then*
1. Use this output as the input to the next function `g()` *then*
1. Use this output as the input to the next function `h()`

Though both approaches achieve the same goal, the latter is much more human-readable because you can clearly read the sequence of operations line-by-line. But what are the hypothetical `x`, `f()`, `g()`, and `h()`?  Throughout this chapter on data wrangling:

1. The starting value, `x`, will be a data frame. For example, the `nba` data frame we have been exploring so far.
1. The sequence of functions, here `f()`, `g()`, and `h()`, will mostly be a sequence of any number of the data wrangling verb-named functions we listed above. For example, the `filter(college == "Michigan")` function and argument specified we previewed earlier.
1. The result will be the transformed/modified data frame that you want.

So instead of 

```{r}
#| eval: false
filter(nba, team_abbreviation=="NYK")
```

we can instead write:

```{r}
#| eval: false
nba %>%
    filter(team_abbreviation=="NYK")
```

The benefits of this may not be immediately obvious. But the ability to form a *chain* of data wrangling operations by combining tidyverse functions (verbs) into a single sequence will be utilized extensively and is made possible by the pipe operator `%>%`.


## summarize

The next common task when working with data frames is to compute *summary statistics*. Summary statistics are single numerical values that summarize a large number of values. Commonly known examples of summary statistics include the mean (i.e., the "average"), the median, and the mode. Other examples of summary statistics that might not immediately come to mind include the *sum*, the smallest value also called the *minimum*, the largest value also called the *maximum*, and the *standard deviation*.

Let's calculate two summary statistics of the `draft_round` variable in the `nba` data frame: the mean and standard deviation. To compute these summary statistics, we need the `mean()` and `sd()` *summary functions* in R. Summary functions in R take in many values and return a single value.  More precisely, we'll use the `mean()` and `sd()` summary functions within the `summarize()` function from the `dplyr` package. The `summarize()` function takes in a data frame and returns a data frame with only one row corresponding to the value of the summary statistic(s). 

We'll save the results in a new data frame called `summary_round` that will have two columns/variables: the `mean` and the `std_dev`:

```{r}
summary_round <- nba %>% 
  summarize(mean = mean(draft_round), std_dev = sd(draft_round))
summary_round
```

Why are the values returned `NA`? `NA` is how R encodes *missing values* where `NA` indicates "not available" or "not applicable." If a value for a particular row and a particular column does not exist, `NA` is stored instead. Values can be missing for many reasons. Perhaps the data was collected but someone forgot to enter it. Perhaps the data was not collected at all because it was too difficult to do so. Perhaps there was an erroneous value that someone entered that has been corrected to read as missing. You'll often encounter issues with missing values when working with real data.

Going back to our `summary_round` output, by default any time you try to calculate a summary statistic of a variable that has one or more `NA` missing values in R, `NA` is returned. If what you wish to calculate is the summary of all the valid, non-missing values, you can set the `na.rm` argument to `TRUE`, where `rm` is short for "remove". The code that follows computes the mean and standard deviation of all non-missing values of `age`:

```{r}
summary_round <- nba %>% 
  summarize(mean = mean(draft_round, na.rm = TRUE), 
            std_dev = sd(draft_round, na.rm = TRUE))
summary_round
```

Notice how the `na.rm = TRUE` are used as arguments to the `mean()` and `sd()` summary functions individually, and **not** to the `summarize()` function. 

However, one needs to be cautious whenever ignoring missing values as we've just done. We will consider the possible ramifications of blindly sweeping rows with missing values "under the rug". This is in fact why the `na.rm` argument to any summary statistic function in R is set to `FALSE` by default. In other words, R does not ignore rows with missing values by default. R is alerting you to the presence of missing data and you should be mindful of this missingness and any potential causes of this missingness throughout your analysis.

What are other summary functions we can use inside the `summarize()` verb to compute summary statistics? You can use any function in R that takes many values and returns just one. Here are just a few:

* `mean()`: the average
* `sd()`: the standard deviation, which is a measure of spread
* `min()` and `max()`: the minimum and maximum values, respectively
* `IQR()`: interquartile range
* `sum()`: the total amount when adding multiple numbers
* `n()`: a count of the number of rows in each group. This particular summary function will make more sense when `group_by()` is covered below.


::: {.callout-caution}
## Sometimes missingness is data

Imagine a public health researcher is studying the effect of smoking on lung cancer for a large number of patients who are observed every five years. She notices that a large number of patients have missing data points, particularly in later observations.  The researcher takes the approach outlined above, choosing to ignore these patients in her analysis. How might this be misleading?
:::

Let's see just a couple more examples of summaries.  Here we ask for the number of unique college that appear in the data set:

```{r}
nba %>%
    summarise(n_colleges = n_distinct(college))
```

Here we combine a filter operation with a summarize operation to calculate the average number of points scored by players in the 2000-2001 NBA season:

```{r}
nba %>%
    filter(season == "2000-01") %>%
    summarize(total_points = mean(pts))
```


## group_by

Say instead of a single mean number of points for the entire NBA, you would like a mean number of points separately for each college players attended. In other words, we would like to compute the mean number of points split by college. We can do this by "grouping" the `pts` measurements by the values of another variable, in this case by the values of the variable `college` . Run the following code:

```{r}
nba %>%
    group_by(college) %>% 
    summarize(mean = mean(pts),
            std_dev = sd(pts))
```

This code is similar to the previous code that created `summary_round`, but with an extra `group_by(month)` added before the `summarize()`. Grouping the `nba` dataset by `college` and then applying the `summarize()` functions yields a data frame that displays the mean and standard deviation temperature split by the different colleges that appear in the data set.

It is important to note that the `group_by()` function doesn't change data frames by itself. Rather it changes the *meta-data*, or data about the data, specifically the grouping structure. It is only after we apply the `summarize()` function that the data frame changes. 

For example, when we run this code:

```{r}
nba
```

Observe that the first line of the output reads ``# A tibble: `r nba %>% nrow()` x `r nba %>% ncol()` ``. This is an example of meta-data, in this case the number of observations/rows and variables/columns in `nba`. The actual data itself are the subsequent table of values. Now let's pipe the `nba` data frame into `group_by(college)`:

```{r}
nba %>% 
  group_by(college)
```

Observe that now there is additional meta-data: `# Groups:   college [345]` indicating that the grouping structure meta-data has been set based on the unique values of the variable `college`. On the other hand, observe that the data has not changed: it is still a table of `r nba %>% nrow()` $\times$ `r nba %>% ncol()` values.

Only by combining a `group_by()` with another data wrangling operation, in this case `summarize()`, will the data actually be transformed. 

```{r}
nba %>% 
  group_by(college) %>% 
  summarize(mean = mean(pts))
```

If you would like to **remove** this grouping structure meta-data, we can pipe the resulting data frame into the `ungroup()` function:

```{r}
nba %>% 
  group_by(college) %>% 
  ungroup()
```

Observe how the `# Groups:   college [345]` meta-data is no longer present. 

Let's now revisit the `n()` counting summary function we briefly introduced previously. Recall that the `n()` function counts rows. This is opposed to the `sum()` summary function that returns the sum of a numerical variable. For example, suppose we'd like to count how many NBA players went to each of the different colleges, we can run this:

```{r}
by_college <- nba %>% 
  group_by(college) %>% 
  summarize(count = n())
by_college
```

We see that there are 119 rows in which `college=="Alabama"` and 279 rows in which `college=="Arizona"`. Note there is a subtle but important difference between `sum()` and `n()`; while `sum()` returns the sum of a numerical variable (adding), `n()` returns a count of the number of rows/observations (counting).


### Grouping by more than one variable

You are not limited to grouping by one variable. Say you want to know the number of players coming from each college *for each NBA season*. We can also group by a second variable `season` using `group_by(college, season)`:

```{r}
by_college_annually <- nba %>% 
  group_by(college, season) %>% 
  summarize(count = n())
by_college_annually
```

Observe that there are now `r by_college_annually %>% nrow()` rows to `by_college_annually` because there are `r nba %>% n_distinct("college")` unique colleges. 

Why do we `group_by(college, season)` and not `group_by(college)` and then `group_by(season)`? Let's investigate:

```{r}
by_college_annually_incorrect <- nba %>% 
  group_by(college) %>% 
  group_by(season) %>% 
  summarize(count = n())
by_college_annually_incorrect
```

What happened here is that the second `group_by(season)` overwrote the grouping structure meta-data of the earlier `group_by(college)`, so that in the end we are only grouping by `season`. The lesson here is if you want to `group_by()` two or more variables, you should include all the variables at the same time in the same `group_by()` adding a comma between the variable names.


## mutate

Another common transformation of data is to create/compute new variables based on existing ones. For example, the heights in our `nba` data set are measured in units of centimeters.  But say you are more comfortable thinking of inches instead of centimeters. The formula to convert centimeters to inches is:

$$
\text{height in inches} = \frac{\text{height in centimeters.}}{2.54}
$$

We can apply this formula to the `player_height` variable using the `mutate()` function from the `dplyr` package, which takes existing variables and mutates them to create new ones. 

```{r}
nba <- nba %>% 
  mutate(player_height_in_inch = player_height / 2.54)
```

In this code, we `mutate()` the `nba` data frame by creating a new variable `player_height_in_inch = height / 2.54` and then *overwrite* the original `nba` data frame. Why did we overwrite the data frame `nba`, instead of assigning the result to a new data frame like `nba_new`? As a rough rule of thumb, as long as you are not losing original information that you might need later, it's acceptable practice to overwrite existing data frames with updated ones, as we did here. On the other hand, why did we not overwrite the variable `player_height`, but instead created a new variable called `player_height_in_inch`?  Because if we did this, we would have erased the original information contained in `player_height` of temperatures in centimeters that may still be valuable to us.

Let's now compute average heights in both inches and centimeters using the `group_by()` and `summarize()`:

```{r}
summary_college_height <- nba %>%
    group_by(college) %>% 
    summarize(mean_height_in_inch_in_cm = mean(player_height),
              mean_height_in_inch_in_in = mean(player_height_in_inch))
summary_college_height
```

Let's consider another example. We can imagine placing players on a continuum, with one end representing the "star" players and the other end representing "support" players.  We can quantify this dimension by by comparing a player's average points per game (`pts`) to his average assists per game (`ast`).  Let's calculate this new variable using the `mutate()` function:

```{r}
nba <- nba %>% 
  mutate(star_support = pts - ast)
```

Let's take a look at only the `pts`, `ast`, and the resulting `star_support` variables for a few rows in our updated `nba` data frame.

```{r}
#| echo: false
#| tbl-cap: "Five rows showing `star_support`"
nba %>% 
  select(pts, ast, star_support) %>% 
  slice(c(1,2,7,399,4))
```

The player in the third row scored an average of 17.2 points per game and made an average of 3.4 assists per game, so his `star_support` value is $17.2-3.5=13.8$. On the other hand, the player in the fourth row averaged 1.5 points and 2.9 assists, so its `star_support` value is $1.5 - 2.9 = -1.4$.

Let's look at some summary statistics of the `star_support` variable by considering multiple summary functions at once in the same `summarize()` code:

```{r}
star_support_summary <- nba %>% 
  summarize(
    min = min(star_support, na.rm = TRUE),
    q1 = quantile(star_support, 0.25, na.rm = TRUE),
    median = quantile(star_support, 0.5, na.rm = TRUE),
    q3 = quantile(star_support, 0.75, na.rm = TRUE),
    max = max(star_support, na.rm = TRUE),
    mean = mean(star_support, na.rm = TRUE),
    sd = sd(star_support, na.rm = TRUE),
    missing = sum(is.na(star_support))
  )
star_support_summary
```

We see for example that the average value is 6.36 minutes, whereas the largest is 30.9!  This summary contains quite a bit of information. However, it is often easier to visualize data to evaluate how values are distributed.  We'll take a look at that in @sec-ggplot2.

To close out our discussion on the `mutate()` function to create new variables, note that we can create multiple new variables at once in the same `mutate()` code. Furthermore, within the same `mutate()` code we can refer to new variables we just created. Consider following:

```{r}
#| eval: false
nba <- nba %>% 
  mutate(
    player_height_in_inch = player_height / 2.54,
    player_weight_in_lbs = player_weight / 2.2,
    bmi = 703 * (player_weight_in_lbs / (player_height_in_inch^2)),
  )
```


## arrange

One of the most commonly performed data wrangling tasks is to sort a data frame's rows in the alphanumeric order of one of the variables. The `dplyr` package's `arrange()` function allows us to sort/reorder a data frame's rows according to the values of the specified variable.

Suppose we are interested in determining the average number of points per game scored by NBA players from different colleges:

```{r}
mean_pts <- nba %>% 
  group_by(college) %>% 
  summarize(mean = mean(pts), 
            std_dev = sd(pts))
mean_pts
```

Observe that by default the rows of the resulting `mean_pts` data frame are sorted in alphabetical order of `college`. Say instead we would like to see the same data, but sorted from the highest to the lowest average points (`mean`) instead:

```{r}
mean_pts %>% 
  arrange(mean)
```

This is, however, the opposite of what we want. The rows are sorted with the least frequent destination airports displayed first. This is because `arrange()` always returns rows sorted in **ascending** order by default. To switch the ordering to be in "descending" order instead, we use the `desc()` function as so:

```{r}
mean_pts %>% 
  arrange(desc(mean))
```


## Activities {#sec-tibblesactivities}

If you haven't already, load load `nba` data set we downloaded back in @sec-dataactivities. And let's pass `c("Undrafted")` as the `na` argument:

```{r}
#| eval: false
nba <- read_csv("./data/nba_all_seasons.csv", na = c("Undrafted"))
```

1. Produce a data frame consisting exclusively of players under the age of 25.
1. Produce a data frame consisting exclusively of players who attended "Marquette".
1. Produce a data frame consisting exclusively of players who are **either** under the age of 25 **or** attended "Marquette".
1. Produce a data frame consisting exclusively of players under the age of 25 **and** attended "Marquette".
1. Determine the average height of all players.
1. Determine the median height of all players who attended "Connecticut".
1. For each college appearing in the data set, what is the average `pts` and `ast` of players who attended that college?
1. Try calculating these school-wise averages separately for each season.
1. Create a new column in the data set called `reb_diff` and calculate it as the difference between the offensive rebound percentage (`oreb_pct`) and the defensive rebound percentage (`dreb_pct`).
1. What is the standard deviation of `reb_diff`?
1. For each college appearing in the data set, what is the average `pts`of players who attended that college.  Which college has produced the highest average?  Which college has produced the lowest average?