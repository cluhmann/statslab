# Manipulating Data {#sec-dplyr}

```{r}
#| echo: false
#| output: false
library(tidyverse)
```

Once you have data loaded, it's time to begin what is variously called "wrangling", "munging", cleaning, or "pre-processing".  These various terms refer to the activities that are almost always necessary between the point at which you acquire your "raw" data and the point at which you are ready to begin your statistical analyses.  For example, you might wish to add together several different variables to get a new variable that is the sum. Or you might need to normalize a variable's measured values so that it has a mean of zero and standard deviation of zero.

## What is dplyr {#sec-whatisdplyr}

In the tidyverse, the package responsible for such activities is called `dplyr`. This package contains a set of functions, each of which manipulates data in a particular way.  Each of these functions is very useful on its own. However, the real power of dplyr comes from the fact that you can repeatedly apply dplyr functions, each operating on the result from the previous. These "chains" of functions allow you to **compose** complex data manipulation operations that ultimately transform your data into whatever you need.  For this reason, dplyr is sometimes said to instantiate a "grammar" of data manipulation, a grammar which is made up of several "verbs" (functions).  These function include:

1. `filter()`: select a subset of rows from a data frame
1. `arrange()`: sort a data frame's rows
1. `mutate()` create new columns/variables based on existing columns/variables
1. `summarize()`: aggregate one or more columns/variables with a *summary statistic* (e.g., the mean)
1. `group_by()`: assign rows to *groups*, such that each group shares some values in common

Because dplyr is part of the tidyverse, these all work similarly.  Specifically, each dplyr function:

1. Takes a dataframe as its first argument
1. Takes additional arguments that often indicate which columns are to be operated on
1. Returns a modified dataframe

Let's see some of these characteristics in action.  To do so, we'll first load the data we downloaded back in @sec-dataactivities.  Run the following:

```{r}
#| output: false
nba <- read_csv("./data/nba_all_seasons.csv", na = c("Undrafted"))
```

Note that we have utilized the `na` argument here. This tells `read_csv` that we would like any values found in the file that match `"Undrafted"` to be treated as "missing".  R uses `NA` to represent missing values and so any `"Undrafted"` values will be converted in to `NA`.

## filter

The `filter()` function allows you to specify criteria about the values of a variable in your data set and then filters out only the rows that match that criteria.

The `team_abbreviation` for the New York Knicks is `"NYK"`. Run the following:

```{r}
filter(nba, team_abbreviation == "NYK")
```

If you prefer a more thorough inspection, you can look at the results in RStudio's spreadsheet viewer to ensure that only players on the Knicks heading to Portland are chosen:

```{r}
#| eval: false
View(filter(nba, team_abbreviation == "NYK"))
```

In either case, we are asking for a test of equality, keeping any rows where `team_abbreviation=="NYK"` is true and removing (filtering) any rows where `team_abbreviation=="NYK"` is false.  To do so, we use the double equal sign `==` (not the single equal sign `=`). Trying `filter(nba, team_abbreviation = "NYK")` will yield an error. This is a convention across many programming languages. If you are new to coding, you'll probably forget to use the double equal sign `==` a few times before you get the hang of it.

The equality operator is not the only operator available to us.  Others include:

- `>` corresponds to "greater than"
- `<` corresponds to "less than"
- `>=` corresponds to "greater than or equal to"
- `<=` corresponds to "less than or equal to"
- `!=` corresponds to "not equal to." The `!` is used in many programming languages to indicate "not."

Furthermore, you can combine multiple criteria using operators that make comparisons:

- `|` corresponds to "or"
- `&` corresponds to "and"

Let's look at an example that uses the \index{operators!not} `!` "not" operator to pick rows that *don't* match a criteria. As mentioned earlier, the `!` can be read as "not." Here we are filtering rows corresponding to players thare are **not** from the United States:

```{r}
#| eval: false
filter(nba, country != "USA")
```

Let's combine two different requirements using the `|`:

```{r}
#| eval: false
filter(nba, country == "Jamaica" | college == "Michigan")
```

This will select players that are **either** from Jamica **or** went to Michigan (or both).  We can also request the inverse of this.

```{r}
#| eval: false
filter(nba,!(country == "Jamaica" | college == "Michigan"))
```

This will select players that are **not** from Jamaica **and** those players that did **not** go to Michigan.  Note the use of parentheses around ``(country=="Jamaica" | college=="Michigan")`.  If we used the `!`, but did not use the parentheses, we would only applying the "not" to the first test (country=="Jamaica"), not to the combination of `(country=="Jamaica" | college=="Michigan")`.  You can try it and compare the results:

```{r}
#| eval: false
filter(nba,!country == "Jamaica" | college == "Michigan")
```

This request, in contrast to the one above, is for players that are **not** from Jamaica **or** went to Michigan (or both).  So be very careful about the order of operations and use parentheses liberally.  It helps to minimize errors and makes your code more explicit and therefore more readable.

Let's see a slightly more complicated request that combines several different operators:

```{r}
#| eval: false
filter(nba,
       team_abbreviation == "NYK" &
           (country != "USA" | college == "Michigan") & age >= 25)
```

Here we have filtered the data to retain all rows corresponding to players from the NY Knicks, who are age 25 or older, and either went to Michigan or are not from the United States.  You can this this yourself and verify that the output matches these requirements (remember that you can use `View()` if you wish to inspect the entire result).

Let's request players that went to either Michigan, Duke, or Georgetown.

```{r}
#| eval: false
filter(nba,
       college == "Michigan" | college == "Duke" | college == "Georgetown")
```

This works, but as we progressively include more collegs, this will get unwieldy to write. A slightly shorter approach uses the `%in%` operator along with the `c()` function. Recall from Subsection @sec-vectors that the `c()` function "combines" or "concatenates" values into a single *vector* of values.

```{r}
#| eval: false
filter(nba,
       college %in% c("Michigan", "Duke", "Georgetown", "UCLA", "Kentucky"))
```

What this code is doing is filtering our for all flights where `college` is in the vector of colleges `c("Michigan", "Duke", "Georgetown", "UCLA", "Kentucky")`. This approach produces results that are similar to a sequence of `|` operators, but takes much less energy to write (and read). The `%in%` operator is useful for looking for matches commonly in one vector/variable compared to another.

As a final note, we recommend that `filter()` should often be among the first tidyverse "verbs" you consider applying to your data. This cleans your dataset to only those rows you care about, or to put it another way, it narrows down the scope of your data to just the observations you care about.

## pipe

Before we go any further, let's first introduce a nifty tool that gets loaded with the `dplyr` package: the pipe operator `%>%`. The pipe operator allows us to combine multiple tidyverse operations into a single sequential *chain* of actions.

Let's start with a hypothetical example. Say you would like to perform a hypothetical sequence of operations on a hypothetical data frame `x` using hypothetical functions `f()`, `g()`, and `h()`:

1. Take `x` *then*
1. Use `x` as an input to a function `f()` *then*
1. Use the output of `f(x)` as an input to a function `g()` *then*
1. Use the output of `g(f(x))` as an input to a function `h()`

One way to achieve this sequence of operations is by using nesting parentheses as follows:

```{r}
#| eval: false
h(g(f(x)))
```

This code isn't so hard to read since we are applying only three functions: `f()`, then `g()`, then `h()` and each of the functions has a short name. Further, each of these functions also only has one argument. However, you can imagine that this will get progressively more difficult to read as the number of functions applied in your sequence increases and the arguments in each function grow more numerous. This is where the pipe operator `%>%` comes in handy. The pipe takes the output of one function and then "pipes" it to be the input of the next function. Furthermore, a helpful trick is to read `%>%` as "then" or "and then". For example, you can obtain the same output as the hypothetical sequence of functions as follows:

```{r}
#| eval: false
x %>%
    f() %>%
    g() %>%
    h()
```

You would read this sequence as:

1. Take `x` *then*
1. Use this output as the input to the next function `f()` *then*
1. Use this output as the input to the next function `g()` *then*
1. Use this output as the input to the next function `h()`

Though both approaches achieve the same goal, the latter is much more human-readable because you can clearly read the sequence of operations line-by-line. But what are the hypothetical `x`, `f()`, `g()`, and `h()`?  Throughout this chapter on data wrangling:

1. The starting value, `x`, will be a data frame. For example, the `nba` data frame we have been exploring so far.
1. The sequence of functions, here `f()`, `g()`, and `h()`, will mostly be a sequence of any number of the data wrangling verb-named functions we listed above. For example, the `filter(college == "Michigan")` function and argument specified we previewed earlier.
1. The result will be the transformed/modified data frame that you want.

So instead of 

```{r}
#| eval: false
filter(nba, team_abbreviation == "NYK")
```

we can instead write:

```{r}
#| eval: false
nba %>%
    filter(team_abbreviation == "NYK")
```

The benefits of this may not be immediately obvious. But the ability to form a *chain* of data wrangling operations by combining tidyverse functions (verbs) into a single sequence will be utilized extensively and is made possible by the pipe operator `%>%`.


## summarize {#sec-summarize}

The next common task when working with data frames is to compute *summary statistics*. Summary statistics are single numerical values that summarize a large number of values. Commonly known examples of summary statistics include the mean (i.e., the "average"), the median, and the mode. Other examples of summary statistics that might not immediately come to mind include the *sum*, the smallest value also called the *minimum*, the largest value also called the *maximum*, and the *standard deviation*.

Let's calculate two summary statistics of the `draft_round` variable in the `nba` data frame: the mean and standard deviation. To compute these summary statistics, we need the `mean()` and `sd()` *summary functions* in R. Summary functions in R take in many values and return a single value.  More precisely, we'll use the `mean()` and `sd()` summary functions within the `summarize()` function from the `dplyr` package. The `summarize()` function takes in a data frame and returns a data frame with only one row corresponding to the value of the summary statistic(s). 

We'll save the results in a new data frame called `summary_round` that will have two columns/variables: the `mean` and the `std_dev`:

```{r}
summary_round <- nba %>%
    summarize(mean = mean(draft_round),
              std_dev = sd(draft_round))
summary_round
```

Why are the values returned `NA`? `NA` is how R encodes *missing values* where `NA` indicates "not available" or "not applicable." If a value for a particular row and a particular column does not exist, `NA` is stored instead. Values can be missing for many reasons. Perhaps the data was collected but someone forgot to enter it. Perhaps the data was not collected at all because it was too difficult to do so. Perhaps there was an erroneous value that someone entered that has been corrected to read as missing. You'll often encounter issues with missing values when working with real data.

Going back to our `summary_round` output, by default any time you try to calculate a summary statistic of a variable that has one or more `NA` missing values in R, `NA` is returned. If what you wish to calculate is the summary of all the valid, non-missing values, you can set the `na.rm` argument to `TRUE`, where `rm` is short for "remove". The code that follows computes the mean and standard deviation of all non-missing values of `age`:

```{r}
summary_round <- nba %>%
    summarize(
        mean = mean(draft_round, na.rm = TRUE),
        std_dev = sd(draft_round, na.rm = TRUE)
    )
summary_round
```

Notice how the `na.rm = TRUE` are used as arguments to the `mean()` and `sd()` summary functions individually, and **not** to the `summarize()` function. 

However, one needs to be cautious whenever ignoring missing values as we've just done. We will consider the possible ramifications of blindly sweeping rows with missing values "under the rug". This is in fact why the `na.rm` argument to any summary statistic function in R is set to `FALSE` by default. In other words, R does not ignore rows with missing values by default. R is alerting you to the presence of missing data and you should be mindful of this missingness and any potential causes of this missingness throughout your analysis.

What are other summary functions we can use inside the `summarize()` verb to compute summary statistics? You can use any function in R that takes many values and returns just one. Here are just a few:

* `mean()`: the average
* `sd()`: the standard deviation, which is a measure of spread
* `min()` and `max()`: the minimum and maximum values, respectively
* `IQR()`: interquartile range
* `sum()`: the total amount when adding multiple numbers
* `n()`: a count of the number of rows in each group. This particular summary function will make more sense when `group_by()` is covered below.


::: {.callout-caution}
## Sometimes missingness is data

Imagine a public health researcher is studying the effect of smoking on lung cancer for a large number of patients who are observed every five years. She notices that a large number of patients have missing data points, particularly in later observations.  The researcher takes the approach outlined above, choosing to ignore these patients in her analysis. How might this be misleading?
:::

Let's see just a couple more examples of summaries.  Here we ask for the number of unique college that appear in the data set:

```{r}
nba %>%
    summarise(n_colleges = n_distinct(college))
```

Here we combine a filter operation with a summarize operation to calculate the average number of points scored by players in the 2000-2001 NBA season:

```{r}
nba %>%
    filter(season == "2000-01") %>%
    summarize(total_points = mean(pts))
```


## select {#sec-select}

Sometimes, as in the `nba` data frame, you have many, many columns. Often many more than you need or want. The `select` function (verb) allows you to select columns from a data frame using a flexible, but concise mini-language.  Let's see a couple of simple examples.  Let's first select the `pts` column so that we can isolate it and inspect it without having to deal with the (many) other columns:

```{r}
nba %>%
    select(pts)
```

What if we wish to inspect small number of columns?

```{r}
nba %>%
    select(player_name, team_abbreviation, pts, ast)
```

We can also use select to **remove** individual columns:

```{r}
nba %>%
    select(-player_name)
```

But select is far more powerful than these simple examples suggest.  Let's again select several columns, but group the column names using the `c()` function we have seen before:


```{r}
nba %>%
    select(c(player_name, team_abbreviation, pts, ast))
```

Now, we can select all the remaining columns, simply by inverting the selection criteria:

```{r}
nba %>%
    select(!c(player_name, team_abbreviation, pts, ast))
```

Or we can select columns based on **pieces** of the column names, rather than the entire name.  Some of the columns in our `nba` data represent raw numbers (e.g., `ast`) and some represent percentages (e.g., `ast_pct`). Let's select any non-percentage columns:

```{r}
nba %>%
    select(!ends_with("_pct"))
```

::: {.callout-caution}
## Do not assume columns are ordered

Some dplyr functions allow users to select columns based on the order in which they are stored in the data frame. You can, for example, the first column in the `nba` data set is `player_name`. The fifth column in the `nba` data set is `player_weight`. Calling `select(nba, player_name:player_weight)` will select the first 5 columns of the data frame. I strongly discourage you from relying on this sort of functionality. It is extremely easy to reorder the columns and there is no loss of information when you do so. The columns are named, so reordering means that you can always find the information in that column again, regardless of the column order. For these reasons, someone (e.g., whoever is collecting your data) may arbitrarily reorder columns and think that doing so is harmless. If you avoid relying on the column order, it will be harmless!
:::


## group_by {#sec-groupby}

Say instead of a single mean number of points for the entire NBA, you would like a mean number of points separately for each college players attended. In other words, we would like to compute the mean number of points split by college. We can do this by "grouping" the `pts` measurements by the values of another variable, in this case by the values of the variable `college` . Run the following code:

```{r}
nba %>%
    group_by(college) %>%
    summarize(mean = mean(pts),
              std_dev = sd(pts))
```

This code is similar to the previous code that created `summary_round`, but with an extra `group_by(college)` added before the `summarize()`. Grouping the `nba` dataset by `college` and then applying the `summarize()` functions yields a data frame that displays the mean and standard deviation of `pts` split by the different colleges that appear in the data set.

It is important to note that the `group_by()` function doesn't change data frames by itself. Rather it changes the *meta-data*, or data about the data, specifically the grouping structure. It is only after we apply the `summarize()` function that the data frame changes. 

For example, when we run this code:

```{r}
nba
```

Observe that the first line of the output reads ``# A tibble: `r nba %>% nrow()` x `r nba %>% ncol()` ``. This is an example of meta-data, in this case the number of observations/rows and variables/columns in `nba`. The actual data itself are the subsequent table of values. Now let's pipe the `nba` data frame into `group_by(college)`:

```{r}
nba %>%
    group_by(college)
```

Observe that now there is additional meta-data: `# Groups: college [345]` indicating that the grouping structure meta-data has been set based on the unique values of the variable `college`. On the other hand, observe that the data has not changed: it is still a table of `r nba %>% nrow()` $\times$ `r nba %>% ncol()` values.

Only by combining a `group_by()` with another data wrangling operation, in this case `summarize()`, will the data actually be transformed. 

```{r}
nba %>%
    group_by(college) %>%
    summarize(mean = mean(pts))
```

If you would like to **remove** this grouping structure meta-data, we can pipe the resulting data frame into the `ungroup()` function:

```{r}
nba %>%
    group_by(college) %>%
    ungroup()
```

Observe how the `# Groups: college [345]` meta-data is no longer present. 

Let's now revisit the `n()` counting summary function we briefly introduced previously. Recall that the `n()` function counts rows. This is opposed to the `sum()` summary function that returns the sum of a numerical variable. For example, suppose we'd like to count how many rows in `nba` are associated with each of the different colleges, we can run this:

```{r}
by_college <- nba %>%
    group_by(college) %>%
    summarize(count = n())
by_college
```

We see that there are 119 rows in which `college=="Alabama"` and 279 rows in which `college=="Arizona"`. Note there is a subtle but important difference between `sum()` and `n()`; while `sum()` returns the sum of a numerical variable (adding), `n()` returns a count of the number of rows/observations (counting).


### Grouping by more than one variable

You are not limited to grouping by one variable. Say you want to know the number of players coming from each college *for each NBA season*. We can also group by a second variable `season` using `group_by(college, season)`:

```{r}
by_college_annually <- nba %>%
    group_by(college, season) %>%
    summarize(count = n())
by_college_annually
```

Observe that there are now `r by_college_annually %>% nrow()` rows to `by_college_annually` because there are `r nba %>% n_distinct("college")` unique colleges. 

Why do we `group_by(college, season)` and not `group_by(college)` and then `group_by(season)`? Let's investigate:

```{r}
by_college_annually_incorrect <- nba %>%
    group_by(college) %>%
    group_by(season) %>%
    summarize(count = n())
by_college_annually_incorrect
```

What happened here is that the second `group_by(season)` overwrote the grouping structure meta-data of the earlier `group_by(college)`, so that in the end we are only grouping by `season`. The lesson here is if you want to `group_by()` two or more variables, you should include all the variables at the same time in the same `group_by()` adding a comma between the variable names.


## mutate {#sec-mutate}

Another common transformation of data is to create/compute new variables based on existing ones. For example, the heights in our `nba` data set are measured in units of centimeters.  But say you are more comfortable thinking of inches instead of centimeters. The formula to convert centimeters to inches is:

$$
\text{height in inches} = \frac{\text{height in centimeters.}}{2.54}
$$

We can apply this formula to the `player_height` variable using the `mutate()` function from the `dplyr` package, which takes existing variables and mutates them to create new ones. 

```{r}
nba <- nba %>%
    mutate(player_height_in_inch = player_height / 2.54)
```

In this code, we `mutate()` the `nba` data frame by creating a new variable `player_height_in_inch = height / 2.54` and then *overwrite* the original `nba` data frame. Why did we overwrite the data frame `nba`, instead of assigning the result to a new data frame like `nba_new`? As a rough rule of thumb, as long as you are not losing original information that you might need later, it's acceptable practice to overwrite existing data frames with updated ones, as we did here. On the other hand, why did we not overwrite the variable `player_height`, but instead created a new variable called `player_height_in_inch`?  Because if we did this, we would have erased the original information contained in `player_height` of heights in centimeters that may still be valuable to us.

Let's consider another example. We can imagine placing players on a continuum, with one end representing the "star" players and the other end representing "support" players.  We can quantify this dimension by by comparing a player's average points per game (`pts`) to his average assists per game (`ast`).  Let's calculate this new variable using the `mutate()` function:

```{r}
nba <- nba %>%
    mutate(star_support = pts - ast)
```

Let's take a look at only the `pts`, `ast`, and the resulting `star_support` variables for a few rows in our updated `nba` data frame.

```{r}
#| echo: false
#| tbl-cap: "Five rows showing `star_support`"
nba %>%
    select(pts, ast, star_support) %>%
    slice(c(1, 2, 7, 399, 4))
```

The player in the third row scored an average of 17.2 points per game and made an average of 3.4 assists per game, so his `star_support` value is $17.2-3.5=13.8$. On the other hand, the player in the fourth row averaged 1.5 points and 2.9 assists, so its `star_support` value is $1.5 - 2.9 = -1.4$.

Let's look at some summary statistics of the `star_support` variable by considering multiple summary functions at once in the same `summarize()` code:

```{r}
star_support_summary <- nba %>%
    summarize(
        min = min(star_support, na.rm = TRUE),
        q1 = quantile(star_support, 0.25, na.rm = TRUE),
        median = quantile(star_support, 0.5, na.rm = TRUE),
        q3 = quantile(star_support, 0.75, na.rm = TRUE),
        max = max(star_support, na.rm = TRUE),
        mean = mean(star_support, na.rm = TRUE),
        sd = sd(star_support, na.rm = TRUE),
        missing = sum(is.na(star_support))
    )
star_support_summary
```

We see for example that the average value is 6.36 points, whereas the largest is 30.9!  This summary contains quite a bit of information. However, it is often easier to visualize data to evaluate how values are distributed.  We'll take a look at that in @sec-ggplot2.

To close out our discussion on the `mutate()` function to create new variables, note that we can create multiple new variables at once in the same `mutate()` code. Furthermore, within the same `mutate()` code we can refer to new variables we just created. Consider following:

```{r}
#| eval: false
nba <- nba %>%
    mutate(
        player_height_in_inch = player_height / 2.54,
        player_weight_in_lbs = player_weight / 2.2,
        bmi = 703 * (player_weight_in_lbs / (player_height_in_inch ^ 2)),
    )
```


## arrange {#sec-arrange}

One of the most commonly performed data wrangling tasks is to sort a data frame's rows in the alphanumeric order of one of the variables. The `dplyr` package's `arrange()` function allows us to sort/reorder a data frame's rows according to the values of the specified variable.

Suppose we are interested in determining the average number of points per game scored by NBA players from different colleges:

```{r}
mean_pts <- nba %>%
    group_by(college) %>%
    summarize(mean = mean(pts),
              std_dev = sd(pts))
mean_pts
```

Observe that by default the rows of the resulting `mean_pts` data frame are sorted in alphabetical order of `college`. Say instead we would like to see the same data, but sorted from the highest to the lowest average points (`mean`) instead:

```{r}
mean_pts %>%
    arrange(mean)
```

This is, however, the opposite of what we want. The rows are sorted with the players scoring the least points displayed first. This is because `arrange()` always returns rows sorted in **ascending** order by default. To switch the ordering to be in "descending" order instead, we use the `desc()` function as so:

```{r}
mean_pts %>%
    arrange(desc(mean))
```


## slice {#sec-slice}

Now that you can put the rows of a data frame into some sensible order, we will learn how to select rows based on their position within the data frame (contrast this with the earlier advice about ignoring the position/order of _columns_ within the data frame). Within dplyr, there is a very useful function called `slice()`. In some ways, this function is similar to `filter()`.  However, `filter()` produces subsets rows based on TRUE or FALSE values (e.g., whether values are greater than zero).  Instead, `slice()` takes rows numbers (i.e., integers) and returns the rows in the corresponding positions.  Let's look at a few examples.

```{r}
nba %>%
    slice(1, 2, 1048)
```

This code grabs rows 1, 2, and 1048 and returns a data frame including these three rows.  As usual, you can instead use ranges instead of writing things like `slice(1, 2, 3, 4, 5, 6, ...)`:


```{r}
# grab the first 10 rows
nba %>%
    slice(1:10)
```

We can also grab the last several rows:

```{r}
# probably don't do this
nba %>%
    slice(12000:nrow(nba))
```

Note, however, that this method uses the number of rows (`nrow()`) in the full data frame. As a result, this will probably only work if we are applying `slice()` directly to our `nba` data frame and not do not have any additional verbs in our chain before our call to `slice()`.  How can we grab the last `N` rows if we don't (necessarily) know how many rows are in the data frame we are applying `slice()` to? A more robust approach would be to use the closely related `slice_tail()` function.  This function is specifically designed to produce the **last** rows of a data frame:

```{r}
nba %>%
    slice_tail(n = 10)
```

Ok. This all seems fine. But unless you know ahead of time what data might live in particular rows, this may not seem all that useful.  The real power comes when you combine `slice()` with something like `arrange()`.  Let's see how these verbs work together to produce something a bit more useful.

```{r}
nba %>%
    arrange(desc(pts)) %>%
    slice(1:10) %>%
    select("player_name", "season", "pts")
```

This code first sorts the data frame by the `pts` column (highest to lowest because of our application of `desc()`) and the grabs the first 10 rows. Because we first use `arrange()`, we know that these 10 rows are associated with the 10 highest season scoring averages in our data set. We then `select()` a relatively small number of columns so we can view it easily.

We have seen `slice()` and `slice_tail()` in action. But there are more functions in the family.  Like `slice_tail()`, there is also `slice_head()`, which selected the first rows of a data frame.  There are also the functions `slice_min()` and `slice_max()`, which combine sorting and slicing operations.  So we can redo our code above to find the 10 highest season scoring averages in our data set using `slice_max()`:

```{r}
nba %>%
    slice_max(pts, n = 10) %>%
    select("player_name", "season", "pts")
```

All of these functions (except `slice()` itself) accept either an `n` argument (the number of rows to grab) or a `prop` argument (the proportion of all rows to grab).  As with all functions in dplyr and the tidyverse, the real power is ultimately realized by including these function into extended chains of operations. You can `group_by` and then `slice()`, you can `mutate()` and then `arrange()` and then `slice_tail()`, etc. etc.  There are an infinite possibilities.




## Activities {#sec-tibblesactivities}

If you haven't already, load the `nba` data set we downloaded back in @sec-dataactivities. And let's pass `c("Undrafted")` as the `na` argument:

```{r}
#| eval: false
nba <- read_csv("./data/nba_all_seasons.csv", na = c("Undrafted"))
```

1. Produce a data frame consisting exclusively of players under the age of 25.
1. Produce a data frame consisting exclusively of players who attended "Marquette".
1. Produce a data frame consisting exclusively of players who are **either** under the age of 25 **or** attended "Marquette".
1. Produce a data frame consisting exclusively of players under the age of 25 **and** attended "Marquette".
1. Determine the average height of all players.
1. Determine the minimum weight in the `nba` data set.
1. Determine the range of ages seen in the `nba` data set.
1. Determine the interquartile range of all `pts` seen in the `nba` data set.
1. Determine the median height of all players who attended "Connecticut".
1. Use `select` to produce a data frame that only includes columns containing player names and team abbreviations.
1. Use `select` to produce a data frame that includes **all** columns **except** those containing player names and team abbreviations.
1. Use `select` to produce a data frame that includes **all** columns whose column names include the string "ea" (hint: use `constains()`).
1. Use `select` to produce a data frame that includes **all** columns whose column names **does not** include an underscore (i.e., "_").
1. **Bonus:** Use `select` to produce a data frame that includes **all** columns whose column names contain more than 11 characters (hint: you may need to use some combination of `names()` and `nchar()`).
1. For each college appearing in the data set, what is the average (mean) of `pts` and `ast` among players who attended that college?
1. Calculate the mean of `player_weight` and the mean of `player_height` for each season in the data set.
1. Try calculating these per-school averages separately for each season (e.g. means for each combination of `college` and `season`).
1. Create a new column in the data set called `reb_diff` and calculate it as the difference between the offensive rebound percentage (`oreb_pct`) and the defensive rebound percentage (`dreb_pct`).
1. Calculate the mean and standard deviation of `reb_diff`.
1. Find the 10 largest values of `reb_diff`.
1. Create a new column in the data set called `pts_over_mean` and calculate it as the difference between that row's `pts` value and the overall mean of `pts` in the entire data set.
1. Generate a data frame with 3 columns: the player's name, season, and `pts_over_mean` so that you can quickly view your new column and who and when it is associated with.
1. Use `filter()` to find those rows corresponding to players who went to "UCLA", and then use arrange() to order the data by `pts`.
1. For each college appearing in the data set, what is the average `pts`of players who attended that college.  Which college has produced the highest average?  Which college has produced the lowest average?
1. Use `filter()` to select those data associated with the New York Knicks (abbreviated `NYK`), and then use `select()` to remove the column containing the team abbreviation column.  Next, remove all undrafted players (e.g., those who do not have a valid `draft_year`). Of these players, find the 6 players with the highest `pts`. If there are duplicates (i.e., any player appearing multiple times in the top 6), find the 6 _unique_ players with the highest `pts` (so that there are 6 players but no duplicates).
1. Group the data by `college` and `team_abbreviation`. Now calculate the sum of `pts` for each of these combinations. Now add up all these means. You should have a single number (the sum of some large number of means). Does the chain of operations you initially try work as expected? Does the documentation on dplyr's `ungroup()` function help?