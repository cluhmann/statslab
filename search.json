[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "statslab",
    "section": "",
    "text": "Introduction\nContent covering statistics, data analysis, and computer programming. Sections are listed on the left."
  },
  {
    "objectID": "rlang.html#r-the-language",
    "href": "rlang.html#r-the-language",
    "title": "1  R & RStudio",
    "section": "1.1 R: The Language",
    "text": "1.1 R: The Language\nReleased in 1993, R is an opensource successor to S, a commercially available statistical programming language developed at Bell Labs in the 1970s. The history is not particularly relevant. However, understanding that R was originally developed with the explicit goal of teaching introductory statistics (a bit like Matlab) may help to understand some of the design choices made by the developers of R.\nThere are several attributes that make R useful for teaching statistics:\n\nR is interpreted\nR is weakly typed\nR’s the focus on data analysis is built into its native data types\nR has strong graphical abilities\n\nDon’t worry if all of these benefits are meaningless to you at this point. We’ll get into the details soon enough. The short version is that the first two bullet points are features that make programmers’ lives easier/more convenient and last two mean that R is ready-made to do data analysis.\nThere are also a variety of downsides to R, but we will try to sidestep these as much as we can. One way we will do this is by exclusively using packages from the tidyverse. Once you are familiar with one way of working in R, you can explore the many, many alternatives."
  },
  {
    "objectID": "rlang.html#rstudio",
    "href": "rlang.html#rstudio",
    "title": "1  R & RStudio",
    "section": "1.2 RStudio",
    "text": "1.2 RStudio\nThe interface between a programmer (or data analyst) and a programming language is often an integrate development environment (IDE), which is just a fancy term for a piece of software that collects a bunch of inter-related tools often useful when programming. We will be using RStudio as our IDE. As the name suggests, RStudio was designed as an R-first IDE, making things easy for those new to R and/or programming.\nFirst time users often confuse R and RStudio. At its simplest, R is like a car’s engine while RStudio is like a car’s dashboard as illustrated in Figure Figure 1.1. More precisely, R is the programming language that performs computations and RStudio is how users interact with R.\n\n\n\nFigure 1.1: Analogy of difference between R and RStudio."
  },
  {
    "objectID": "rlang.html#sec-install",
    "href": "rlang.html#sec-install",
    "title": "1  R & RStudio",
    "section": "1.3 Installation",
    "text": "1.3 Installation\nInstallation involves two major steps. The first step is to install R itself. You can get the most recent version from CRAN (the comprehensive R archive network): https://cloud.r-project.org. There are ready-made installers available for Linux, macOS, and Windows.\nThe second step is to install RStudio. You can download it from Posit: https://posit.co/download/rstudio-desktop/. Again, there are versions available for all major platforms."
  },
  {
    "objectID": "rlang.html#using-r-via-rstudio",
    "href": "rlang.html#using-r-via-rstudio",
    "title": "1  R & RStudio",
    "section": "1.4 Using R via RStudio",
    "text": "1.4 Using R via RStudio\nRecall our car analogy from earlier. Much as we don’t drive a car by interacting directly with the engine but rather by interacting with elements on the car’s dashboard, we won’t be using R directly but rather we will use RStudio’s interface. After you install R and RStudio on your computer, you’ll have two new programs or applications you can open. We’ll always work in RStudio and not in the R application. Figure Figure 1.2 shows what icon you should be clicking on your computer.\n\n\n\nFigure 1.2: Icons of R versus RStudio on your computer.\n\n\nAfter you open RStudio, you should see something similar to Figure Figure 1.3. (Note that slight differences might exist if the RStudio interface is updated after 2019 to not be this by default.)\n\n\n\nFigure 1.3: RStudio interface to R.\n\n\nNote the three panes dividing the screen: the console pane, the files pane, and the environment pane. The console pane is where you enter R commands and R returns the corresponding results (and/or error messages if you make a mistake). We’ll dig into the other panes a bit more later."
  },
  {
    "objectID": "rlang.html#sec-packages",
    "href": "rlang.html#sec-packages",
    "title": "1  R & RStudio",
    "section": "1.5 Other Packages",
    "text": "1.5 Other Packages\nOne of the main strengths of R (arguably the most important strength) is the availability of many, many packages (or libraries) that extend the functionality of R in all sort of different ways. These libraries are written by a worldwide community of R users and can be downloaded for free. We will rely on a variety of these packages, so we take a moment now to discuss how packages work in R and RStudio.\nA good analogy for R packages is they are like apps you can download onto your mobile phone. So R is like a new mobile phone: while it has a certain amount of features when you use it for the first time, it doesn’t have everything. R packages are like the apps you can download onto your phone from Apple’s App Store or Android’s Google Play.\nLet’s continue this analogy by considering the Instagram app for editing and sharing pictures. Say you have purchased a new phone and you would like to share a photo you have just taken with friends on Instagram. You need to:\n\nInstall the app: Since your phone is new and does not include the Instagram app, you need to download the app from either the App Store or Google Play. You do this once and you’re set for the time being. You might need to do this again in the future when there is an update to the app.\nOpen the app: After you’ve installed Instagram, you need to open it.\n\nOnce Instagram is open on your phone, you can then proceed to share your photo with your friends and family. The process is very similar for using an R package. You need to:\n\nInstall the package: This is like installing an app on your phone. Most packages are not installed by default when you install R and RStudio. Thus, if you want to use a package, you need to install it first. Once you’ve installed a package, you likely won’t install it again unless you want to update it to a newer version.\n“Load” the package: “Loading” a package is like opening an app on your phone. Packages are not “loaded” by default when you start RStudio on your computer; you need to “load” each package you want to use every time you start RStudio.\n\nLet’s take a look at how these two steps come together to install and load the ggplot2 package for data visualization.\n\n1.5.1 Package Installation\nThere are two ways to install an R package: an easy way and a more advanced way. Let’s install the ggplot2 package the easy way first as shown in Figure Figure 1.4. In the Files pane of RStudio:\n\nClick on the “Packages” tab.\nClick on “Install” next to Update.\nType the name of the package under “Packages (separate multiple with space or comma):” In this case, type ggplot2.\nClick “Install.”\n\n\n\n\nFigure 1.4: Installing packages in R the easy way.\n\n\nAn alternative but slightly less convenient way to install a package is by typing install.packages(\"ggplot2\") in the console pane of RStudio and pressing Return/Enter on your keyboard. Note you must include the quotation marks around the name of the package.\nMuch like an app on your phone, you only have to install a package once. However, if you want to update a previously installed package to a newer version, you need to reinstall it by repeating the earlier steps.\n\n\n1.5.2 Package Loading\nRecall that after you’ve installed a package, you need to “load it.” In other words, you need to “open it.” We do this by using the library() command.\nFor example, to load the ggplot2 package, run the following code in the console pane. What do we mean by “run the following code”? Either type or copy-and-paste the following code into the console pane and then hit the Enter key.\n\nlibrary(ggplot2)\n\nIf after running the earlier code, a blinking cursor returns next to the &gt; “prompt” sign, it means you were successful and the ggplot2 package is now loaded and ready to use. If, however, you get a red “error message” that reads ...\nError in library(ggplot2) : there is no package called ‘ggplot2’\n... it means that you didn’t successfully install it. If you get this error message, go back to Subsection Section 1.5.1 on R package installation and make sure to install the ggplot2 package before proceeding.\n\n\n1.5.3 Package Use\nOne very common mistake new R users make when wanting to use particular packages is they forget to “load” them first by using the library() command we just saw. Remember: you have to load each package you want to use every time you start RStudio. If you don’t first “load” a package, but attempt to use one of its features, you’ll see an error message similar to:\nError: could not find function\nThis is a different error message than the one you just saw on a package not having been installed yet. R is telling you that you are trying to use a function in a package that has not yet been “loaded.” R doesn’t know where to find the function you are using. Almost all new users forget to do this when starting out, and it is a little annoying to get used to doing it. However, you’ll remember with practice and after some time it will become second nature for you."
  },
  {
    "objectID": "rlang.html#quitting-r",
    "href": "rlang.html#quitting-r",
    "title": "1  R & RStudio",
    "section": "1.6 Quitting R",
    "text": "1.6 Quitting R\n\n\n\nFigure 1.5: The dialog box that shows up when you try to close RStudio.\n\n\nThere’s one last thing I should cover in this chapter: how to quit R. You can just shut down the application in the normal way (e.g., File-&gt;Quit Session). However, R also has a function, q(), that you can use to quit, which can be handy. Regardless of what method you use to quit R, when you do so for the first time R will probably ask you if you want to save the “workspace image”. We’ll talk a lot more about loading and saving data in Chapter 3, but I figured we’d better quickly cover this now otherwise you’re going to get annoyed when you close R at the end of the chapter. If you’re using RStudio, you’ll see a dialog box that looks like the one shown in Figure @ref(fig:quitR). If you’re using a text based interface you’ll see this:\n\nq()\n\n## Save workspace image? [y/n/c]: \n\nThe y/n/c part here is short for “yes / no / cancel”. Type y if you want to save, n if you don’t, and c if you’ve changed your mind and you don’t want to quit after all.\nWhat does this actually mean? What’s going on is that R wants to know if you want to save all those variables that you’ve been creating, so that you can use them later. This sounds like a great idea, so it’s really tempting to type y or click the “Save” button. To be honest though, I very rarely do this, and it kind of annoys me a little bit.. what R is really asking is if you want it to store these variables in a “default” data file, which it will automatically reload for you next time you open R. And quite frankly, if I’d wanted to save the variables, then I’d have already saved them before trying to quit. Not only that, I’d have saved them to a location of my choice, so that I can find it again later. So I personally never bother with this.\nIn fact, every time I install R on a new machine one of the first things I do is change the settings so that it never asks me again. You can do this in RStudio really easily: use the menu system to find the RStudio option; the dialog box that comes up will give you an option to tell R never to whine about this again (see Figure 1.5). On a Mac, you can open this window by going to the “RStudio” menu and selecting “Preferences”. On a Windows machine you go to the “Tools” menu and select “Global Options”. Under the “General” tab you’ll see an option that reads “Save workspace to .Rdata on exit”. By default this is set to “ask”. If you want R to stop asking, change it to “never”."
  },
  {
    "objectID": "rlang.html#sec-rlangactivities",
    "href": "rlang.html#sec-rlangactivities",
    "title": "1  R & RStudio",
    "section": "1.7 Activities",
    "text": "1.7 Activities\n\nInstall both R and RStudio using the instructions found in Section 1.3\nOpen RStudio\nSettings\n\nGo to “Tools” menu and select “Global Options…”. On the “General” tab, uncheck the box that says “Restore .RData into workspace at startup”.\n\nSet the “Save workspace to .Rdata on exit” option to “never”.\nInstall the tidyverse package using the instructions found in Section 1.5.1\nVerify installation of the tidyverse package by loading the package using the instructions found in Section 1.5.2\nCreate a directory/folder somewhere convenient on your hard drive and name it “statslab”\nSet the “working directory” for R by going to “Session” menu and selecting “Set Working Directory” and then “Choose Directory…”. Select the “statslab” directory/folder you just created.\nAny time you wish to begin (or return to) work for this course, perform this step so that all of your file (data, scripts, etc.) are saved in a central location. Future activities will provide further instruction assuming that you a) have created this directory/folder and b) have set it to be your working directory in RStudio."
  },
  {
    "objectID": "programming.html#commands-at-the-console",
    "href": "programming.html#commands-at-the-console",
    "title": "2  R as a Calculator",
    "section": "2.1 Commands at the console",
    "text": "2.1 Commands at the console\nOne of the easiest things you can do with R is use it as a simple calculator, so it’s a good place to start. For instance, try typing 10 + 20, and hitting enter. The simple act of typing it rather than “just reading” makes a big difference. It makes the concepts more concrete, and it ties the abstract ideas (programming and statistics) to the actual context in which you need to use them. Statistics is something you do, not just something you read about in a textbook.] When you do this, you’ve entered a command, and R will “execute” that command. What you see on screen now will be this:\n&gt; 10 + 20\n[1] 30\nShould be much surprise here. But there’s a few things worth talking about, even with such a simple example. First, it’s important that you understand how to read the code example. In this example, what was typed into the RStudio console was the 10 + 20 part. The &gt; symbol as not typed. That’s just the command prompt and isn’t part of the actual command. The [1] 30 part was also not typed into the console. That’s what R printed out in response to the 10 + 20 code.\nSecond, it’s important to understand how the output is formatted. Obviously, the correct answer to the sum 10 + 20 is 30, and not surprisingly R has printed that out as part of its output. But it’s also printed out this [1] part, which probably doesn’t make a lot of sense to you right now. You’re going to see that a lot. I’ll talk about what this means in a bit more detail later on, but for now you can think of [1] 30 as if R were saying “the answer to the 1st question you asked is 30”. That’s not quite accurate, but it’s close enough for now. And in any case it’s not really very interesting at the moment: we only asked R to calculate one thing, so obviously there’s only one answer. Later on this will change, and the [1] part will start to make a bit more sense. For now, I just don’t want you to get confused or concerned by it.\n\n2.1.1 An important digression about formatting\nNow that I’ve taught you these rules I’m going to change them pretty much immediately. That is because I want you to be able to copy code from the book directly into R if if you want to test things or conduct your own analyses. However, if you copy this kind of code (that shows the command prompt and the results) directly into R you will get an error:\n\n&gt; 10 + 20\n\nError: &lt;text&gt;:1:1: unexpected '&gt;'\n1: &gt;\n    ^\n\n\nSo instead, I’m going to provide code in a slightly different format so that it looks like this…\n\n10 + 20\n\n[1] 30\n\n\nThere are two main differences.\n\nIn your console, the “&gt;” is the prompt and you type your code after (to the right of) this prompt.\nWe’ll often show the output of a bit of code, but the output will be displayed after the block of code itself.\n\nFor your purposes, this also means that you can easily copy code from any of these code blocks and paste it into your RStudio console in order to execute.\n\n\n2.1.2 Be very careful to avoid typos\nBefore we go on to talk about other types of calculations that we can do with R, there’s a few other things I want to point out. The first thing is that, though R is good software, it’s still software. R, like any programming language, is pretty stupid and because it’s stupid it can’t handle typos. It takes it on faith that you meant to type exactly what you actually typed. For example, suppose that you forgot to hit the shift key when trying to type +, and as a result your command ended up being 10 = 20 rather than 10 + 20. Here’s what happens:\n\n10 = 20\n\nError in 10 = 20: invalid (do_set) left-hand side to assignment\n\n\nWhat’s happened here is that R has attempted to interpret 10 = 20 as a command, and spits out an error message because the command doesn’t make any sense to it. When a human looks at this, and then looks down at his or her keyboard and sees that + and = are on the same key, it’s pretty obvious that the command was a typo. But R doesn’t know this, so it gets upset. And, if you look at it from its perspective, this makes sense. All that R “knows” is that 10 is a legitimate number, 20 is a legitimate number, and = is a legitimate part of the language too. In other words, from its perspective this really does look like the user meant to type 10 = 20, since all the individual parts of that statement are legitimate and it’s too stupid to realize that this is probably a typo. Therefore, R takes it on faith that this is exactly what you meant… it only “discovers” that the command is nonsense when it tries to follow your instructions, typo and all. And then it complains by spitting out an error.\nEven more subtle is the fact that some typos won’t produce errors at all, because they happen to correspond to “well-formed” R commands. For instance, suppose that not only did I forget to hit the shift key when trying to type 10 + 20, I also managed to press the key next to one I meant do. The resulting typo would produce the command 10 - 20. Clearly, R has no way of knowing that you meant to add 20 to 10, not subtract 20 from 10, so what happens this time is this:\n\n10 - 20\n\n[1] -10\n\n\nIn this case, R produces the right answer, but to the the wrong question.\nTo some extent, I’m stating the obvious here, but it’s important. The people who wrote R are smart. You, the user, are smart. But R is a programming language and programming languages are a way to tell computers what to do and computers are dumb. And because they are dumb, they are mindlessly obedient. R does exactly what you tell it to do. R will not try and second-guess what you “actually meant”; there is no “autocorrect”. This is for good reason. When doing advanced stuff – and even the simplest of statistics is pretty advanced in a lot of ways – it’s risky to let a mindless automaton like R try to overrule the human user. So it’s your responsibility to be careful. Always make sure you type exactly what you mean. When dealing with computers, it’s not enough to type “approximately” the right thing. In general, you absolutely must be precise in what you tell R to do … like all machines it is too stupid to be anything other than absurdly literal in its interpretation.\n\n\n2.1.3 R is (a bit) flexible with spacing\nOf course, now that I’ve been so uptight about the importance of always being precise, I should point out that there are some exceptions. Or, more accurately, there are some situations in which R does show a bit more flexibility than my previous description suggests. The first thing R is smart enough to do is ignore redundant spacing. What I mean by this is that, when I typed 10 + 20 before, I could equally have done this\n\n10    + 20\n\n[1] 30\n\n\nor this\n\n10+20\n\n[1] 30"
  },
  {
    "objectID": "programming.html#simple-calculations",
    "href": "programming.html#simple-calculations",
    "title": "2  R as a Calculator",
    "section": "2.2 Simple calculations",
    "text": "2.2 Simple calculations\nOkay, now that we’ve discussed some of the tedious details associated with typing R commands, let’s get back to learning how to use the most powerful piece of statistical software in the world as a $2 calculator. So far, all we know how to do is addition. Clearly, a calculator that only did addition would be a bit stupid, so we’ll discuss other simple calculations you can perform using R. But first, some more terminology. Addition is an example of an “operation” that you can perform (specifically, an arithmetic operation), and the operator that performs it is +. To people with a programming or mathematics background, this terminology probably feels pretty natural, but to other people it might feel like I’m trying to make something very simple (addition) sound more complicated than it is (by calling it an arithmetic operation). To some extent, that’s true: if addition was the only operation that we were interested in, it’d be a bit silly to introduce all this extra terminology. However, as we go along, we’ll start using more and more different kinds of operations, so it’s probably a good idea to get the language straight now, while we’re still talking about very familiar concepts like addition!\n\n2.2.1 Adding, subtracting, multiplying and dividing\nSo, now that we have the terminology, let’s learn how to perform some arithmetic operations in R. To that end, Table 2.1 lists the operators that correspond to the basic arithmetic we learned in primary school: addition, subtraction, multiplication and division.\n\n\nTable 2.1: ?(caption)\n\n\n\n\n(a) Basic arithmetic operations in R. These five operators are used very frequently throughout the text, so it’s important to be familiar with them at the outset.\n\n\noperation\noperator\nexample input\nexample output\n\n\n\n\naddition\n+\n10 + 2\n12\n\n\nsubtraction\n-\n9 - 3\n6\n\n\nmultiplication\n*\n5 * 5\n25\n\n\ndivision\n/\n10 / 3\n3\n\n\npower\n^\n5 ^ 2\n25\n\n\n\n\n\n\nAs you can see, R uses fairly standard symbols to denote each of the different operations you might want to perform: addition is done using the + operator, subtraction is performed by the - operator, and so on. So if I wanted to find out what 57 times 61 is (and who wouldn’t?), I can use R instead of a calculator, like so:\n\n57 * 61\n\n[1] 3477\n\n\nSo that’s handy.\n\n\n2.2.2 Taking powers\nThe first four operations listed in (tab-arithmetic1?) are things we all learned at a young age, but they aren’t the only arithmetic operations built into R. There are three other arithmetic operations that I should probably mention: taking powers, doing integer division, and calculating a modulus. Of the three, the most important is probably taking powers.\nFor those of you who can still remember your high school math, this should be familiar. And if not, it’s not complicated. As I’m sure everyone will probably remember the moment they read this, the act of multiplying a number \\(x\\) by itself \\(n\\) times is called “raising \\(x\\) to the \\(n\\)-th power”. Mathematically, this is written as \\(x^n\\). Some values of \\(n\\) have special names: in particular \\(x^2\\) is called \\(x\\)-squared, and \\(x^3\\) is called \\(x\\)-cubed. So, the 4th power of 5 is calculated like this: \\[\n5^4 = 5 \\times 5 \\times 5 \\times 5\n\\]\nOne way that we could calculate \\(5^4\\) in R would be to type in the complete multiplication as it is shown in the equation above. That is, we could do this\n\n5 * 5 * 5 * 5\n\n[1] 625\n\n\nbut it does seem a bit tedious. It would be very annoying indeed if you wanted to calculate \\(5^{15}\\), since the command would end up being quite long. Therefore, to make our lives easier, we use the power operator instead. When we do that, our command to calculate \\(5^4\\) goes like this:\n\n5 ^ 4\n\n[1] 625\n\n\nMuch easier.\n\n\n2.2.3 Doing calculations in the right order\nOkay. At this point, you know how to take one of the most powerful pieces of statistical software in the world, and use it as a $2 calculator. And as a bonus, you’ve learned a few very basic programming concepts. That’s not nothing (you could argue that you’ve just saved yourself $2) but on the other hand, it’s not very much either. In order to use R more effectively, we need to introduce more programming concepts.\nIn most situations where you would want to use a calculator, you might want to do multiple calculations. R lets you do this, just by typing in longer commands. In fact, we’ve already seen an example of this earlier, when I typed in 5 * 5 * 5 * 5. However, let’s try a slightly different example:\n\n1 + 2 * 4\n\n[1] 9\n\n\nClearly, this isn’t a problem for R either. However, it’s worth stopping for a second, and thinking about what R just did. Clearly, since it gave us an answer of 9 it must have multiplied 2 * 4 (to get an interim answer of 8) and then added 1 to that. But, suppose it had decided to just go from left to right: if R had decided instead to add 1+2 (to get an interim answer of 3) and then multiplied by 4, it would have come up with an answer of 12.\nTo answer this, you need to know the order of operations that R uses. If you remember back to your high school maths classes, it’s actually the same order that you got taught when you were at school: the “BEDMAS” order. That is, first calculate things inside Brackets (), then calculate Exponents ^, then Division / and Multiplication *, then Addition + and Subtraction -. So, to continue the example above, if we want to force R to calculate the 1+2 part before the multiplication, all we would have to do is enclose it in brackets:\n\n(1 + 2) * 4 \n\n[1] 12\n\n\nThis is a fairly useful thing to be able to do. The only other thing I should point out about order of operations is what to expect when you have two operations that have the same priority: that is, how does R resolve ties? For instance, multiplication and division are actually the same priority, but what should we expect when we give R a problem like 4 / 2 * 3 to solve? If it evaluates the multiplication first and then the division, it would calculate a value of two-thirds. But if it evaluates the division first it calculates a value of 6. The answer, in this case, is that R goes from left to right, so in this case the division step would come first:\n\n4 / 2 * 3\n\n[1] 6\n\n\nAll of the above being said, it’s helpful to remember that parentheses always come first. So, if you’re ever unsure about what order R will do things in, an easy solution is to enclose the thing you want it to do first in parentheses In addition, making the order of operations explicit makes your code more readable. By enclosing the division in parentheses (e.g., (4 / 2) * 3) we make it clear which thing happens first."
  },
  {
    "objectID": "programming.html#sec-assign",
    "href": "programming.html#sec-assign",
    "title": "2  R as a Calculator",
    "section": "2.3 Storing a number as a variable",
    "text": "2.3 Storing a number as a variable\nOne of the most important things to be able to do in R (or any programming language, for that matter) is to store information in variables. At a conceptual level you can think of a variable as label for a certain piece of information, or even several different pieces of information. For example, when using R as a calculator, there may be times when you want to store an intermediate result along the way. For example, when calculating an average (the sum divided by the count), you might wish to save the sum before dividing that sum by the count. Let’s look at the very basics for how we create variables and work with them.\n\n2.3.1 Variable assignment using &lt;- and -&gt;\nSince we’ve been working with numbers so far, let’s start by creating variables to store our numbers. And since most people like concrete examples, let’s invent one. Suppose I’m trying to calculate how much money I’m going to make selling this book. There’s several different numbers I might want to store. Firstly, I need to figure out how many copies I’ll sell. This isn’t exactly Harry Potter, so let’s assume I’m only going to sell one copy per student in my class. That’s 30 sales, so let’s create a variable called sales. What I want to do is assign a value to my variable sales, and that value should be 30. We do this by using the assignment operator, which is &lt;-. Here’s how we do it:\n\nsales &lt;- 30\n\nWhen you hit enter, R doesn’t print out any output. It just gives you another command prompt. However, behind the scenes R has created a variable called sales and given it a value of 30. You can check that this has happened by asking R to print the variable on screen. And the simplest way to do that is to type the name of the variable and hit enter.\n\nsales\n\n[1] 30\n\n\nSo that’s nice to know. Anytime you can’t remember what R has got stored in a particular variable, you can just type the name of the variable and hit enter.\nOkay, so now we know how to assign variables. Actually, there’s a bit more you should know. Firstly, one of the curious features of R is that there are several different ways of making assignments. In addition to the &lt;- operator, we can also use -&gt; and =, and it’s pretty important to understand the differences between them. Let’s start by considering -&gt;, since that’s the easy one (we’ll discuss the use of = in Section 2.4.1. As you might expect from just looking at the symbol, it’s almost identical to &lt;-. It’s just that the arrow (i.e., the assignment) goes from left to right. So if I wanted to define my sales variable using -&gt;, I would write it like this:\n\n30 -&gt; sales\n\nThis has the same effect: and it still means that I’m only going to sell 30 copies. Sigh. Apart from this superficial difference, &lt;- and -&gt; are identical. In fact, as far as R is concerned, they’re actually the same operator, just in a “left form” and a “right form”.\n\n\n2.3.2 Doing calculations using variables\nOkay, let’s get back to my original story. In my quest to become rich, I’ve written this textbook. To figure out how good a strategy is, I’ve started creating some variables in R. In addition to defining a sales variable that counts the number of copies I’m going to sell, I can also create a variable called royalty, indicating how much money I get per copy. Let’s say that my royalties are about $7 per book:\n\nsales &lt;- 30\nroyalty &lt;- 7\n\nThe nice thing about variables (in fact, the whole point of having variables) is that we can do anything with a variable that we ought to be able to do with the information that it stores. That is, since R allows me to multiply 30 by 7\n\n30 * 7\n\n[1] 210\n\n\nit also allows me to multiply sales by royalty\n\nsales * royalty\n\n[1] 210\n\n\nAs far as R is concerned, the sales * royalty command is the same as the 30 * 7 command. Not surprisingly, I can assign the output of this calculation to a new variable, which I’ll call revenue. And when we do this, the new variable revenue gets the value 35. So let’s do that, and then get R to print out the value of revenue so that we can verify that it’s done what we asked:\n\nrevenue &lt;- sales * royalty\nrevenue\n\n[1] 210\n\n\nThat’s fairly straightforward. A slightly more subtle thing we can do is reassign the value of my variable, based on its current value. For instance, suppose that one of my students (no doubt under the influence of psychotropic drugs) loves the book so much that he or she donates me an extra $550. The simplest way to capture this is by a command like this:\n\nrevenue &lt;- revenue + 550\nrevenue\n\n[1] 760\n\n\nIn this calculation, R has taken the old value of revenue (i.e., 210) and added 550 to that value, producing a value of 760 This new value is assigned to the revenue variable, overwriting its previous value. In any case, we now know that I’m expecting to make $760 off this. Pretty sweet, I thinks to myself. Or at least, that’s what I thinks until I do a few more calculation and work out what the implied hourly wage I’m making off this looks like.\n\n\n2.3.3 Rules and conventions for naming variables\nIn the examples that we’ve seen so far, my variable names (sales and revenue) have just been English-language words written using lowercase letters. However, R allows a lot more flexibility when it comes to naming your variables, as the following list of rules illustrates:\n\nVariable names can only use the upper case alphabetic characters A-Z as well as the lower case characters a-z. You can also include numeric characters 0-9 in the variable name, as well as the period . or underscore _ character. In other words, you can use SaL.e_s as a variable name (though I can’t think why you would want to), but you can’t use Sales?.\nVariable names cannot include spaces: therefore my sales is not a valid name, but my.sales is.\nVariable names are case sensitive: that is, Sales and sales are different variable names.\nVariable names must start with a letter or a period. You can’t use something like _sales or 1sales as a variable name. You can use .sales as a variable name if you want, but it’s not usually a good idea. By convention, variables starting with a . are used for special purposes, so you should avoid doing so.\nVariable names cannot be one of the reserved keywords. These are special names that R needs to keep “safe” from us mere users, so you can’t use them as the names of variables. The keywords are: if, else, repeat, while, function, for, in, next, break, TRUE, FALSE, NULL, Inf, NaN, NA, NA_integer_, NA_real_, NA_complex_, and finally, NA_character_. Don’t feel especially obliged to memorize these: if you make a mistake and try to use one of the keywords as a variable name, R will complain about it like the whiny little automaton it is.\n\nIn addition to those rules that R enforces, there are some informal conventions that people tend to follow when naming variables. One of them you’ve already seen: i.e., don’t use variables that start with a period. But there are several others. You aren’t obliged to follow these conventions, and there are many situations in which it’s advisable to ignore them, but it’s generally a good idea to follow them when you can:\n\nUse informative variable names. As a general rule, using meaningful names like sales and revenue is preferred over arbitrary ones like variable1 and variable2. Otherwise it’s very hard to remember what the contents of different variables are, and it becomes hard to understand what your commands actually do.\nUse short variable names. Typing is a pain and no-one likes doing it. So we much prefer to use a name like sales over a name like sales.for.this.book.that.you.are.reading. Obviously there’s a bit of a tension between using informative names (which tend to be long) and using short names (which tend to be meaningless), so use a bit of common sense when trading off these two conventions.\nUse one of the conventional naming styles for multi-word variable names. Suppose I want to name a variable that stores “my new salary”. Obviously I can’t include spaces in the variable name, so how should I do this? There are two main conventions that you sometimes see R users employing. First, there is “camel case” in which you use capital letters at the beginning of each constituent word (except the first one), which gives you myNewSalary as the variable name. Second, there is “snake case” in which you separate words using underscores, as in my_new_salary. Finally, you may also see some R users separating words using periods, which would give you my.new.salary. Do not do this because it is syntactically ambiguous (and thus makes your code difficult to read/understand)."
  },
  {
    "objectID": "programming.html#sec-functions",
    "href": "programming.html#sec-functions",
    "title": "2  R as a Calculator",
    "section": "2.4 Functions",
    "text": "2.4 Functions\nThe symbols +, -, *, etc. are examples of operators. As we’ve seen, you can do quite a lot of calculations just by using these operators. However, in order to do more advanced calculations (and later on, to do actual statistics), you’re going to need to start using functions. We’ll see more detail about functions and how they work later, but for now let’s just dive in and use a few. To get started, suppose I wanted to take the square root of 225. The square root, in case your high school math is a bit rusty, is just the opposite of squaring a number. So, for instance, since “5 squared is 25” I can say that “5 is the square root of 25”. This is the usual notation:\n\\[\n\\sqrt{25} = 5\n\\]\nSometimes you’ll also see it written like this:\n\\(25^{0.5} = 5\\)\nThis second way of writing it is kind of useful to “remind” you of the mathematical fact that “square root of \\(x\\)” is actually the same as “raising \\(x\\) to the power of 0.5”. Personally, I’ve never found this to be terribly meaningful psychologically, though I have to admit it’s quite convenient mathematically. Anyway, it’s not important. What is important is that you remember what a square root is, since we’re going to need it later on.\nYou may be able to calculate the square root of 25 in your head. But it gets more difficult when the numbers get bigger, and pretty much impossible if they’re not whole numbers. This is where something like R comes in very handy. Let’s say I wanted to calculate \\(\\sqrt{225}\\), the square root of 225. There’s two ways I could do this using R. First, since the square root of 255 is the same thing as raising 225 to the power of 0.5, we could use the power operator ^, just like we did earlier:\n\n225 ^ 0.5\n\n[1] 15\n\n\nHowever, there’s a second way that we can do this, since R also provides a square root function, sqrt(). To calculate the square root of 255 using this function, what I do is insert the number 225 in the parentheses. That is, the command I type is this:\n\nsqrt( 225 )\n\n[1] 15\n\n\nAs you might expect from our previous discussion, the spaces in between the parentheses are purely cosmetic. We could have typed sqrt(225) or sqrt( 225   ) and gotten the same result. When we use a function to do something, we generally refer to this as calling the function, and the values that we type into the function (there can be more than one) are referred to as the arguments of that function.\nObviously, the sqrt() function doesn’t really give us any new functionality, since we already knew how to do square root calculations by using the power operator ^, though it maybe be more explicit, clearer, and thus easier to read to use sqrt(). However, there are lots of other functions in R: in fact, almost everything of interest that I’ll talk about in this book is an R function of some kind. For example, one function that we will need to use in this book is the absolute value function. Compared to the square root function, it’s extremely simple: it just converts negative numbers to positive numbers, and leaves positive numbers alone. Mathematically, the absolute value of \\(x\\) is written \\(|x|\\) or sometimes \\(\\mbox{abs}(x)\\). Calculating absolute values in R is pretty easy, since R provides the abs() function that you can use for this purpose. When you feed it a positive number…\n\nabs( 21 )\n\n[1] 21\n\n\nthe absolute value function does nothing to it at all. But when you feed it a negative number, it spits out the positive version of the same number, like this:\n\nabs( -13 )\n\n[1] 13\n\n\nIn all honesty, there’s nothing that the absolute value function does that you couldn’t do just by looking at the number and erasing the minus sign if there is one. However, there’s a few places later in the book where we have to use absolute values, so I thought it might be a good idea to explain the meaning of the term early on.\nBefore moving on, it’s worth noting that – in the same way that R allows us to put multiple operations together into a longer command, like 1 + (2*4) for instance – it also lets us put functions together and even combine functions with operators if we so desire. For example, the following is a perfectly legitimate command:\n\nsqrt( 1 + abs(-8) )\n\n[1] 3\n\n\nWhen R executes this command, starts out by calculating the value of abs(-8), which produces an intermediate value of 8. Having done so, the command simplifies to sqrt( 1 + 8 ). To solve the square root it first needs to add 1 + 8 to get 9, at which point it evaluates sqrt(9), and so it finally outputs a value of 3.\n\n2.4.1 Function arguments, their names and their defaults\nThere’s two more fairly important things that you need to understand about how functions work in R, and that’s the use of “named” arguments, and “default values” for arguments. Not surprisingly, that’s not to say that this is the last we’ll hear about how functions work, but they are the last things we desperately need to discuss in order to get you started. To understand what these two concepts are all about, I’ll introduce another function. The round() function can be used to round some value to the nearest whole number. For example, I could type this:\n\nround( 3.1415 )\n\n[1] 3\n\n\nPretty straightforward, really. However, suppose I only wanted to round it to two decimal places: that is, I want to get 3.14 as the output. The round() function supports this, by allowing you to input a second argument to the function that specifies the number of decimal places that you want to round the number to. In other words, I could do this:\n\nround( 3.14165, 2 )\n\n[1] 3.14\n\n\nWhat’s happening here is that I’ve specified two arguments: the first argument is the number that needs to be rounded (i.e., 3.14165), the second argument is the number of decimal places that it should be rounded to (i.e., 2), and the two arguments are separated by a comma. In this simple example, it’s quite easy to remember which one argument comes first and which one comes second, but for more complicated functions this is not easy. Fortunately, most R functions make use of argument names. For the round() function, for example the number that needs to be rounded is specified using the x argument, and the number of decimal points that you want it rounded to is specified using the digits argument. Because we have these names available to us, we can specify the arguments to the function by name. We do so like this:\n\nround( x = 3.1415, digits = 2 )\n\n[1] 3.14\n\n\nNotice that this is kind of similar in spirit to variable assignment, except that = is used here, rather than &lt;-. In both cases we’re specifying specific values to be associated with a label. However, there are some differences between what we were doing earlier on when creating variables, and what we’re doing here when specifying arguments, and so as a consequence it’s important that you use = in this context.\nAs you can see, specifying the arguments by name involves a lot more typing, but it’s also explicit and thus a lot easier to read. Because of this, the commands in this book will usually specify arguments by name, since that makes it clearer to you what I’m doing. However, one important thing to note is that when specifying the arguments using their names, it doesn’t matter what order you type them in. But if you don’t use the argument names, then you have to input the arguments in the correct order. In other words, these three commands all produce the same output…\n\nround( 3.14165, 2 )\n\n[1] 3.14\n\nround( x = 3.1415, digits = 2 )\n\n[1] 3.14\n\nround( digits = 2, x = 3.1415 )\n\n[1] 3.14\n\n\nbut this one does not…\n\nround( 2, 3.14165 )\n\n[1] 2\n\n\nHow do you find out what the correct order is? There’s a few different ways, but the easiest one is to look at the help documentation for the function (e.g., ? round). However, if you’re ever unsure, it’s probably best to actually type in the argument name.\nOkay, so that’s the first thing I said you’d need to know: argument names. The second thing you need to know about is default values. Notice that the first time I called the round() function I didn’t actually specify the digits argument at all, and yet R somehow knew that this meant it should round to the nearest whole number. How did that happen? The answer is that the digits argument has a default value of 0, meaning that if you decide not to specify a value for digits then R will act as if you had typed digits = 0. This is quite handy: the vast majority of the time when you want to round a number you want to round it to the nearest whole number, and it would be pretty annoying to have to specify the digits argument every single time. On the other hand, sometimes you actually do want to round to something other than the nearest whole number, and it would be even more annoying if R didn’t allow this! Thus, by having digits = 0 as the default value, we get the best of both worlds."
  },
  {
    "objectID": "programming.html#sec-vectors",
    "href": "programming.html#sec-vectors",
    "title": "2  R as a Calculator",
    "section": "2.5 Storing many numbers as a vector",
    "text": "2.5 Storing many numbers as a vector\nAt this point we’ve covered functions in enough detail to get us safely through most of the rest of the book, so let’s return to our discussion of variables. When variables were introduced in Section 2.3 we saw how we can use variables to store a single number. In this section, we’ll extend this idea and look at how to store multiple numbers within the one variable. In R, a variable stores multiple values is called a vector. So let’s create one.\n\n2.5.1 Creating a vector\nLet’s stick to my silly “get rich quick by textbook writing” example. Suppose the textbook company (if there actually was one, that is) sends sales data on a monthly basis. Since my class start in late February, we might expect most of the sales to occur towards the start of the year. Let’s suppose that I have 100 sales in February, 200 sales in March and 50 sales in April, and no other sales for the rest of the year. What I would like to do is have a variable – let’s call it sales.by.month – that stores all this sales data. The first number stored should be 0 since I had no sales in January, the second should be 100, and so on. The simplest way to do this in R is to use the combine function, c(). To do so, all we have to do is type all the numbers you want to store in a comma separated list, like this:\n\nsales.by.month &lt;- c(0, 100, 200, 50, 0, 0, 0, 0, 0, 0, 0, 0)\nsales.by.month\n\n [1]   0 100 200  50   0   0   0   0   0   0   0   0\n\n\nTo use the correct terminology here, we have a single variable here called sales.by.month: this variable is a vector that consists of 12 elements.\n\n\n2.5.2 A handy digression\nNow that we’ve learned how to put information into a vector, the next thing to understand is how to pull that information back out again. However, before I do so it’s worth taking a slight detour. If you’ve been following along, typing all the commands into R yourself, it’s possible that the output that you saw when we printed out the sales.by.month vector was slightly different to what I showed above. This would have happened if the window (or the RStudio panel) that contains the R console is really, really narrow. If that were the case, you might have seen output that looks something like this:\n\nsales.by.month\n\n [1]   0 100 200  50\n [5]   0   0   0   0\n [9]   0   0   0   0\n\n\nBecause there wasn’t much room on the screen, R has printed out the results over three lines. But that’s not the important thing to notice. The important point is that the first line has a [1] in front of it, whereas the second line starts with [5] and the third with [9]. It’s pretty clear what’s happening here. For the first row, R has printed out the 1st element through to the 4th element, so it starts that row with a [1]. For the second row, R has printed out the 5th element of the vector through to the 8th one, and so it begins that row with a [5] so that you can tell where it’s up to at a glance. It might seem a bit odd to you that R does this, but in some ways it’s a kindness, especially when dealing with larger data sets!\n\n\n2.5.3 Getting information out of vectors\nTo get back to the main story, let’s consider the problem of how to get information out of a vector. At this point, you might have a sneaking suspicion that the answer has something to do with the [1] and [9] things that R has been printing out. And of course you are correct. Suppose I want to pull out the February sales data only. February is the second month of the year, so let’s try this:\n\nsales.by.month[2]\n\n[1] 100\n\n\nYep, that’s the February sales all right. But there’s a subtle detail to be aware of here: notice that R outputs [1] 100, not [2] 100. This is because R is being extremely literal. When we typed in sales.by.month[2], we asked R to find exactly one thing, and that one thing happens to be the second element of our sales.by.month vector. So, when it outputs [1] 100 what R is saying is that the first number that we just asked for is 100. This behaviour makes more sense when you realise that we can use this trick to create new variables. For example, I could create a february.sales variable like this:\n\nfebruary.sales &lt;- sales.by.month[2]\nfebruary.sales\n\n[1] 100\n\n\nObviously, the new variable february.sales should only have one element and so when I print it out this new variable, the R output begins with a [1] because 100 is the value of the first (and only) element of february.sales. The fact that this also happens to be the value of the second element of sales.by.month is irrelevant. We’ll pick this topic up again shortly ( Section 2.8).\n\n\n2.5.4 Altering the elements of a vector\nSometimes you’ll want to change the values stored in a vector. Imagine my surprise when the publisher rings me up to tell me that the sales data for May are wrong. There were actually an additional 25 books sold in May, but there was an error or something so they hadn’t told me about it. How can I fix my sales.by.month variable? One possibility would be to assign the whole vector again from the beginning, using c(). But that’s a lot of typing. Also, it’s a little wasteful: why should R have to redefine the sales figures for all 12 months, when only the 5th one is wrong? Fortunately, we can tell R to change only the 5th element, using this trick:\n\nsales.by.month[5] &lt;- 25\nsales.by.month\n\n [1]   0 100 200  50  25   0   0   0   0   0   0   0\n\n\nAnother way to edit variables is to use the edit() or fix() functions. I won’t discuss them in detail right now, but you can check them out on your own.\n\n\n2.5.5 Useful things to know about vectors\nBefore moving on, I want to mention a couple of other things about vectors. Firstly, you often find yourself wanting to know how many elements there are in a vector (usually because you’ve forgotten). You can use the length() function to do this. It’s quite straightforward:\n\nlength( x = sales.by.month )\n\n[1] 12\n\n\nSecondly, you often want to alter all of the elements of a vector at once. For instance, suppose I wanted to figure out how much money I made in each month. Since I’m earning an exciting $7 per book (no seriously, that’s actually pretty close to what authors get on the very expensive textbooks that you’re expected to purchase), what I want to do is multiply each element in the sales.by.month vector by 7. R makes this pretty easy, as the following example shows:\n\nsales.by.month * 7\n\n [1]    0  700 1400  350  175    0    0    0    0    0    0    0\n\n\nIn other words, when you multiply a vector by a single number, all elements in the vector get multiplied. The same is true for addition, subtraction, division and taking powers. So that’s neat. On the other hand, suppose I wanted to know how much money I was making per day, rather than per month. Since not every month has the same number of days, I need to do something slightly different. Firstly, I’ll create two new vectors:\n\ndays.per.month &lt;- c(31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31)\nprofit &lt;- sales.by.month * 7\n\nObviously, the profit variable is the same one we created earlier, and the days.per.month variable is pretty straightforward. What I want to do is divide every element of profit by the corresponding element of days.per.month. Again, R makes this pretty easy:\n\nprofit / days.per.month\n\n [1]  0.000000 25.000000 45.161290 11.666667  5.645161  0.000000  0.000000\n [8]  0.000000  0.000000  0.000000  0.000000  0.000000\n\n\nI still don’t like all those zeros, but that’s not what matters here. Notice that the second element of the output is 25, because R has divided the second element of profit (i.e. 700) by the second element of days.per.month (i.e. 28). Similarly, the third element of the output is equal to 1400 divided by 31, and so on. We’ll talk more about calculations involving vectors later on, but that’s enough detail for now."
  },
  {
    "objectID": "programming.html#sec-text",
    "href": "programming.html#sec-text",
    "title": "2  R as a Calculator",
    "section": "2.6 Storing text data",
    "text": "2.6 Storing text data\nA lot of the time your data will be numeric in nature, but not always. Sometimes your data really needs to be described using text, not using numbers. To address this, we need to consider the situation where our variables store text. To create a variable that stores the word “hello”, we can type this:\n\ngreeting &lt;- \"hello\"\ngreeting\n\n[1] \"hello\"\n\n\nWhen interpreting this, it’s important to recognise that the quote marks here aren’t part of the string itself. They’re just something that we use to make sure that R knows to treat the characters that they enclose as a piece of text data, known as a character string. In other words, R treats \"hello\" as a string containing the word “hello”; but if I had typed hello instead, R would go looking for a variable by that name! You can also use 'hello' to specify a character string.\nOkay, so that’s how we store the text. Next, it’s important to recognise that when we do this, R stores the entire word \"hello\" as a single element: our greeting variable is not a vector of five different letters. Rather, it has only the one element, and that element corresponds to the entire character string \"hello\". To illustrate this, if I actually ask R to find the first element of greeting, it prints the whole string:\n\ngreeting[1]\n\n[1] \"hello\"\n\n\nOf course, there’s no reason why I can’t create a vector of character strings. For instance, if we were to continue with the example of my attempts to look at the monthly sales data for my book, one variable I might want would include the names of all 12 months. To do so, I could type in a command like this\n\nmonths &lt;- c(\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n            \"July\", \"August\", \"September\", \"October\", \"November\", \n            \"December\")\n\nThis is a character vector containing 12 elements, each of which is the name of a month. So if I wanted R to tell me the name of the fourth month, all I would do is this:\n\nmonths[4]\n\n[1] \"April\"\n\n\n\n2.6.1 Working with text\nWorking with text data is somewhat more complicated than working with numeric data. There is much to discuss here, but for purposes of the current chapter we only need this bare bones sketch. The only other thing I want to do before moving on is show you an example of a function that can be applied to text data. So far, most of the functions that we have seen (i.e., sqrt(), abs() and round()) only make sense when applied to numeric data (e.g., you can’t calculate the square root of “hello”), and we’ve seen one function that can be applied to pretty much any variable or vector (i.e., length()). So it might be nice to see an example of a function that can be applied to text.\nThe function I’m going to introduce you to is called nchar(), and what it does is count the number of individual characters that make up a string. Recall earlier that when we tried to calculate the length() of our greeting variable it returned a value of 1: the greeting variable contains only the one string, which happens to be \"hello\". But what if I want to know how many letters there are in the word? Sure, I could count them, but that’s boring, and more to the point it’s a terrible strategy if what I wanted to know was the number of letters in War and Peace. That’s where the nchar() function is helpful:\n\nnchar( x = greeting )\n\n[1] 5\n\n\nThat makes sense, since there are in fact 5 letters in the string \"hello\". Better yet, you can apply nchar() to whole vectors. So, for instance, if I want R to tell me how many letters there are in the names of each of the 12 months, I can do this:\n\nnchar( x = months )\n\n [1] 7 8 5 5 3 4 4 6 9 7 8 8\n\n\nSo that’s nice to know. The nchar() function can do a bit more than this, and there’s a lot of other functions that you can do to extract more information from text or do all sorts of fancy things. However, the goal here is not to teach any of that! The goal right now is just to see an example of a function that actually does work when applied to text."
  },
  {
    "objectID": "programming.html#logicals",
    "href": "programming.html#logicals",
    "title": "2  R as a Calculator",
    "section": "2.7 Storing “true or false” data",
    "text": "2.7 Storing “true or false” data\nTime to move onto a third kind of data. A key concept in that a lot of R relies on is the idea of a logical value. A logical value is an assertion about whether something is true or false. This is implemented in R in a pretty straightforward way. There are two logical values, namely TRUE and FALSE. Despite the simplicity, a logical values are very useful things. Let’s see how they work.\n\n2.7.1 Assessing mathematical truths\nIn George Orwell’s classic book 1984, one of the slogans used by the totalitarian Party was “two plus two equals five”, the idea being that the political domination of human freedom becomes complete when it is possible to subvert even the most basic of truths. It’s a terrifying thought, especially when the protagonist Winston Smith finally breaks down under torture and agrees to the proposition. “Man is infinitely malleable”, the book says. I’m pretty sure that this isn’t true of humans but it’s definitely not true of R. R is not infinitely malleable. It has rather firm opinions on the topic of what is and isn’t true, at least as regards basic mathematics. If I ask it to calculate 2 + 2, it always gives the same answer, and it’s not bloody 5:\n\n2 + 2\n\n[1] 4\n\n\nOf course, so far R is just doing the calculations. I haven’t asked it to explicitly assert that \\(2+2 = 4\\) is a true statement. If I want R to make an explicit judgement, I can use a command like this:\n\n2 + 2 == 4\n\n[1] TRUE\n\n\nWhat I’ve done here is use the equality operator, ==, to force R to make a “true or false” judgement. Okay, let’s see what R thinks of the Party slogan:\n\n2+2 == 5\n\n[1] FALSE\n\n\nBooyah! Freedom and ponies for all! Or something like that. Anyway, it’s worth having a look at what happens if I try to force R to believe that two plus two is five by making an assignment statement like 2 + 2 = 5 or 2 + 2 &lt;- 5. When I do this, here’s what happens:\n\n2 + 2 = 5\n\nError in 2 + 2 = 5: target of assignment expands to non-language object\n\n\nR doesn’t like this very much. It recognises that 2 + 2 is not a variable (that’s what the “non-language object” part is saying), and it won’t let you try to “reassign” it. While R is pretty flexible, and actually does let you do some quite remarkable things to redefine parts of R itself, there are just some basic, primitive truths that it refuses to give up. It won’t change the laws of addition, and it won’t change the definition of the number 2.\nThat’s probably for the best.\n\n\n2.7.2 Logical operations\nSo now we’ve seen logical operations at work, but so far we’ve only seen the simplest possible example. You probably won’t be surprised to discover that we can combine logical operations with other operations and functions in a more complicated way, like this:\n\n3*3 + 4*4 == 5*5\n\n[1] TRUE\n\n\nor this\n\nsqrt( 25 ) == 5\n\n[1] TRUE\n\n\nNot only that, but as Table Table 2.2 illustrates, there are several other logical operators that you can use, corresponding to some basic mathematical concepts.\n\n\nTable 2.2: ?(caption)\n\n\n\n\n(a) Some logical operators. Technically I should be calling these “binary relational operators”, but quite frankly I don’t want to. It’s my book so no-one can make me.\n\n\noperation\noperator\nexample input\nanswer\n\n\n\n\nless than\n&lt;\n2 &lt; 3\nTRUE\n\n\nless than or equal to\n&lt;=\n2 &lt;= 2\nTRUE\n\n\ngreater than\n&gt;\n2 &gt; 3\nFALSE\n\n\ngreater than or equal to\n&gt;=\n2 &gt;= 2\nTRUE\n\n\nequal to\n==\n2 == 3\nFALSE\n\n\nnot equal to\n!=\n2 != 3\nTRUE\n\n\n\n\n\n\nHopefully these are all pretty self-explanatory: for example, the less than operator &lt; checks to see if the number on the left is less than the number on the right. If it’s less, then R returns an answer of TRUE:\n\n99 &lt; 100\n\n[1] TRUE\n\n\nbut if the two numbers are equal, or if the one on the right is larger, then R returns an answer of FALSE, as the following two examples illustrate:\n\n100 &lt; 100\n\n[1] FALSE\n\n100 &lt; 99\n\n[1] FALSE\n\n\nIn contrast, the less than or equal to operator &lt;= will do exactly what it says. It returns a value of TRUE if the number of the left hand side is less than or equal to the number on the right hand side. So if we repeat the previous two examples using &lt;=, here’s what we get:\n\n100 &lt;= 100\n\n[1] TRUE\n\n100 &lt;= 99\n\n[1] FALSE\n\n\nAnd at this point I hope it’s pretty obvious what the greater than operator &gt; and the greater than or equal to operator &gt;= do! Next on the list of logical operators is the not equal to operator != which – as with all the others – does what it says it does. It returns a value of TRUE when things on either side are not identical to each other. Therefore, since \\(2+2\\) isn’t equal to \\(5\\), we get:\n\n2 + 2 != 5\n\n[1] TRUE\n\n\nWe’re not quite done yet. There are three more logical operations that are worth knowing about, listed in Table Table 2.3.\n\n\nTable 2.3: ?(caption)\n\n\n\n\n(a) Some more logical operators.\n\n\noperation\noperator\nexample input\nanswer\n\n\n\n\nnot\n!\n!(1==1)\nFALSE\n\n\nor\n|\n(1==1) | (2==3)\nTRUE\n\n\nand\n&\n(1==1) & (2==3)\nFALSE\n\n\n\n\n\n\nThese are the not operator !, the and operator &, and the or operator |. Like the other logical operators, their behaviour is more or less exactly what you’d expect given their names. For instance, if I ask you to assess the claim that “either \\(2+2 = 4\\) or \\(2+2 = 5\\)” you’d say that it’s true. Since it’s an “either-or” statement, all we need is for one of the two parts to be true. That’s what the | operator does:\n\n(2+2 == 4) | (2+2 == 5)\n\n[1] TRUE\n\n\nOn the other hand, if I ask you to assess the claim that “both \\(2+2 = 4\\) and \\(2+2 = 5\\)” you’d say that it’s false. Since this is an and statement we need both parts to be true. And that’s what the & operator does:\n\n(2+2 == 4) & (2+2 == 5)\n\n[1] FALSE\n\n\nFinally, there’s the not operator, which is simple but annoying to describe in English. If I ask you to assess my claim that “it is not true that \\(2+2 = 5\\)” then you would say that my claim is true; because my claim is that “\\(2+2 = 5\\) is false”. And I’m right. If we write this as an R command we get this:\n\n! (2+2 == 5)\n\n[1] TRUE\n\n\nIn other words, since 2+2 == 5 is a FALSE statement, it must be the case that !(2+2 == 5) is a TRUE one. Essentially, what we’ve really done is claim that “not false” is the same thing as “true”. Obviously, this isn’t really quite right in real life. But R lives in a much more black or white world: for R everything is either true or false. No shades of gray are allowed. We can actually see this much more explicitly, like this:\n\n! FALSE\n\n[1] TRUE\n\n\nOf course, in our \\(2+2 = 5\\) example, we didn’t really need to use “not” ! and “equals to” == as two separate operators. We could have just used the “not equals to” operator != like this:\n\n2+2 != 5\n\n[1] TRUE\n\n\nBut there are many situations where you really do need to use the ! operator. We’ll see some later on.\n\n\n2.7.3 Storing and using logical data\nUp to this point, I’ve introduced numeric data (Section 2.3 and Section 2.5) and character data (Section 2.6). So you might not be surprised to discover that these TRUE and FALSE values that R has been producing are actually a third kind of data, called logical data. That is, when I asked R if 2 + 2 == 5 and it said [1] FALSE in reply, it was actually producing information that we can store in variables. For instance, I could create a variable called is.the.Party.correct, which would store R’s opinion:\n\nis.the.Party.correct &lt;- 2 + 2 == 5\nis.the.Party.correct\n\n[1] FALSE\n\n\nAlternatively, you can assign the value directly, by typing TRUE or FALSE in your command. Like this:\n\nis.the.Party.correct &lt;- FALSE\nis.the.Party.correct\n\n[1] FALSE\n\n\nBetter yet, because it’s kind of tedious to type TRUE or FALSE over and over again, R provides you with a shortcut: you can use T and F instead (but it’s case sensitive: t and f won’t work).\n::: {.callout-caution} ## TRUE and FALSE\nTRUE and FALSE are reserved keywords in R, so you can trust that they always mean what they say they do. Unfortunately, the shortcut versions T and F do not have this property. It’s even possible to create variables that set up the reverse meanings, by typing commands like T &lt;- FALSE and F &lt;- TRUE. This is kind of insane, and something that is generally thought to be a design flaw in R. Anyway, the long and short of it is that it’s safer to use TRUE and FALSE.:::\nSo this works:\n\nis.the.Party.correct &lt;- F\nis.the.Party.correct\n\n[1] FALSE\n\n\nbut this doesn’t:\n\nis.the.Party.correct &lt;- f\n\nError in eval(expr, envir, enclos): object 'f' not found\n\n\n\n\n2.7.4 Vectors of logicals\nThe next thing to mention is that you can store vectors of logical values in exactly the same way that you can store vectors of numbers (Section 2.5) and vectors of text data (Section 2.6). Again, we can define them directly via the c() function, like this:\n\nx &lt;- c(TRUE, TRUE, FALSE)\nx\n\n[1]  TRUE  TRUE FALSE\n\n\nor you can produce a vector of logicals by applying a logical operator to a vector. This might not make a lot of sense to you, so let’s unpack it slowly. First, let’s suppose we have a vector of numbers (i.e., a “non-logical vector”). For instance, we could use the sales.by.month vector that we were using in Section 2.5. Suppose I wanted R to tell me, for each month of the year, whether I actually sold a book in that month. I can do that by typing this:\n\nsales.by.month &gt; 0\n\n [1] FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nand again, I can store this in a vector if I want, as the example below illustrates:\n\nany.sales.this.month &lt;- sales.by.month &gt; 0\nany.sales.this.month\n\n [1] FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nIn other words, any.sales.this.month is a logical vector whose elements are TRUE only if the corresponding element of sales.by.month is greater than zero. For instance, since I sold zero books in January, the first element is FALSE.\n\n\n2.7.5 Applying logical operation to text\nIn a moment (Section 2.8) I’ll show you why these logical operations and logical vectors are so handy, but before I do so I want to very briefly point out that you can apply them to text as well as to logical data. It’s just that we need to be a bit more careful in understanding how R interprets the different operations. In this section I’ll talk about how the equal to operator == applies to text, since this is the most important one. Obviously, the not equal to operator != gives the exact opposite answers to == so I’m implicitly talking about that one too, but I won’t give specific commands showing the use of !=. There are a variety of other operators, but those will do for now.\nOkay, let’s see how it works. In one sense, it’s very simple. For instance, I can ask R if the word \"cat\" is the same as the word \"dog\", like this:\n\n\"cat\" == \"dog\"\n\n[1] FALSE\n\n\nThat’s pretty obvious, and it’s good to know that even R can figure that out. Similarly, R does recognise that a \"cat\" is a \"cat\":\n\n\"cat\" == \"cat\"\n\n[1] TRUE\n\n\nAgain, that’s exactly what we’d expect. However, what you need to keep in mind is that R is not at all tolerant when it comes to grammar and spacing. If two strings differ in any way whatsoever, R will say that they’re not equal to each other, as the following examples indicate:\n\n\" cat\" == \"cat\"\n\n[1] FALSE\n\n\"cat\" == \"CAT\"\n\n[1] FALSE\n\n\"cat\" == \"c a t\"\n\n[1] FALSE"
  },
  {
    "objectID": "programming.html#sec-indexing",
    "href": "programming.html#sec-indexing",
    "title": "2  R as a Calculator",
    "section": "2.8 Indexing vectors",
    "text": "2.8 Indexing vectors\nOne last thing to add before finishing up this chapter. So far, whenever I’ve had to get information out of a vector, all I’ve done is typed something like months[4]; and when I do this R prints out the fourth element of the months vector. In this section, I’ll show you two additional tricks for getting information out of the vector.\n\n2.8.1 Extracting multiple elements\nOne very useful thing we can do is pull out more than one element at a time. In the previous example, we only used a single number (i.e., 2) to indicate which element we wanted. Alternatively, we can use a vector. So, suppose I wanted the data for February, March and April. What I could do is use the vector c(2,3,4) to indicate which elements I want R to pull out. That is, I’d type this:\n\nsales.by.month[ c(2,3,4) ]\n\n[1] 100 200  50\n\n\nNotice that the order matters here. If I asked for the data in the reverse order (i.e., April first, then March, then February) by using the vector c(4,3,2), then R outputs the data in the reverse order:\n\nsales.by.month[ c(4,3,2) ]\n\n[1]  50 200 100\n\n\nA second thing to be aware of is that R provides you with handy shortcuts for very common situations. For instance, suppose that I wanted to extract everything from the 2nd month through to the 8th month. One way to do this is to do the same thing I did above, and use the vector c(2,3,4,5,6,7,8) to indicate the elements that I want. That works just fine\n\nsales.by.month[ c(2,3,4,5,6,7,8) ]\n\n[1] 100 200  50  25   0   0   0\n\n\nbut it’s kind of a lot of typing. To help make this easier, R lets you use 2:8 as shorthand for c(2,3,4,5,6,7,8), which makes things a lot simpler. First, let’s just check that this is true:\n\n2:8\n\n[1] 2 3 4 5 6 7 8\n\n\nNext, let’s check that we can use the 2:8 shorthand as a way to pull out the 2nd through 8th elements of sales.by.months:\n\nsales.by.month[2:8]\n\n[1] 100 200  50  25   0   0   0\n\n\nSo that’s kind of neat.\n\n\n2.8.2 Logical indexing\nAt this point, I can introduce an extremely useful tool called logical indexing. In the last section, I created a logical vector any.sales.this.month, whose elements are TRUE for any month in which I sold at least one book, and FALSE for all the others. However, that big long list of TRUEs and FALSEs is a little bit hard to read, so what I’d like to do is to have R select the names of the months for which I sold any books. Earlier on, I created a vector months that contains the names of each of the months. This is where logical indexing is handy. What I need to do is this:\n\nmonths[ sales.by.month &gt; 0 ]\n\n[1] \"February\" \"March\"    \"April\"    \"May\"     \n\n\nTo understand what’s happening here, it’s helpful to notice that sales.by.month &gt; 0 is the same logical expression that we used to create the any.sales.this.month vector in the last section. In fact, I could have just done this:\n\nmonths[ any.sales.this.month ]\n\n[1] \"February\" \"March\"    \"April\"    \"May\"     \n\n\nand gotten exactly the same result. In order to figure out which elements of months to include in the output, what R does is look to see if the corresponding element in any.sales.this.month is TRUE. Thus, since element 1 of any.sales.this.month is FALSE, R does not include \"January\" as part of the output; but since element 2 of any.sales.this.month is TRUE, R does include \"February\" in the output. Note that there’s no reason why I can’t use the same trick to find the actual sales numbers for those months. The command to do that would just be this:\n\nsales.by.month [ sales.by.month &gt; 0 ]\n\n[1] 100 200  50  25\n\n\nIn fact, we can do the same thing with text. Here’s an example. Suppose that – to continue the saga of the textbook sales – I later find out that the bookshop only had sufficient stocks for a few months of the year. They tell me that early in the year they had \"high\" stocks, which then dropped to \"low\" levels, and in fact for one month they were \"out\" of copies of the book for a while before they were able to replenish them. Thus I might have a variable called stock.levels which looks like this:\n\nstock.levels&lt;-c(\"high\", \"high\", \"low\", \"out\", \"out\", \"high\",\n                \"high\", \"high\", \"high\", \"high\", \"high\", \"high\")\n\nstock.levels\n\n [1] \"high\" \"high\" \"low\"  \"out\"  \"out\"  \"high\" \"high\" \"high\" \"high\" \"high\"\n[11] \"high\" \"high\"\n\n\nThus, if I want to know the months for which the bookshop was out of my book, I could apply the logical indexing trick, but with the character vector stock.levels, like this:\n\nmonths[stock.levels == \"out\"]\n\n[1] \"April\" \"May\"  \n\n\nAlternatively, if I want to know when the bookshop was either low on copies or out of copies, I could do this:\n\nmonths[stock.levels == \"out\" | stock.levels == \"low\"]\n\n[1] \"March\" \"April\" \"May\"  \n\n\nor this\n\nmonths[stock.levels != \"high\" ]\n\n[1] \"March\" \"April\" \"May\"  \n\n\nEither way, I get the answer I want.\nAt this point, I hope you can see why logical indexing is such a useful thing. It’s a very basic, yet very powerful way to manipulate data. Subsequent chapters will talk a lot more about how to manipulate data, since it’s a critical skill for real world research that is often overlooked in introductory statistics courses It does take a bit of practice to become completely comfortable using logical indexing, so it’s a good idea to play around with these sorts of commands. Try creating a few different variables of your own, and then ask yourself questions like “how do I get R to spit out all the elements that are [blah]”. Practice makes perfect, and it’s only by practicing logical indexing that you’ll perfect the art of yelling frustrated insults at your computer."
  },
  {
    "objectID": "programming.html#activities",
    "href": "programming.html#activities",
    "title": "2  R as a Calculator",
    "section": "2.9 Activities",
    "text": "2.9 Activities\n\nOpen RStudio and"
  },
  {
    "objectID": "data.html#sec-load",
    "href": "data.html#sec-load",
    "title": "3  Working with Data",
    "section": "3.1 Loading and saving data",
    "text": "3.1 Loading and saving data\nThere are several different types of files that are likely to be relevant to us when doing data analysis. There are three in particular that are especially important from the perspective of this book:\n\nComma separated value (CSV) files are those with a .csv file extension. These are just regular old text files, and they can be opened with almost any software. This means that storing data in CSV files does not tie users to any particular software and keeps things simple.\nWorkspace files are those with a .Rdata file extension. This is the standard kind of file that R uses to store data and variables. They’re called “workspace files” because you can use them to save your whole workspace.\n\n\n3.1.1 Importing data from CSV files using read_csv\nOne quite commonly used data format is the humble “comma separated value” file, also called a CSV file, and usually bearing the file extension .csv. CSV files are just plain old-fashioned text files, and what they store is basically just a table of data. This is illustrated in Figure Figure 3.1, which shows a file called booksales.csv that I’ve created. As you can see, each row corresponds to a variable, and each row represents the book sales data for one month. The first row doesn’t contain actual data though: it has the names of the variables.\n\n\n\nFigure 3.1: The booksales.csv data file. On the left, I’ve opened the file in using a spreadsheet program (OpenOffice), which shows that the file is basically a table. On the right, the same file is open in a standard text editor (the TextEdit program on a Mac), which shows how the file is formatted. The entries in the table are wrapped in quote marks and separated by commas.\n\n\nIf RStudio were not available to you, the easiest way to open this file would be to use the read.csv() function. This function is pretty flexible, and I’ll talk a lot more about it’s capabilities in Section 3.1.2 for more details, but for now there’s only two arguments to the function that I’ll mention:\n\nfile. This should be a character string that specifies a path to the file that needs to be loaded. You can use an absolute path or a relative path to do so.\nheader. This is a logical value indicating whether or not the first row of the file contains variable names. The default value is TRUE.\n\nTherefore, to import the CSV file, the command I need is:\n\nbooks &lt;- read_csv(\"./data/booksales.csv\")\n\nThere are two very important points to notice here. Firstly, notice that I didn’t try to use the load() function, because that function is only meant to be used for .Rdata files. If you try to use load() on other types of data, you get an error. Secondly, notice that when I imported the CSV file I assigned the result to a variable, which I imaginatively called books file. There’s a reason for this. The idea behind an .Rdata file is that it stores a whole workspace. So, if you had the ability to look inside the file yourself you’d see that the data file keeps track of all the variables and their names. So when you load() the file, R restores all those original names. CSV files are treated differently: as far as R is concerned, the CSV only stores one variable, but that variable is big table. So when you import that table into the workspace, R expects you to give it a name.] Let’s have a look at what we’ve got:\n\nprint(books)\n\n# A tibble: 12 × 4\n   Month      Days Sales Stock.Levels\n   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n 1 January      31     0 high        \n 2 February     28   100 high        \n 3 March        31   200 low         \n 4 April        30    50 out         \n 5 May          31     0 out         \n 6 June         30     0 high        \n 7 July         31     0 high        \n 8 August       31     0 high        \n 9 September    30     0 high        \n10 October      31     0 high        \n11 November     30     0 high        \n12 December     31     0 high        \n\n\nYou can instead open the data in RStudio’s spreadsheet viewer:\n\nView(books)\n\nThe books data set is quite small, so just calling print() is fine. But for larger data sets, the View() and the spreadsheet viewer allows for a more thorough inspection.\nClearly, it’s worked, but the format of this output is a bit unfamiliar. We haven’t seen anything like this before. What you’re looking at is a data frame, which is a very important kind of variable in R, and one I’ll discuss at length in Chapter 5. For now, let’s just be happy that we imported the data and that it looks about right.\n\n\n\n\n\n\nNote\n\n\n\nIn a lot of books you’ll see the read.table() or read.csv() functions used for this purpose instead of read_csv(). They’re similar functions, but read_csv() is a function from the readr package, one of the many packages that make up “the tidyverse”. The tidyverse is a set of interoperable packages we will be using throughout this course. Further information about the tidyverse can be found in Chapter 4.\n\n\n\n\n3.1.2 Importing data from CSV files using RStudio\n\n\n\n\n\n\nWarning\n\n\n\nAs detailed above, read_csv() is how we will import CSV data files into R. This section details an alternative way to import CSV data files, but it will not produce the same kind of variable. Using RStudio’s built-in “Import Dataset” functionality will produce a dataframe, whereas read_csv produces a “tibble”. We will Further information about the difference between plain dataframes and tibbles can be found in Chapter 5.\n\n\nIn the environment panel in RStudio you should see a button called “Import Dataset”. Click on that, and it will give you a couple of options: select the “From Text File…” option, and it will open up a very familiar dialog box asking you to select a file: if you’re on a Mac, it’ll look like the usual Finder/Explorer window that you use to choose a file. I’m assuming that you’re familiar with your own computer, so you should have no problem finding the CSV file that you want to import! Find the one you want, then click on the “Open” button. When you do this, you’ll see a window that looks like the one in Figure 3.2.\n\n\n\nFigure 3.2: A dialog box on a Mac asking you to select the CSV file R should try to import. Mac users will recognise this immediately: it’s the usual way in which a Mac asks you to find a file. Windows users won’t see this: they’ll see the usual explorer window that Windows always gives you when it wants you to select a file.\n\n\nThe import data set window is relatively straightforward to understand.\n\n\n\nThe RStudio window for importing a CSV file into R.\n\n\nIn the top left corner, you need to type the name of the variable you R to create. By default, that will be the same as the file name: our file is called booksales.csv, so RStudio suggests the name booksales. If you’re happy with that, leave it alone. If not, type something else. Immediately below this are a few things that you can tweak to make sure that the data gets imported correctly:\n\nHeading. Does the first row of the file contain raw data, or does it contain headings for each variable? The booksales.csv file has a header at the top, so I selected “yes”.\nSeparator. What character is used to separate different entries? In most CSV files this will be a comma (it is “comma separated” after all). But you can change this if your file is different.\nDecimal. What character is used to specify the decimal point? In English speaking countries, this is almost always a period (i.e., .). That’s not universally true: many European countries use a comma. So you can change that if you need to.\nQuote. What character is used to denote a block of text? That’s usually going to be a double quote mark. It is for the booksales.csv file, so that’s what I selected.\n\nOne nice thing about the RStudio window is that it shows you the raw data file at the top of the window, and it shows you a preview of the data at the bottom. If the data at the bottom doesn’t look right, try changing some of the settings on the left hand side. Once you’re happy, click “Import”. When you do, two commands appear in the R console:\n\nbooksales &lt;- read.csv(\"~/Rbook/data/booksales.csv\")\nView(booksales)\n\nThe first of these commands is the one that loads the data. The second one will display a pretty table showing the data in RStudio.\nNote, however, that this variable looks a bit different from the sales variable we created using read_csv. This is because RStudio has used read.csv() rather than read_csv() as we did in the previous section. Again, the differences will be discussed more in Chapter 5.\n\n\n3.1.3 Loading workspace files using R\nWhen I used the list.files() command to list the contents of the /Users/dan/Rbook/data directory, the output referred to a file called booksales.Rdata. Let’s say I want to load the data from this file into my workspace. The way I do this is with the load() function. There are two arguments to this function, but the only one we’re interested in is\n\nfile. This should be a character string that specifies a path to the file that needs to be loaded. You can use an absolute path or a relative path to do so.\n\nUsing the absolute file path, the command would look like this:\n\nload( file = \"/home/christian/Documents/teaching/statslab/data/booksales.Rdata\" )\n\nbut this is long and ugly. Given that the working directory is /home/christian/Documents/teaching/statslab, I could use a relative file path, like so:\n\nload( file = \"./data/booksales.Rdata\" )\n\nAnother strategy would be to first change the working directory to whatever directory contains the desired file (setwd() or Session-&gt;Set Working Directory) and then load the file using only the file name (e.g., load(file = \"booksales.Rdata\")). This may seem tempting, but we will avoid doing this because changing the working directory can be confusing if you’re not paying close attention. Instead, we will assume that a) our working directory is the “statslab” directory we created previously (Section 4.3) and b) all of our data files (CSVs and workspaces) are located in the “data” subdirectory. Outside of this class, however, you may not be able to move data files and/or R scripts. In such cases, it may be easier to modify the working directory. Just be aware that you may need to change the working directory both before and after loading the desired file(s). Don’t get lost (you can always figure out what the current working directory is with getwd())!\n\n\n3.1.4 Loading workspace files using RStudio\nOkay, so how do we open an .Rdata file using the RStudio file panel? It’s terribly simple. First, use the file panel to find the folder that contains the file you want to load. If you look at Figure Figure 3.3, you can see that there are several .Rdata files listed. Let’s say I want to load the booksales.Rdata file. All I have to do is click on the file name. RStudio brings up a little dialog box asking me to confirm that I do want to load this file. I click yes. The following command then turns up in the console,\n\nload(\"./data/booksales.Rdata\")\n\nand the new variables will appear in the workspace (you’ll see them in the Environment panel in RStudio, or if you type who()). So easy it barely warrants having its own section.\n\n\n\nFigure 3.3: The file panel is the area shown in the lower right hand corner. It provides a very easy way to browse and navigate your computer using R. See main text for details.\n\n\n\n\n3.1.5 Saving a workspace file using save\nNot surprisingly, saving data is very similar to loading data. Although RStudio provides a simple way to save files (see below), it’s worth understanding the actual commands involved. There are two commands you can use to do this, save() and save.image(). If you’re happy to save all of the variables in your workspace into the data file, then you should use save.image(). And if you’re happy for R to save the file into the current working directory, all you have to do is this:\n\nsave.image( file = \"myfile.Rdata\" )\n\nSince file is the first argument, you can shorten this to save.image(\"myfile.Rdata\"); and if you want to save to a different directory, then (as always) you need to be more explicit about specifying the path to the file. Suppose, however, I have several variables in my workspace, and I only want to save some of them. For instance, I might have this as my workspace:\n\nwho()\n##   -- Name --   -- Class --   -- Size --\n##   data         data.frame    3 x 2     \n##   handy        character     1         \n##   junk         numeric       1        \n\nI want to save data and handy, but not junk. But I don’t want to delete junk right now, because I want to use it for something else later on. This is where the save() function is useful, since it lets me indicate exactly which variables I want to save. Here is one way I can use the save function to solve my problem:\n\nsave(data, handy, file = \"myfile.Rdata\")\n\nImportantly, you must specify the name of the file argument. The reason is that if you don’t do so, R will think that \"myfile.Rdata\" is actually a variable that you want to save, and you’ll get an error message. Finally, I should mention a second way to specify which variables the save() function should save, which is to use the list argument. You do so like this:\n\nvars_to_save &lt;- c(\"data\", \"handy\")   # the variables to be saved\nsave( file = \"booksales2.Rdata\", list = vars_to_save )   # the command to save them\n\n\n\n3.1.6 Saving a workspace file using RStudio\nRStudio allows you to save the workspace pretty easily. In the environment panel ((fig:workspace?)) you can see the “save” button. There’s no text, but it’s the same icon that gets used on every computer everywhere: it’s the one that looks like a floppy disk. You know, those things that haven’t been used in about 20 years.\n\n\n\nFigure 3.4: The RStudio Environment panel shows you the contents of the workspace. The view shown above is the list view. To switch to the grid view, click on the menu item on the top right that currently reads list. Select grid from the dropdown menu, and then it will switch to a view like the one shown in the other workspace figure\n\n\nAlternatively, go to the “Session” menu and click on the “Save Workspace As…” option. This will bring up the standard “save” dialog box for your operating system. Type in the name of the file that you want to save it to, and all the variables in your workspace will be saved to disk. You’ll see an R command like this:\n\nsave.image(\"~/Desktop/Untitled.RData\")\n\nPretty straightforward, really."
  },
  {
    "objectID": "data.html#sec-dataactivities",
    "href": "data.html#sec-dataactivities",
    "title": "3  Working with Data",
    "section": "3.2 Activities",
    "text": "3.2 Activities\n\nDownload data\nDownload the following file: TBD\nInside the “statslab” directory/folder you created at the end of the previous chapter (Section 4.3), create a sub-directory/folder and name it “data”\nUnzip the data files and place them in the “data” directory/folder you just created\nOpen the 2015 data file (nba_all_seasons.csv) in your favorite spreadsheet software (e.g., Microsoft Excel)\nBriefly review the content of this file."
  },
  {
    "objectID": "tidyverse.html#tidy-data-ex",
    "href": "tidyverse.html#tidy-data-ex",
    "title": "4  The Tidyverse",
    "section": "4.1 “Tidy” data",
    "text": "4.1 “Tidy” data\nLet’s now learn about the concept of “tidy” data format.\n\n4.1.1 Definition of “tidy” data\nYou have surely heard the word “tidy” in your life:\n\n“Tidy up your room!”\n“Write your homework in a tidy way so it is easier to provide feedback.”\nMarie Kondo’s best-selling book, The Life-Changing Magic of Tidying Up: The Japanese Art of Decluttering and Organizing, and Netflix TV series Tidying Up with Marie Kondo.\n“I am not by any stretch of the imagination a tidy person, and the piles of unread books on the coffee table and by my bed have a plaintive, pleading quality to me - ‘Read me, please!’” - Linda Grant\n\nWhat does it mean for your data to be “tidy”? While “tidy” has a clear English meaning of “organized,” the word “tidy” in data science using R means that your data follows a standardized format. We will follow Hadley Wickham’s definition of “tidy” data:\n\nA dataset is a collection of values, usually either numbers (if quantitative) or strings AKA text data (if qualitative/categorical). Values are organised in two ways. Every value belongs to a variable and an observation. A variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units. An observation contains all values measured on the same unit (like a person, or a day, or a city) across attributes.\n“Tidy” data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\n\n\n\n\nTidy data\n\n\nFor example, say you have the following table of stock prices:\n\n\n\nStock prices (non-tidy format)\n\n\nDate\nBoeing stock price\nAmazon stock price\nGoogle stock price\n\n\n\n\n2009-01-01\n$173.55\n$174.90\n$174.34\n\n\n2009-01-02\n$172.61\n$171.42\n$170.04\n\n\n\n\n\n\n\nAlthough the data are neatly organized in a rectangular spreadsheet-type format, they do not follow the definition of data in “tidy” format. While there are three variables corresponding to three unique pieces of information (date, stock name, and stock price), there are not three columns. In “tidy” data format, each variable should be its own column. Notice that both tables present the same information, but in different formats.\n\n\n\nStock prices (tidy format)\n\n\nDate\nStock Name\nStock Price\n\n\n\n\n2009-01-01\nBoeing\n$173.55\n\n\n2009-01-01\nAmazon\n$174.90\n\n\n2009-01-01\nGoogle\n$174.34\n\n\n2009-01-02\nBoeing\n$172.61\n\n\n2009-01-02\nAmazon\n$171.42\n\n\n2009-01-02\nGoogle\n$170.04\n\n\n\n\n\n\n\nNow we have the requisite three columns Date, Stock Name, and Stock Price. On the other hand, consider this data:\n\n\n\nExample of tidy data\n\n\nDate\nBoeing Price\nWeather\n\n\n\n\n2009-01-01\n$173.55\nSunny\n\n\n2009-01-02\n$172.61\nOvercast\n\n\n\n\n\n\n\nIn this case, even though the variable “Boeing Price” occurs just like in our non-“tidy” data, the data is “tidy” since there are three variables corresponding to three unique pieces of information: Date, Boeing price, and the Weather that particular day."
  },
  {
    "objectID": "tidyverse.html#tidyverse-package",
    "href": "tidyverse.html#tidyverse-package",
    "title": "4  The Tidyverse",
    "section": "4.2 tidyverse package",
    "text": "4.2 tidyverse package\nThe following four packages, which are among four of the most frequently used R packages for data science, will be heavily used throughout the book:\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(tidyr)\n\nThe ggplot2 package is for data visualization, dplyr is for data wrangling, readr is for importing CSV files into R (we used it in Section 3.1.1), and tidyr is for converting data to “tidy” format. There is a much quicker way to load these packages than by individually loading them: by installing and loading the tidyverse package. The tidyverse package acts as an “umbrella” package whereby installing/loading it will install/load multiple packages at once for you.\nAfter installing the tidyverse package as you would a normal package (see Section 1.5), running:\n\nlibrary(tidyverse)\n\naccomplishes the the same things as running:\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(stringr)\nlibrary(forcats)\n\nThe purrr, tibble, stringr, and forcats are left for a more advanced book; check out R for Data Science to learn about these packages.\nFor the remainder of this book, we’ll start every chapter by running library(tidyverse) instead of loading the various component packages individually. The tidyverse “umbrella” package gets its name from the fact that all the functions in all its packages are designed to have common inputs and outputs: data frames that are in “tidy” format. This standardization of input and output makes transitions between different functions in the different packages as seamless as possible. For more information, check out the tidyverse.org webpage for the package."
  },
  {
    "objectID": "tidyverse.html#sec-rlangactivities",
    "href": "tidyverse.html#sec-rlangactivities",
    "title": "4  The Tidyverse",
    "section": "4.3 Activities",
    "text": "4.3 Activities\n\nOpen RStudio\nIf you have not done so already, install the tidyverse package using the instructions found in Section 1.5.1\nVerify installation of the tidyverse package by loading the package using the instructions found in Section 1.5.2\nCheck out all the cheatsheets available for tidyverse package by going to the Help menu and selecting “Cheat Sheets”. You can look over the readr cheat sheet for now, but remember this resource as we will be diving into the other tidyverse packages (e.g., dplyr, ggplot2) in future chapters."
  },
  {
    "objectID": "tibbles.html#what-is-a-tibble",
    "href": "tibbles.html#what-is-a-tibble",
    "title": "5  Dataframes",
    "section": "5.1 What is a tibble?",
    "text": "5.1 What is a tibble?\nA tibble is a type of dataframe commonly used in the tidyverse. Tibbles contain the same basic information as a corresponding dataframe, but tibbles are slightly different in a variety of minor ways. Let’s take a look at the iris data represented as a tibble. We’ll again confine our inspection to the first five rows:\n\n\n# A tibble: 5 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1          5.1         3.5          1.4         0.2 setosa \n2          4.9         3            1.4         0.2 setosa \n3          4.7         3.2          1.3         0.2 setosa \n4          4.6         3.1          1.5         0.2 setosa \n5          5           3.6          1.4         0.2 setosa \n\n\nNote that the data is identical to that seen above. However, the tibble provides some additional useful information. It tells us that this particular tibble has five columns and, because we only asked for the first five rows, also has five rows. In addition, it tells us about the data type of each column. The first four columns are of type dbl, which is double-precision floating point number (a decimal). The last column is of type fct, or factor. In R, factors are used to represent categorical variables, variables that have a fixed set of possible values (in this case, the set of species)."
  },
  {
    "objectID": "tibbles.html#tidying-your-data",
    "href": "tibbles.html#tidying-your-data",
    "title": "5  Dataframes",
    "section": "5.2 Tidying your data",
    "text": "5.2 Tidying your data\nFor the rest of this book, we will primarily deal with data that is already in “tidy” format as explained Chapter 4. However, many data sets you encounter in the world are in so-called “wide” format. If you wish to use the tidyverse packages, you will first have to convert these date sets to “tidy” format. To do so, we recommend using the pivot_longer() function in the tidyr package.\nTo illustrated, let’s load some data from the fivethirtyeight package. The fivethirtyeight package provides access to the data sets used in many articles published by the data journalism website, FiveThirtyEight.com. For a complete list of all data sets included in the fivethirtyeight package, check out the package webpage.\nLet’s focus our attention on the drinks dataframe:\n\n\n# A tibble: 5 × 5\n  country     beer_servings spirit_servings wine_servings total_litres_of_pure…¹\n  &lt;chr&gt;               &lt;int&gt;           &lt;int&gt;         &lt;int&gt;                  &lt;dbl&gt;\n1 Afghanistan             0               0             0                    0  \n2 Albania                89             132            54                    4.9\n3 Algeria                25               0            14                    0.7\n4 Andorra               245             138           312                   12.4\n5 Angola                217              57            45                    5.9\n# ℹ abbreviated name: ¹​total_litres_of_pure_alcohol\n\n\nAfter reading the help file (i.e., by running ?drinks), you’ll see that drinks is a dataframe containing results from a survey of beer, spirits, and wine consumption originally reported on FiveThirtyEight.com in Mona Chalabi’s article: “Dear Mona Followup: Where Do People Drink The Most Beer, Wine And Spirits?”.\nLet’s narrow down the data a bit. We’ll only consider 4 countries (the United States, China, Italy, and Saudi Arabia), omit the total_litres_of_pure_alcohol variable, and rename the other variables to something a bit more convenient. Don’t worry about the code here. We’ll get into all of these operations more in Chapter 6.\n\ndrinks_smaller &lt;- drinks %&gt;% \n  filter(country %in% c(\"USA\", \"China\", \"Italy\", \"Saudi Arabia\")) %&gt;% \n  select(-total_litres_of_pure_alcohol) %&gt;% \n  rename(beer = beer_servings, spirit = spirit_servings, wine = wine_servings)\ndrinks_smaller\n\n# A tibble: 4 × 4\n  country       beer spirit  wine\n  &lt;chr&gt;        &lt;int&gt;  &lt;int&gt; &lt;int&gt;\n1 China           79    192     8\n2 Italy           85     42   237\n3 Saudi Arabia     0      5     0\n4 USA            249    158    84\n\n\nNote that this data is not in “tidy” format. However, we can convert it to tidy format by using the pivot_longer() function from the tidyr package as follows:\n\ndrinks_smaller_tidy &lt;- drinks_smaller %&gt;% \n  pivot_longer(names_to = \"type\", \n               values_to = \"servings\", \n               cols = -country)\ndrinks_smaller_tidy\n\n# A tibble: 12 × 3\n   country      type   servings\n   &lt;chr&gt;        &lt;chr&gt;     &lt;int&gt;\n 1 China        beer         79\n 2 China        spirit      192\n 3 China        wine          8\n 4 Italy        beer         85\n 5 Italy        spirit       42\n 6 Italy        wine        237\n 7 Saudi Arabia beer          0\n 8 Saudi Arabia spirit        5\n 9 Saudi Arabia wine          0\n10 USA          beer        249\n11 USA          spirit      158\n12 USA          wine         84\n\n\nWe set the arguments to pivot_longer() as follows:\n\nnames_to here corresponds to the name of the variable in the new “tidy”/long dataframe that will contain the column names of the original data. Observe how we set names_to = \"type\". In the resulting drinks_smaller_tidy, the column type contains the three types of alcohol beer, spirit, and wine. Since type is a variable name that doesn’t appear in drinks_smaller, we use quotation marks around it. You’ll receive an error if you just use names_to = type here.\nvalues_to here is the name of the variable in the new “tidy” dataframe that will contain the values of the original data. Observe how we set values_to = \"servings\" since each of the numeric values in each of the beer, wine, and spirit columns of the drinks_smaller data corresponds to a value of servings. In the resulting drinks_smaller_tidy, the column servings contains the 4 \\(\\times\\) 3 = 12 numerical values. Note again that servings doesn’t appear as a variable in drinks_smaller so it again needs quotation marks around it for the values_to argument.\nThe third argument cols is the columns in the drinks_smaller dataframe you either want to or don’t want to “tidy.” Observe how we set this to -country indicating that we don’t want to “tidy” the country variable in drinks_smaller and rather only beer, spirit, and wine. Since country is a column that appears in drinks_smaller we don’t put quotation marks around it.\n\nThe third argument here of cols is a little nuanced, so let’s consider code that’s written slightly differently but that produces the same output:\n\n#|eval: false\ndrinks_smaller %&gt;% \n  pivot_longer(names_to = \"type\", \n               values_to = \"servings\", \n               cols = c(beer, spirit, wine))\n\n# A tibble: 12 × 3\n   country      type   servings\n   &lt;chr&gt;        &lt;chr&gt;     &lt;int&gt;\n 1 China        beer         79\n 2 China        spirit      192\n 3 China        wine          8\n 4 Italy        beer         85\n 5 Italy        spirit       42\n 6 Italy        wine        237\n 7 Saudi Arabia beer          0\n 8 Saudi Arabia spirit        5\n 9 Saudi Arabia wine          0\n10 USA          beer        249\n11 USA          spirit      158\n12 USA          wine         84\n\n\nNote that the third argument now specifies which columns we want to “tidy” with c(beer, spirit, wine), instead of the columns we don’t want to “tidy” using -country. We use the c() function to create a vector of the columns in drinks_smaller that we’d like to “tidy.” Note that since these three columns appear one after another in the drinks_smaller dataframe, we could also do the following for the cols argument:\n\ndrinks_smaller %&gt;% \n  pivot_longer(names_to = \"type\", \n               values_to = \"servings\", \n               cols = beer:wine)\n\n# A tibble: 12 × 3\n   country      type   servings\n   &lt;chr&gt;        &lt;chr&gt;     &lt;int&gt;\n 1 China        beer         79\n 2 China        spirit      192\n 3 China        wine          8\n 4 Italy        beer         85\n 5 Italy        spirit       42\n 6 Italy        wine        237\n 7 Saudi Arabia beer          0\n 8 Saudi Arabia spirit        5\n 9 Saudi Arabia wine          0\n10 USA          beer        249\n11 USA          spirit      158\n12 USA          wine         84\n\ndrinks_smaller_tidy &lt;- drinks_smaller %&gt;%\n  gather(type, servings, -country)\n\nLet’s see what our “tidy” formatted dataframe, drinks_smaller_tidy, looks like.\n\n\n# A tibble: 12 × 3\n   country      type   servings\n   &lt;chr&gt;        &lt;chr&gt;     &lt;int&gt;\n 1 China        beer         79\n 2 Italy        beer         85\n 3 Saudi Arabia beer          0\n 4 USA          beer        249\n 5 China        spirit      192\n 6 Italy        spirit       42\n 7 Saudi Arabia spirit        5\n 8 USA          spirit      158\n 9 China        wine          8\n10 Italy        wine        237\n11 Saudi Arabia wine          0\n12 USA          wine         84\n\n\nConverting “wide” format data to “tidy” format often confuses new R users. The only way to learn to get comfortable with the pivot_longer() function is with practice, practice, and more practice using different data sets. For example, run ?pivot_longer and look at the examples in the bottom of the help file.\nIf however you want to convert a “tidy” dataframe to “wide” format, you will need to use the pivot_wider() function instead. Run ?pivot_wider and look at the examples in the bottom of the help file for examples.\nYou can also view examples of both pivot_longer() and pivot_wider() on the tidyverse.org webpage. There’s a nice example to check out the different functions available for data tidying and a case study using data from the World Health Organization on that webpage. Furthermore, each week the R4DS Online Learning Community posts a data set in the weekly #TidyTuesday event that might serve as a nice place for you to find other data to explore and transform."
  },
  {
    "objectID": "tibbles.html#sec-tibblesactivities",
    "href": "tibbles.html#sec-tibblesactivities",
    "title": "5  Dataframes",
    "section": "5.3 Activities",
    "text": "5.3 Activities\n\nLet’s load data we downloaded back in Section 3.2. Run the following:\n\n\nnba &lt;- read_csv(\"./data/nba_all_seasons.csv\")\n\n\nNow let’s take a quick peek at what’s in there by running the following:\n\n\nhead(nba)\n\n\nConsider the following questions:\n\nHow many columns are there in nba?\nHow many rows are there in nba?\nWhat is the data type of the player_height column?\nWhat is the data type of the player_weight column?\nWhat is the data type of the pts column?\nWhat is the data type of the draft_year column?\nWhat is the data type of the draft_round column?\nWhat is the data type of the draft_number column?\n\nLet’s take a moment to talk about what this data set contains. This data reflects statistics for players in the National Basketball Association (NBA). The data covers seasons from 1996-1997 to 2021-2022 (NBA seasons run from approximately October to June of the following year). Each row represents a player . Columns include each player’s name (player_name), the team the player played for (team_abbreviation), where the player went to college (college), the player’s height (player_height) and weight (player_weight), the year, round, and order in which the player was drafted (draft_year, draft_round, and draft_number) as well as a variety of statistics about the player’s performance (e.g., points scored, pts).\n\n\nDo any of these data types seem incorrect? Which ones?\nLet’s take a closer look. Open the data in RStudio’s spreadsheet viewer by running the following:\n\n\nView(nba)\n\n\nInspect the college column. Do you see any unusual values?\nGiven what you see in the tibble and the description above, do you think this data is in tidy format?"
  },
  {
    "objectID": "dplyr.html#sec-whatisdplyr",
    "href": "dplyr.html#sec-whatisdplyr",
    "title": "6  Manipulating Data",
    "section": "6.1 What is dplyr",
    "text": "6.1 What is dplyr\nIn the tidyverse, the package responsible for such activities is called dplyr. This package contains a set of functions, each of which manipulates data in a particular way. Each of these functions is very useful on its own. However, the real power of dplyr comes from the fact that you can repeatedly applying dplyr functions, each operating on the result from the previous. These “chains” of functions allow you to compose complex data manipulation operations that ultimately transform your data into whatever you need. For this reason, dplyr is sometimes said to instantiate a “grammar” of data manipulation, a grammar which is made up of several “verbs” (functions). These function include:\n\nfilter(): select a subset of rows from a data frame\narrange(): sort a data frame’s rows\nmutate() create new columns/variables based on existing columns/variables\nsummarize(): aggregate one or more columns/variables with a summary statistic (e.g., the mean)\ngroup_by(): assign rows to groups, such that each group shares some values in common\n\nBecause dplyr is part of the tidyverse, these all work similarly. Specifically, each dplyr function:\n\nTakes a dataframe as its first argument\nTakes additional arguments that often indicate which columns are to be operated on\nReturns a modified dataframe\n\nLet’s see some of these characteristics in action. To do so, we’ll first load the data we downloaded back in Section 3.2. Run the following:\n\nnba &lt;- read_csv(\"./data/nba_all_seasons.csv\", na = c(\"Undrafted\"))\n\nNote that we have utilized the na argument here. This tells read_csv that we would like any values found in the file that match \"Undrafted\" to be treated as “missing”. R uses NA to represent missing values and so any \"Undrafted\" values will be converted in to NA."
  },
  {
    "objectID": "dplyr.html#filter",
    "href": "dplyr.html#filter",
    "title": "6  Manipulating Data",
    "section": "6.2 filter",
    "text": "6.2 filter\nThe filter() function allows you to specify criteria about the values of a variable in your data set and then filters out only the rows that match that criteria.\nThe team_abbreviation for the New York Knicks is \"NYK\". Run the following and look at the results in RStudio’s spreadsheet viewer to ensure that only players on the Knicks heading to Portland are chosen here:\n\nfilter(nba, team_abbreviation==\"NYK\")\n\n# A tibble: 410 × 22\n    ...1 player_name team_abbreviation   age player_height player_weight college\n   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;             &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;  \n 1    81 Herb Willi… NYK                  39          211.         118.  Ohio S…\n 2   146 Allan Hous… NYK                  26          198.          90.7 Tennes…\n 3   195 Buck Willi… NYK                  37          203.         102.  Maryla…\n 4   204 Charles Oa… NYK                  33          206.         111.  Virgin…\n 5   209 Chris Chil… NYK                  29          190.          88.5 Boise …\n 6   212 Chris Jent  NYK                  27          201.          99.8 Ohio S…\n 7   218 Charlie Wa… NYK                  26          188.          86.2 Florid…\n 8   230 Scott Broo… NYK                  31          180.          74.8 Califo…\n 9   293 Walter McC… NYK                  23          208.         104.  Kentuc…\n10   347 Larry John… NYK                  28          201.         119.  Nevada…\n# ℹ 400 more rows\n# ℹ 15 more variables: country &lt;chr&gt;, draft_year &lt;dbl&gt;, draft_round &lt;dbl&gt;,\n#   draft_number &lt;dbl&gt;, gp &lt;dbl&gt;, pts &lt;dbl&gt;, reb &lt;dbl&gt;, ast &lt;dbl&gt;,\n#   net_rating &lt;dbl&gt;, oreb_pct &lt;dbl&gt;, dreb_pct &lt;dbl&gt;, usg_pct &lt;dbl&gt;,\n#   ts_pct &lt;dbl&gt;, ast_pct &lt;dbl&gt;, season &lt;chr&gt;\n\n\nIf you prefer a more thorough inspection, you can open the data in RStudio’s spreadsheet viewer:\n\nView(filter(nba, team_abbreviation==\"NYK\"))\n\nIn either case, we are asking for a test of equality, keeping any rows where team_abbreviation==\"NYK\" is true and removing (filtering) any rows where team_abbreviation==\"NYK\" is false. To do so, we use the double equal sign ==, not a single equal sign =. In other words filter(nba, team_abbreviation = \"NYK\") will yield an error. This is a convention across many programming languages. If you are new to coding, you’ll probably forget to use the double equal sign == a few times before you get the hang of it.\nThe equality operator is not the only operator available to us. Others include:\n\n&gt; corresponds to “greater than”\n&lt; corresponds to “less than”\n&gt;= corresponds to “greater than or equal to”\n&lt;= corresponds to “less than or equal to”\n!= corresponds to “not equal to.” The ! is used in many programming languages to indicate “not.”\n\nFurthermore, you can combine multiple criteria using operators that make comparisons:\n\n| corresponds to “or”\n& corresponds to “and”\n\nLet’s look at an example that uses the ! “not” operator to pick rows that don’t match a criteria. As mentioned earlier, the ! can be read as “not.” Here we are filtering rows corresponding to players thare are not from the United States:\n\nfilter(nba, country!=\"USA\")\n\nLet’s combine two different requirements using the |:\n\nfilter(nba, country==\"Jamaica\" | college==\"Michigan\")\n\nThis will select players that are either from Jamica or went to Michigan (or both). We can also request the inverse of this.\n\nfilter(nba, !(country==\"Jamaica\" | college==\"Michigan\"))\n\nThis will select players that are not from Jamica and those players that did not go to Michigan. Note the use of parentheses around `(country==\"Jamaica\" | college==\"Michigan\"). If we used the !, but did not use the parentheses, we would only applying the “not” to the first test (country==“Jamaica”), not to the combination of (country==\"Jamaica\" | college==\"Michigan\"). You can try it and compare the results:\n\nfilter(nba, !country==\"Jamaica\" | college==\"Michigan\")\n\nThis request, in contrast to the one above, is for players that are not from Jamaica or went to Michigan (or both). So be very careful about the order of operations and use parentheses liberally. It helps to minimize errors and makes your code more explicit and therefore more readable.\nLet’s see a slightly more complicated request that combines several different operators:\n\nfilter(nba, team_abbreviation==\"NYK\" & (country!=\"USA\" | college==\"Michigan\") & age &gt;= 25)\n\nHere we have filtered the data to retain all rows corresponding to players from the NY Knicks, who are age 25 or older, and either went to Michigan or are not from the United States. You can this this yourself and verify that the output matches these requirements (remember that you can use View() if you wish to inspect the entire result).\nLet’s request players that went to either Michigan, Duke, or Georgetown.\n\nfilter(nba, college==\"Michigan\" | college==\"Duke\" | college==\"Georgetown\")\n\nThis works, but as we progressively include more collegs, this will get unwieldy to write. A slightly shorter approach uses the %in% operator along with the c() function. Recall from Subsection Section 2.5 that the c() function “combines” or “concatenates” values into a single vector of values.\n\nfilter(nba, college %in% c(\"Michigan\", \"Duke\", \"Georgetown\", \"UCLA\", \"Kentucky\"))\n\nWhat this code is doing is filtering our for all flights where college is in the vector of airports c(\"Michigan\", \"Duke\", \"Georgetown\", \"UCLA\", \"Kentucky\"). This approach produces results that are similar to a sequence of | operators, but takes much less energy to write (and read). The %in% operator is useful for looking for matches commonly in one vector/variable compared to another.\nAs a final note, we recommend that filter() should often be among the first tidyverse “verbs” you consider applying to your data. This cleans your dataset to only those rows you care about, or to put it another way, it narrows down the scope of your data to just the observations you care about."
  },
  {
    "objectID": "dplyr.html#pipe",
    "href": "dplyr.html#pipe",
    "title": "6  Manipulating Data",
    "section": "6.3 pipe",
    "text": "6.3 pipe\nBefore we go any further, let’s first introduce a nifty tool that gets loaded with the dplyr package: the pipe operator %&gt;%. The pipe operator allows us to combine multiple tidyverse operations into a single sequential chain of actions.\nLet’s start with a hypothetical example. Say you would like to perform a hypothetical sequence of operations on a hypothetical data frame x using hypothetical functions f(), g(), and h():\n\nTake x then\nUse x as an input to a function f() then\nUse the output of f(x) as an input to a function g() then\nUse the output of g(f(x)) as an input to a function h()\n\nOne way to achieve this sequence of operations is by using nesting parentheses as follows:\n\nh(g(f(x)))\n\nThis code isn’t so hard to read since we are applying only three functions: f(), then g(), then h() and each of the functions has a short name. Further, each of these functions also only has one argument. However, you can imagine that this will get progressively more difficult to read as the number of functions applied in your sequence increases and the arguments in each function grow more numerous. This is where the pipe operator %&gt;% comes in handy. The pipe takes the output of one function and then “pipes” it to be the input of the next function. Furthermore, a helpful trick is to read %&gt;% as “then” or “and then”. For example, you can obtain the same output as the hypothetical sequence of functions as follows:\n\nx %&gt;% \n  f() %&gt;% \n  g() %&gt;% \n  h()\n\nYou would read this sequence as:\n\nTake x then\nUse this output as the input to the next function f() then\nUse this output as the input to the next function g() then\nUse this output as the input to the next function h()\n\nThough both approaches achieve the same goal, the latter is much more human-readable because you can clearly read the sequence of operations line-by-line. But what are the hypothetical x, f(), g(), and h()? Throughout this chapter on data wrangling:\n\nThe starting value, x, will be a data frame. For example, the nba data frame we have been exploring so far.\nThe sequence of functions, here f(), g(), and h(), will mostly be a sequence of any number of the data wrangling verb-named functions we listed above. For example, the filter(college == \"Michigan\") function and argument specified we previewed earlier.\nThe result will be the transformed/modified data frame that you want.\n\nSo instead of\n\nfilter(nba, team_abbreviation==\"NYK\")\n\nwe can instead write:\n\nnba %&gt;%\n    filter(team_abbreviation==\"NYK\")\n\nThe benefits of this may not be immediately obvious. But the ability to form a chain of data wrangling operations by combining tidyverse functions (verbs) into a single sequence will be utilized extensively and is made possible by the pipe operator %&gt;%."
  },
  {
    "objectID": "dplyr.html#sec-summarize",
    "href": "dplyr.html#sec-summarize",
    "title": "6  Manipulating Data",
    "section": "6.4 summarize",
    "text": "6.4 summarize\nThe next common task when working with data frames is to compute summary statistics. Summary statistics are single numerical values that summarize a large number of values. Commonly known examples of summary statistics include the mean (i.e., the “average”), the median, and the mode. Other examples of summary statistics that might not immediately come to mind include the sum, the smallest value also called the minimum, the largest value also called the maximum, and the standard deviation.\nLet’s calculate two summary statistics of the draft_round variable in the nba data frame: the mean and standard deviation. To compute these summary statistics, we need the mean() and sd() summary functions in R. Summary functions in R take in many values and return a single value. More precisely, we’ll use the mean() and sd() summary functions within the summarize() function from the dplyr package. The summarize() function takes in a data frame and returns a data frame with only one row corresponding to the value of the summary statistic(s).\nWe’ll save the results in a new data frame called summary_round that will have two columns/variables: the mean and the std_dev:\n\nsummary_round &lt;- nba %&gt;% \n  summarize(mean = mean(draft_round), std_dev = sd(draft_round))\nsummary_round\n\n# A tibble: 1 × 2\n   mean std_dev\n  &lt;dbl&gt;   &lt;dbl&gt;\n1    NA      NA\n\n\nWhy are the values returned NA? NA is how R encodes missing values where NA indicates “not available” or “not applicable.” If a value for a particular row and a particular column does not exist, NA is stored instead. Values can be missing for many reasons. Perhaps the data was collected but someone forgot to enter it. Perhaps the data was not collected at all because it was too difficult to do so. Perhaps there was an erroneous value that someone entered that has been corrected to read as missing. You’ll often encounter issues with missing values when working with real data.\nGoing back to our summary_round output, by default any time you try to calculate a summary statistic of a variable that has one or more NA missing values in R, NA is returned. If what you wish to calculate is the summary of all the valid, non-missing values, you can set the na.rm argument to TRUE, where rm is short for “remove”. The code that follows computes the mean and standard deviation of all non-missing values of age:\n\nsummary_round &lt;- nba %&gt;% \n  summarize(mean = mean(draft_round, na.rm = TRUE), \n            std_dev = sd(draft_round, na.rm = TRUE))\nsummary_round\n\n# A tibble: 1 × 2\n   mean std_dev\n  &lt;dbl&gt;   &lt;dbl&gt;\n1  1.30   0.507\n\n\nNotice how the na.rm = TRUE are used as arguments to the mean() and sd() summary functions individually, and not to the summarize() function.\nHowever, one needs to be cautious whenever ignoring missing values as we’ve just done. We will consider the possible ramifications of blindly sweeping rows with missing values “under the rug”. This is in fact why the na.rm argument to any summary statistic function in R is set to FALSE by default. In other words, R does not ignore rows with missing values by default. R is alerting you to the presence of missing data and you should be mindful of this missingness and any potential causes of this missingness throughout your analysis.\nWhat are other summary functions we can use inside the summarize() verb to compute summary statistics? You can use any function in R that takes many values and returns just one. Here are just a few:\n\nmean(): the average\nsd(): the standard deviation, which is a measure of spread\nmin() and max(): the minimum and maximum values, respectively\nIQR(): interquartile range\nsum(): the total amount when adding multiple numbers\nn(): a count of the number of rows in each group. This particular summary function will make more sense when group_by() is covered below.\n\n\n\n\n\n\n\nSometimes missingness is data\n\n\n\nImagine a public health researcher is studying the effect of smoking on lung cancer for a large number of patients who are observed every five years. She notices that a large number of patients have missing data points, particularly in later observations. The researcher takes the approach outlined above, choosing to ignore these patients in her analysis. How might this be misleading?\n\n\nLet’s see just a couple more examples of summaries. Here we ask for the number of unique college that appear in the data set:\n\nnba %&gt;%\n    summarise(n_colleges = n_distinct(college))\n\n# A tibble: 1 × 1\n  n_colleges\n       &lt;int&gt;\n1        345\n\n\nHere we combine a filter operation with a summarize operation to calculate the average number of points scored by players in the 2000-2001 NBA season:\n\nnba %&gt;%\n    filter(season == \"2000-01\") %&gt;%\n    summarize(total_points = mean(pts))\n\n# A tibble: 1 × 1\n  total_points\n         &lt;dbl&gt;\n1         7.81"
  },
  {
    "objectID": "dplyr.html#sec-groupby",
    "href": "dplyr.html#sec-groupby",
    "title": "6  Manipulating Data",
    "section": "6.5 group_by",
    "text": "6.5 group_by\nSay instead of a single mean number of points for the entire NBA, you would like a mean number of points separately for each college players attended. In other words, we would like to compute the mean number of points split by college. We can do this by “grouping” the pts measurements by the values of another variable, in this case by the values of the variable college . Run the following code:\n\nnba %&gt;%\n    group_by(college) %&gt;% \n    summarize(mean = mean(pts),\n            std_dev = sd(pts))\n\n# A tibble: 345 × 3\n   college                   mean std_dev\n   &lt;chr&gt;                    &lt;dbl&gt;   &lt;dbl&gt;\n 1 \"\"                        9.9    3.79 \n 2 \"Alabama\"                 9.21   5.83 \n 3 \"Alabama A&M\"             2.2   NA    \n 4 \"Alabama Huntsville\"      2.05   0.778\n 5 \"Alabama-Birmingham\"      2.4    1.42 \n 6 \"Albany State (GA)\"       0.45   0.212\n 7 \"American International\"  8.34   2.71 \n 8 \"American University\"     6      8.49 \n 9 \"Arizona\"                 8.93   5.83 \n10 \"Arizona St.\"             0     NA    \n# ℹ 335 more rows\n\n\nThis code is similar to the previous code that created summary_round, but with an extra group_by(month) added before the summarize(). Grouping the nba dataset by college and then applying the summarize() functions yields a data frame that displays the mean and standard deviation temperature split by the different colleges that appear in the data set.\nIt is important to note that the group_by() function doesn’t change data frames by itself. Rather it changes the meta-data, or data about the data, specifically the grouping structure. It is only after we apply the summarize() function that the data frame changes.\nFor example, when we run this code:\n\nnba\n\n# A tibble: 12,305 × 22\n    ...1 player_name team_abbreviation   age player_height player_weight college\n   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;             &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;  \n 1     0 Dennis Rod… CHI                  36          198.          99.8 Southe…\n 2     1 Dwayne Sch… LAC                  28          216.         118.  Florida\n 3     2 Earl Curet… TOR                  39          206.          95.3 Detroi…\n 4     3 Ed O'Bannon DAL                  24          203.         101.  UCLA   \n 5     4 Ed Pinckney MIA                  34          206.         109.  Villan…\n 6     5 Eddie John… HOU                  38          201.          97.5 Illino…\n 7     6 Eddie Jones LAL                  25          198.          86.2 Temple \n 8     7 Elden Camp… LAL                  28          213.         113.  Clemson\n 9     8 Eldridge R… ATL                  29          193.          86.2 Washin…\n10     9 Elliot Per… MIL                  28          183.          72.6 Memphis\n# ℹ 12,295 more rows\n# ℹ 15 more variables: country &lt;chr&gt;, draft_year &lt;dbl&gt;, draft_round &lt;dbl&gt;,\n#   draft_number &lt;dbl&gt;, gp &lt;dbl&gt;, pts &lt;dbl&gt;, reb &lt;dbl&gt;, ast &lt;dbl&gt;,\n#   net_rating &lt;dbl&gt;, oreb_pct &lt;dbl&gt;, dreb_pct &lt;dbl&gt;, usg_pct &lt;dbl&gt;,\n#   ts_pct &lt;dbl&gt;, ast_pct &lt;dbl&gt;, season &lt;chr&gt;\n\n\nObserve that the first line of the output reads # A tibble: 12305 x 22. This is an example of meta-data, in this case the number of observations/rows and variables/columns in nba. The actual data itself are the subsequent table of values. Now let’s pipe the nba data frame into group_by(college):\n\nnba %&gt;% \n  group_by(college)\n\n# A tibble: 12,305 × 22\n# Groups:   college [345]\n    ...1 player_name team_abbreviation   age player_height player_weight college\n   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;             &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;  \n 1     0 Dennis Rod… CHI                  36          198.          99.8 Southe…\n 2     1 Dwayne Sch… LAC                  28          216.         118.  Florida\n 3     2 Earl Curet… TOR                  39          206.          95.3 Detroi…\n 4     3 Ed O'Bannon DAL                  24          203.         101.  UCLA   \n 5     4 Ed Pinckney MIA                  34          206.         109.  Villan…\n 6     5 Eddie John… HOU                  38          201.          97.5 Illino…\n 7     6 Eddie Jones LAL                  25          198.          86.2 Temple \n 8     7 Elden Camp… LAL                  28          213.         113.  Clemson\n 9     8 Eldridge R… ATL                  29          193.          86.2 Washin…\n10     9 Elliot Per… MIL                  28          183.          72.6 Memphis\n# ℹ 12,295 more rows\n# ℹ 15 more variables: country &lt;chr&gt;, draft_year &lt;dbl&gt;, draft_round &lt;dbl&gt;,\n#   draft_number &lt;dbl&gt;, gp &lt;dbl&gt;, pts &lt;dbl&gt;, reb &lt;dbl&gt;, ast &lt;dbl&gt;,\n#   net_rating &lt;dbl&gt;, oreb_pct &lt;dbl&gt;, dreb_pct &lt;dbl&gt;, usg_pct &lt;dbl&gt;,\n#   ts_pct &lt;dbl&gt;, ast_pct &lt;dbl&gt;, season &lt;chr&gt;\n\n\nObserve that now there is additional meta-data: # Groups:   college [345] indicating that the grouping structure meta-data has been set based on the unique values of the variable college. On the other hand, observe that the data has not changed: it is still a table of 12305 \\(\\times\\) 22 values.\nOnly by combining a group_by() with another data wrangling operation, in this case summarize(), will the data actually be transformed.\n\nnba %&gt;% \n  group_by(college) %&gt;% \n  summarize(mean = mean(pts))\n\n# A tibble: 345 × 2\n   college                   mean\n   &lt;chr&gt;                    &lt;dbl&gt;\n 1 \"\"                        9.9 \n 2 \"Alabama\"                 9.21\n 3 \"Alabama A&M\"             2.2 \n 4 \"Alabama Huntsville\"      2.05\n 5 \"Alabama-Birmingham\"      2.4 \n 6 \"Albany State (GA)\"       0.45\n 7 \"American International\"  8.34\n 8 \"American University\"     6   \n 9 \"Arizona\"                 8.93\n10 \"Arizona St.\"             0   \n# ℹ 335 more rows\n\n\nIf you would like to remove this grouping structure meta-data, we can pipe the resulting data frame into the ungroup() function:\n\nnba %&gt;% \n  group_by(college) %&gt;% \n  ungroup()\n\n# A tibble: 12,305 × 22\n    ...1 player_name team_abbreviation   age player_height player_weight college\n   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;             &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;  \n 1     0 Dennis Rod… CHI                  36          198.          99.8 Southe…\n 2     1 Dwayne Sch… LAC                  28          216.         118.  Florida\n 3     2 Earl Curet… TOR                  39          206.          95.3 Detroi…\n 4     3 Ed O'Bannon DAL                  24          203.         101.  UCLA   \n 5     4 Ed Pinckney MIA                  34          206.         109.  Villan…\n 6     5 Eddie John… HOU                  38          201.          97.5 Illino…\n 7     6 Eddie Jones LAL                  25          198.          86.2 Temple \n 8     7 Elden Camp… LAL                  28          213.         113.  Clemson\n 9     8 Eldridge R… ATL                  29          193.          86.2 Washin…\n10     9 Elliot Per… MIL                  28          183.          72.6 Memphis\n# ℹ 12,295 more rows\n# ℹ 15 more variables: country &lt;chr&gt;, draft_year &lt;dbl&gt;, draft_round &lt;dbl&gt;,\n#   draft_number &lt;dbl&gt;, gp &lt;dbl&gt;, pts &lt;dbl&gt;, reb &lt;dbl&gt;, ast &lt;dbl&gt;,\n#   net_rating &lt;dbl&gt;, oreb_pct &lt;dbl&gt;, dreb_pct &lt;dbl&gt;, usg_pct &lt;dbl&gt;,\n#   ts_pct &lt;dbl&gt;, ast_pct &lt;dbl&gt;, season &lt;chr&gt;\n\n\nObserve how the # Groups:   college [345] meta-data is no longer present.\nLet’s now revisit the n() counting summary function we briefly introduced previously. Recall that the n() function counts rows. This is opposed to the sum() summary function that returns the sum of a numerical variable. For example, suppose we’d like to count how many NBA players went to each of the different colleges, we can run this:\n\nby_college &lt;- nba %&gt;% \n  group_by(college) %&gt;% \n  summarize(count = n())\nby_college\n\n# A tibble: 345 × 2\n   college                  count\n   &lt;chr&gt;                    &lt;int&gt;\n 1 \"\"                           5\n 2 \"Alabama\"                  119\n 3 \"Alabama A&M\"                1\n 4 \"Alabama Huntsville\"         2\n 5 \"Alabama-Birmingham\"         7\n 6 \"Albany State (GA)\"          2\n 7 \"American International\"     5\n 8 \"American University\"        2\n 9 \"Arizona\"                  279\n10 \"Arizona St.\"                1\n# ℹ 335 more rows\n\n\nWe see that there are 119 rows in which college==\"Alabama\" and 279 rows in which college==\"Arizona\". Note there is a subtle but important difference between sum() and n(); while sum() returns the sum of a numerical variable (adding), n() returns a count of the number of rows/observations (counting).\n\n6.5.1 Grouping by more than one variable\nYou are not limited to grouping by one variable. Say you want to know the number of players coming from each college for each NBA season. We can also group by a second variable season using group_by(college, season):\n\nby_college_annually &lt;- nba %&gt;% \n  group_by(college, season) %&gt;% \n  summarize(count = n())\n\n`summarise()` has grouped output by 'college'. You can override using the\n`.groups` argument.\n\nby_college_annually\n\n# A tibble: 3,637 × 3\n# Groups:   college [345]\n   college   season  count\n   &lt;chr&gt;     &lt;chr&gt;   &lt;int&gt;\n 1 \"\"        2017-18     2\n 2 \"\"        2018-19     2\n 3 \"\"        2020-21     1\n 4 \"Alabama\" 1996-97     9\n 5 \"Alabama\" 1997-98    10\n 6 \"Alabama\" 1998-99     8\n 7 \"Alabama\" 1999-00     6\n 8 \"Alabama\" 2000-01     7\n 9 \"Alabama\" 2001-02     6\n10 \"Alabama\" 2002-03     4\n# ℹ 3,627 more rows\n\n\nObserve that there are now 3637 rows to by_college_annually because there are 12305 unique colleges.\nWhy do we group_by(college, season) and not group_by(college) and then group_by(season)? Let’s investigate:\n\nby_college_annually_incorrect &lt;- nba %&gt;% \n  group_by(college) %&gt;% \n  group_by(season) %&gt;% \n  summarize(count = n())\nby_college_annually_incorrect\n\n# A tibble: 26 × 2\n   season  count\n   &lt;chr&gt;   &lt;int&gt;\n 1 1996-97   441\n 2 1997-98   439\n 3 1998-99   439\n 4 1999-00   438\n 5 2000-01   441\n 6 2001-02   440\n 7 2002-03   428\n 8 2003-04   442\n 9 2004-05   464\n10 2005-06   458\n# ℹ 16 more rows\n\n\nWhat happened here is that the second group_by(season) overwrote the grouping structure meta-data of the earlier group_by(college), so that in the end we are only grouping by season. The lesson here is if you want to group_by() two or more variables, you should include all the variables at the same time in the same group_by() adding a comma between the variable names."
  },
  {
    "objectID": "dplyr.html#sec-mutate",
    "href": "dplyr.html#sec-mutate",
    "title": "6  Manipulating Data",
    "section": "6.6 mutate",
    "text": "6.6 mutate\nAnother common transformation of data is to create/compute new variables based on existing ones. For example, the heights in our nba data set are measured in units of centimeters. But say you are more comfortable thinking of inches instead of centimeters. The formula to convert centimeters to inches is:\n\\[\n\\text{height in inches} = \\frac{\\text{height in centimeters.}}{2.54}\n\\]\nWe can apply this formula to the player_height variable using the mutate() function from the dplyr package, which takes existing variables and mutates them to create new ones.\n\nnba &lt;- nba %&gt;% \n  mutate(player_height_in_inch = player_height / 2.54)\n\nIn this code, we mutate() the nba data frame by creating a new variable player_height_in_inch = height / 2.54 and then overwrite the original nba data frame. Why did we overwrite the data frame nba, instead of assigning the result to a new data frame like nba_new? As a rough rule of thumb, as long as you are not losing original information that you might need later, it’s acceptable practice to overwrite existing data frames with updated ones, as we did here. On the other hand, why did we not overwrite the variable player_height, but instead created a new variable called player_height_in_inch? Because if we did this, we would have erased the original information contained in player_height of temperatures in centimeters that may still be valuable to us.\nLet’s now compute average heights in both inches and centimeters using the group_by() and summarize():\n\nsummary_college_height &lt;- nba %&gt;%\n    group_by(college) %&gt;% \n    summarize(mean_height_in_inch_in_cm = mean(player_height),\n              mean_height_in_inch_in_in = mean(player_height_in_inch))\nsummary_college_height\n\n# A tibble: 345 × 3\n   college                  mean_height_in_inch_in_cm mean_height_in_inch_in_in\n   &lt;chr&gt;                                        &lt;dbl&gt;                     &lt;dbl&gt;\n 1 \"\"                                            204.                      80.2\n 2 \"Alabama\"                                     200.                      78.6\n 3 \"Alabama A&M\"                                 211.                      83  \n 4 \"Alabama Huntsville\"                          185.                      73  \n 5 \"Alabama-Birmingham\"                          196.                      77.1\n 6 \"Albany State (GA)\"                           206.                      81  \n 7 \"American International\"                      196.                      77  \n 8 \"American University\"                         190.                      75  \n 9 \"Arizona\"                                     198.                      77.8\n10 \"Arizona St.\"                                 196.                      77  \n# ℹ 335 more rows\n\n\nLet’s consider another example. We can imagine placing players on a continuum, with one end representing the “star” players and the other end representing “support” players. We can quantify this dimension by by comparing a player’s average points per game (pts) to his average assists per game (ast). Let’s calculate this new variable using the mutate() function:\n\nnba &lt;- nba %&gt;% \n  mutate(star_support = pts - ast)\n\nLet’s take a look at only the pts, ast, and the resulting star_support variables for a few rows in our updated nba data frame.\n\n\n# A tibble: 5 × 3\n    pts   ast star_support\n  &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;\n1   5.7   3.1          2.6\n2   2.3   0.3          2  \n3  17.2   3.4         13.8\n4   1.5   2.9         -1.4\n5   3.7   0.6          3.1\n\n\nThe player in the third row scored an average of 17.2 points per game and made an average of 3.4 assists per game, so his star_support value is \\(17.2-3.5=13.8\\). On the other hand, the player in the fourth row averaged 1.5 points and 2.9 assists, so its star_support value is \\(1.5 - 2.9 = -1.4\\).\nLet’s look at some summary statistics of the star_support variable by considering multiple summary functions at once in the same summarize() code:\n\nstar_support_summary &lt;- nba %&gt;% \n  summarize(\n    min = min(star_support, na.rm = TRUE),\n    q1 = quantile(star_support, 0.25, na.rm = TRUE),\n    median = quantile(star_support, 0.5, na.rm = TRUE),\n    q3 = quantile(star_support, 0.75, na.rm = TRUE),\n    max = max(star_support, na.rm = TRUE),\n    mean = mean(star_support, na.rm = TRUE),\n    sd = sd(star_support, na.rm = TRUE),\n    missing = sum(is.na(star_support))\n  )\nstar_support_summary\n\n# A tibble: 1 × 8\n    min    q1 median    q3   max  mean    sd missing\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n1  -2.5   2.6    5.1     9  30.9  6.36  4.97       0\n\n\nWe see for example that the average value is 6.36 minutes, whereas the largest is 30.9! This summary contains quite a bit of information. However, it is often easier to visualize data to evaluate how values are distributed. We’ll take a look at that in Chapter 7.\nTo close out our discussion on the mutate() function to create new variables, note that we can create multiple new variables at once in the same mutate() code. Furthermore, within the same mutate() code we can refer to new variables we just created. Consider following:\n\nnba &lt;- nba %&gt;% \n  mutate(\n    player_height_in_inch = player_height / 2.54,\n    player_weight_in_lbs = player_weight / 2.2,\n    bmi = 703 * (player_weight_in_lbs / (player_height_in_inch^2)),\n  )"
  },
  {
    "objectID": "dplyr.html#arrange",
    "href": "dplyr.html#arrange",
    "title": "6  Manipulating Data",
    "section": "6.7 arrange",
    "text": "6.7 arrange\nOne of the most commonly performed data wrangling tasks is to sort a data frame’s rows in the alphanumeric order of one of the variables. The dplyr package’s arrange() function allows us to sort/reorder a data frame’s rows according to the values of the specified variable.\nSuppose we are interested in determining the average number of points per game scored by NBA players from different colleges:\n\nmean_pts &lt;- nba %&gt;% \n  group_by(college) %&gt;% \n  summarize(mean = mean(pts), \n            std_dev = sd(pts))\nmean_pts\n\n# A tibble: 345 × 3\n   college                   mean std_dev\n   &lt;chr&gt;                    &lt;dbl&gt;   &lt;dbl&gt;\n 1 \"\"                        9.9    3.79 \n 2 \"Alabama\"                 9.21   5.83 \n 3 \"Alabama A&M\"             2.2   NA    \n 4 \"Alabama Huntsville\"      2.05   0.778\n 5 \"Alabama-Birmingham\"      2.4    1.42 \n 6 \"Albany State (GA)\"       0.45   0.212\n 7 \"American International\"  8.34   2.71 \n 8 \"American University\"     6      8.49 \n 9 \"Arizona\"                 8.93   5.83 \n10 \"Arizona St.\"             0     NA    \n# ℹ 335 more rows\n\n\nObserve that by default the rows of the resulting mean_pts data frame are sorted in alphabetical order of college. Say instead we would like to see the same data, but sorted from the highest to the lowest average points (mean) instead:\n\nmean_pts %&gt;% \n  arrange(mean)\n\n# A tibble: 345 × 3\n   college                         mean std_dev\n   &lt;chr&gt;                          &lt;dbl&gt;   &lt;dbl&gt;\n 1 Arizona St.                     0     NA    \n 2 Chicago St.                     0     NA    \n 3 Denver                          0     NA    \n 4 Fairfield                       0     NA    \n 5 George Mason                    0     NA    \n 6 Lebanon Valley                  0     NA    \n 7 Lincoln Memorial                0     NA    \n 8 University of Colorado Boulder  0     NA    \n 9 Toledo                          0.2   NA    \n10 Albany State (GA)               0.45   0.212\n# ℹ 335 more rows\n\n\nThis is, however, the opposite of what we want. The rows are sorted with the least frequent destination airports displayed first. This is because arrange() always returns rows sorted in ascending order by default. To switch the ordering to be in “descending” order instead, we use the desc() function as so:\n\nmean_pts %&gt;% \n  arrange(desc(mean))\n\n# A tibble: 345 × 3\n   college                           mean std_dev\n   &lt;chr&gt;                            &lt;dbl&gt;   &lt;dbl&gt;\n 1 Davidson                          19.6    9.95\n 2 Lehigh                            18.4    7.06\n 3 Western Carolina                  16.1    7.50\n 4 Navy                              15.4    4.25\n 5 Marist                            15.4    1.92\n 6 Butler Community College          13.6    6.57\n 7 Louisiana Tech                    13.5    6.78\n 8 Trinity Valley Community College  13.5    6.63\n 9 Weber State                       13.3   11.6 \n10 Central Arkansas                  13.1    4.73\n# ℹ 335 more rows"
  },
  {
    "objectID": "dplyr.html#sec-tibblesactivities",
    "href": "dplyr.html#sec-tibblesactivities",
    "title": "6  Manipulating Data",
    "section": "6.8 Activities",
    "text": "6.8 Activities\nIf you haven’t already, load load nba data set we downloaded back in Section 3.2. And let’s pass c(\"Undrafted\") as the na argument:\n\nnba &lt;- read_csv(\"./data/nba_all_seasons.csv\", na = c(\"Undrafted\"))\n\n\nProduce a data frame consisting exclusively of players under the age of 25.\nProduce a data frame consisting exclusively of players who attended “Marquette”.\nProduce a data frame consisting exclusively of players who are either under the age of 25 or attended “Marquette”.\nProduce a data frame consisting exclusively of players under the age of 25 and attended “Marquette”.\nDetermine the average height of all players.\nDetermine the median height of all players who attended “Connecticut”.\nFor each college appearing in the data set, what is the average pts and ast of players who attended that college?\nTry calculating these school-wise averages separately for each season.\nCreate a new column in the data set called reb_diff and calculate it as the difference between the offensive rebound percentage (oreb_pct) and the defensive rebound percentage (dreb_pct).\nWhat is the standard deviation of reb_diff?\nFor each college appearing in the data set, what is the average ptsof players who attended that college. Which college has produced the highest average? Which college has produced the lowest average?"
  },
  {
    "objectID": "ggplot2.html#grammarofgraphics",
    "href": "ggplot2.html#grammarofgraphics",
    "title": "7  Visualizing Data",
    "section": "7.1 The grammar of graphics",
    "text": "7.1 The grammar of graphics\nWe start with a discussion of a theoretical framework for data visualization known as “the grammar of graphics.” This framework serves as the foundation for the ggplot2 package which we’ll use extensively in this chapter. In Chapter 6, we saw how dplyr provides a “grammar” of data manipulation, a grammar which is made up of several “verbs” (functions like filter and mutate). Similar to dplyr’s grammar of data manipulation, ggplto2 provides a a grammar of graphics that defines a set of rules for constructing statistical graphics by combining different types of layers. This grammar was created by Leland Wilkinson (Wilkinson 2012) and has been implemented in a variety of data visualization software platforms like R, but also Plotly and Tableau.\n\n7.1.1 Components of the grammar\nIn short, the grammar tells us that:\n\nA statistical graphic is a mapping of data variables to aesthetic attributes of geometric objects.\n\nSpecifically, we can break a graphic into the following three essential components:\n\ndata: the dataset containing the variables of interest.\ngeom: the geometric object in question. This refers to the type of object we can observe in a plot. For example: points, lines, and bars.\naes: aesthetic attributes of the geometric object. For example, x/y position, color, shape, and size. Aesthetic attributes are mapped to variables in the data set.\n\nYou might be wondering why we wrote the terms data, geom, and aes in a computer code type font. We’ll see very shortly that we’ll specify the elements of the grammar in R using these terms. However, let’s first break down the grammar with an example.\n\n\n7.1.2 An initial example\nLet’s take another look at our nba data set, this time via the grammar of graphics. Let’s specifically take a look at how things have changed over the years. As always, we need to load the data we downloaded back in Section 3.2. Run the following:\n\nnba &lt;- read_csv(\"./data/nba_all_seasons.csv\", na = c(\"Undrafted\"))\n\nWe need to do a bit of work before we can use season as a measure of time. This is because the season column is currently stored as a character vector, with values such as “2000-01” and “2011-12”. So we need two things: these character vectors need to be trimmed so that we retain only the first 4 characters in each vector and we then need to convert these character vectors to their corresponding numeric values. We’ll do that using the stringr package, yet another package that is part of the tidyverse.\n\nnba &lt;- nba %&gt;%\n    # select first 4 characters of `season`\n    mutate(season_int = substr(nba$season, start=1, stop=4))  %&gt;%\n    # convert to integer\n    mutate(season_int = as.integer(season_int))\n\nNow that we have time represented as a numeric column in our data set, we can use it to plot some data to visualize.\n\n\n`summarise()` has grouped output by 'team_abbreviation'. You can override using\nthe `.groups` argument.\n\n\n\n\n\nLet’s view this plot through the grammar of graphics. First, we have actually used two type of geometric object here: a line object and a point object. The point object provides the small circular data points. The line object provides the line segments connecting the points.\n\nThe data variable season_int gets mapped to the x-position aesthetic of the lines and the points.\nThe data variable pts gets mapped to the y-position aesthetic of the lines and the points.\nThe data variable team gets mapped to the color aesthetic of the lines and the points.\nThe data variable ast gets mapped to the size aesthetic of the points.\n\nThat being said, this is just an example. Plots can specify points, lines, bars, and a variety of other geometric objects.\nLet’s summarize the three essential components of the grammar.\n\n\n# A tibble: 7 × 3\n  geom  aes   `data variable`\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;          \n1 line  x     season_int     \n2 line  y     pts            \n3 line  color team           \n4 point x     season_int     \n5 point y     pts            \n6 point color team           \n7 point size  ast            \n\n\n\n\n7.1.3 Other components\nThere are other components of the grammar of graphics we can control as well. As you start to delve deeper into the grammar of graphics, you’ll start to encounter these topics more frequently. In this book, we’ll keep things simple and only work with these two additional components:\n\nfaceting breaks up a plot into several plots split by the values of another variable\nposition adjustments for barplots\n\nOther more complex components like scales and coordinate systems are left for a more advanced text such as R for Data Science. Generally speaking, the grammar of graphics allows for a high degree of customization of plots and also a consistent framework for easily updating and modifying them.\n\n\n7.1.4 ggplot2 package\nIn this book, we will use the ggplot2 package for data visualization, which is an implementation of the grammar of graphics for R. As we noted earlier, a lot of the previous section was written in a computer code type font. This is because the various components of the grammar of graphics are specified in the ggplot() function included in the ggplot2 package. For the purposes of this book, we’ll always provide the ggplot() function with the following arguments (i.e., inputs) at a minimum:\n\nThe data frame where the variables exist: the data argument.\nThe mapping of the variables to aesthetic attributes: the mapping argument which specifies the aesthetic attributes involved.\n\nAfter we’ve specified these components, we then add layers to the plot using the + sign. The most essential layer to add to a plot is the layer that specifies which type of geometric object we want the plot to involve: points, lines, bars, and others. Other layers we can add to a plot include the plot title, axes labels, visual themes for the plots, and facets (which we’ll see in Section 7.6).\nLet’s now put the theory of the grammar of graphics into practice."
  },
  {
    "objectID": "ggplot2.html#sec-FiveNG",
    "href": "ggplot2.html#sec-FiveNG",
    "title": "7  Visualizing Data",
    "section": "7.2 Five named graphs - the 5NG",
    "text": "7.2 Five named graphs - the 5NG\nIn order to keep things simple in this book, we will only focus on five different types of graphics, each with a commonly given name. We term these “five named graphs” or in abbreviated form, the 5NG:\n\nscatterplots\nlinegraphs\nhistograms\nboxplots\nbarplots\n\nWe’ll also present some variations of these plots, but with this basic repertoire of five graphics in your toolbox, you can visualize a wide array of different variable types. Note that certain plots are only appropriate for categorical variables, while others are only appropriate for numerical variables."
  },
  {
    "objectID": "ggplot2.html#sec-scatterplots",
    "href": "ggplot2.html#sec-scatterplots",
    "title": "7  Visualizing Data",
    "section": "7.3 5NG#1: Scatterplots",
    "text": "7.3 5NG#1: Scatterplots\nThe simplest of the 5NG are scatterplots. They allow you to visualize the relationship between two numerical variables. While you may already be familiar with scatterplots, let’s view them through the lens of the grammar of graphics. Specifically, we will visualize the relationship between the following two numerical variables in the nba data frame:\n\npts: average points per game each player scored\nast: average number of assists per game each player made\n\n\n7.3.1 Scatterplots via geom_point\nLet’s now go over the code that will create the desired scatterplot, while keeping in mind the grammar of graphics framework we introduced above. Let’s take a look at the code and break it down piece-by-piece.\n\nggplot(data = nba, mapping = aes(x = pts, y = ast)) + \n  geom_point()\n\nWithin the ggplot() function, we specify two of the components of the grammar of graphics as arguments (i.e., inputs):\n\nThe data as the nba data frame via data = nba.\nThe aesthetic mapping by setting mapping = aes(x = pts, y = ast). Specifically, the variable pts maps to the x position aesthetic, whereas the variable ast maps to the y position.\n\nWe then add a layer to the ggplot() function call using the + sign. The added layer in question specifies the third component of the grammar: the geometric object. In this case, the geometric object is set to be points by specifying geom_point(). After running these two lines of code in your console, you’ll notice two outputs: a warning message and this graphic.\n\n\n\n\n\nFigure 7.1: Assists versus points\n\n\n\n\nLet’s first unpack the graphic in Figure 7.1. Observe that a positive relationship exists between pts and ast: as the number of points increases, the number of assists also increases. Observe also the large mass of points clustered near (0, 0), the point indicating players have no points and no assists (e.g., what would be expected from a player that doesn’t play very much).\nBefore we continue, let’s make a few more observations about this code that created the scatterplot. Note that the + sign comes at the end of lines, and not at the beginning. You’ll get an error in R if you put it at the beginning of a line. When adding layers to a plot, you are encouraged to start a new line after the + (by pressing the Return/Enter button on your keyboard) so that the code for each layer is on a new line. As we add more and more layers to plots, you’ll see this will greatly improve the legibility of your code.\nTo stress the importance of adding the layer specifying the geometric object, consider Figure 7.2 where no layers are added. Because the geometric object was not specified, we have a blank plot which is not very useful!\n\nggplot(data = nba, mapping = aes(x = pts, y = ast))\n\n\n\n\nFigure 7.2: Assists versus points\n\n\n\n\n\n\n7.3.2 Overplotting\nThe large mass of points near (0, 0) in Figure can cause some confusion since it is hard to tell the true number of points that are actually in this lower corner. This is the result of a phenomenon called overplotting. As one may guess, this corresponds to points being plotted on top of each other over and over again. When overplotting occurs, it is difficult to know the number of points being plotted. There are two methods to address the issue of overplotting. Either by\n\nAdjusting the transparency of the points or\nAdding a little random “jitter”, or random “nudges”, to each of the points.\n\nMethod 1: Changing the transparency\nThe first way of addressing overplotting is to change the transparency/opacity of the points by setting the alpha argument in geom_point(). We can change the alpha argument to be any value between 0 and 1, where 0 sets the points to be 100% transparent and 1 sets the points to be 100% opaque. By default, alpha is set to 1. In other words, if we don’t explicitly set an alpha value, R will use alpha = 1.\nNote how the following code is identical to the code in Section @ref(scatterplots) that created the scatterplot with overplotting, but with alpha = 0.05 added to the geom_point() function:\n\nggplot(data = nba, mapping = aes(x = pts, y = ast)) + \n  geom_point(alpha = 0.05)\n\n\n\n\nFigure 7.3: Assists versus points\n\n\n\n\nThe key feature to note in Figure 7.3 is that the transparency of the points is cumulative: areas with a high-degree of overplotting are darker, whereas areas with a lower degree are less dark. Note furthermore that there is no aes() surrounding alpha = 0.05. This is because we are not mapping a variable to an aesthetic attribute, but rather merely changing the default setting of alpha. In fact, you’ll receive an error if you try to change the second line to read geom_point(aes(alpha = 0.05)).\nMethod 2: Jittering the points\nThe second way of addressing overplotting is by jittering all the points. This means giving each point a small “nudge” in a random direction. You can think of “jittering” as shaking the points around a bit on the plot. Let’s illustrate using a simple example first. Say we have a data frame with 4 identical rows of x and y values: (0,0), (0,0), (0,0), and (0,0). In Figure 7.4, we present both the regular scatterplot of these 4 points (on the left) and its jittered counterpart (on the right).\n\n\n\n\n\nFigure 7.4: Regular and jittered scatterplots\n\n\n\n\nIn the left scatterplot, observe that the 4 points are superimposed on top of each other. While we know there are 4 values being plotted, this fact might not be apparent to others. In the right scatterplot, the points are jittered and it is now plainly evident that this plot involves four points since each point is given a random “nudge.”\nKeep in mind, however, that jittering is strictly a visualization tool; even after creating a jittered scatterplot, the original values saved in the data frame remain unchanged.\nTo create a jittered scatterplot, instead of using geom_point(), we use geom_jitter(). Observe how the following code is very similar to the code that created Figure 7.1, but with geom_point() replaced with geom_jitter().\n\n\n\n\n\nFigure 7.5: Assists versus points jittered scatterplot\n\n\n\n\nIn order to specify how much jitter to add, we adjusted the width and height arguments to geom_jitter(). This corresponds to how hard you’d like to shake the plot in horizontal x-axis units and vertical y-axis units, respectively. In this case, both axes are in counts (number of points, number of assists). How much jitter should we add using the width and height arguments? On the one hand, it is important to add just enough jitter to break any overlap in points, but on the other hand, not so much that we completely alter the original pattern in points.\nAs can be seen in the resulting Figure 7.5, in this case jittering doesn’t really provide much new insight. In this particular case, it can be argued that changing the transparency of the points by setting alpha proved more effective. When would it be better to use a jittered scatterplot? When would it be better to alter the points’ transparency? There is no single right answer that applies to all situations. You need to make a subjective choice and own that choice. At the very least when confronted with overplotting, however, we suggest you make both types of plots and see which one better emphasizes the point you are trying to make.\n\n\n7.3.3 Summary\nScatterplots display the relationship between two numerical variables. They are among the most commonly used plots because they can provide an immediate way to see the trend in one numerical variable versus another. However, if you try to create a scatterplot where either one of the two variables is not numerical, you might get strange results. Be careful!\nWith medium to large datasets, you may need to play around with the different modifications to scatterplots we saw such as changing the transparency/opacity of the points or by jittering the points. This tweaking is often a fun part of data visualization, since you’ll have the chance to see different relationships emerge as you tinker with your plots."
  },
  {
    "objectID": "ggplot2.html#sec-linegraphs",
    "href": "ggplot2.html#sec-linegraphs",
    "title": "7  Visualizing Data",
    "section": "7.4 5NG#2: Linegraphs",
    "text": "7.4 5NG#2: Linegraphs\nThe next of the five named graphs are linegraphs. Linegraphs show the relationship between two numerical variables when the variable on the x-axis is ordinal; there is an inherent ordering to the variable.\nThe most common examples of linegraphs have some notion of time on the x-axis: hours, days, weeks, years, etc. Since time is naturally ordinal, we connect consecutive observations of the variable on the y-axis with a line. Linegraphs that have some notion of time on the x-axis are also called time series plots.\n\n7.4.1 Linegraphs via geom_line\nLet’s a linegraph to visualize a single NBA player’s number of points scored across seasons. To do so, we’ll use geom_line(), instead of using geom_point() as we did for the scatterplots above:\n\nggplot(\n    data = nba %&gt;%\n        filter(player_name == \"Stephen Curry\"),\n    mapping = aes(x = season_int, y = pts)\n) +\n    geom_line()\n\n\n\n\nStephen Curry points over time\n\n\n\n\nLet’s break down this code piece-by-piece in terms of the grammar of graphics. Within the ggplot() function call, we specify two of the components of the grammar of graphics as arguments:\n\nThe data. Here we have provided a filtered version of our nba data set, selecting only those row where player_name==\"Stephen Curry\".\nThe aesthetic mapping by setting mapping = aes(x = season_int, y = pts). Specifically, the variable season_int maps to the x position aesthetic, whereas the variable pts maps to the y position aesthetic.\n\nWe add a layer to the ggplot() function call using the + sign. The layer in question specifies the third component of the grammar: the geometric object in question. In this case, the geometric object is a line set by specifying geom_line().\n\n\n7.4.2 Summary\nLinegraphs, just like scatterplots, display the relationship between two numerical variables. However, it is preferred to use linegraphs over scatterplots when the variable on the x-axis (i.e., the explanatory variable) has an inherent ordering, such as some notion of time."
  },
  {
    "objectID": "ggplot2.html#sec-histograms",
    "href": "ggplot2.html#sec-histograms",
    "title": "7  Visualizing Data",
    "section": "7.5 5NG#3: Histograms",
    "text": "7.5 5NG#3: Histograms\nLet’s consider the pts variable in the nba data frame once again, but unlike with the linegraphs in Section 7.4, let’s say we don’t care about its relationship with time, but rather we only care about how the values of pts distribute. In other words:\n\nWhat are the smallest and largest values?\nWhat is the “center” or “most typical” value?\nHow do the values spread out?\nWhat are frequent and infrequent values?\n\nOne way to visualize this distribution of this single variable pts is to plot them on a horizontal line as we do in Figure 7.6:\n\n\n\n\n\nFigure 7.6: Plot of players’ points per-game point averages.\n\n\n\n\nThis gives us a bit of an idea of how the values of pts are distributed: note that values range from zero to approximately 20. In addition, there appear to be more values falling between approximately 3 and 10 than there are values falling above this range. However, because of the high degree of overplotting in the points, it’s hard to get a sense of exactly how many values are between 5 and 10.\nWhat is commonly produced instead of Figure 7.6 is known as a histogram. A histogram is a plot that visualizes the distribution of a numerical value as follows:\n\nWe first cut up the x-axis into a series of bins, where each bin represents a range of values.\nFor each bin, we count the number of observations that fall in the range corresponding to that bin.\nThen for each bin, we draw a bar whose height marks the corresponding count.\n\nLet’s drill-down on an example of a histogram, shown in Figure 7.7.\n\n\n\n\n\nFigure 7.7: Example histogram.\n\n\n\n\nLet’s focus only on values between 10 points and 20 points for now. Observe that there are five bins of equal width between 10 points and 20 points. Thus we have five bins of width 2 points each: one bin for the 10-12 range, another bin for the 13-14 range, etc.\n\nThe bin for the 10-12 range has a height of around 150. In other words, around 150 players scored a season average of between 10 and 20 points.\nThe bin for the 13-14 range has a height of around 100. In other words, around 100. players scored a season average of between 13 and 14 points.\nThe bin for the 15-16 range has a height of around 50. In other words, around 50 players scored a season average of between 15 and 16 points.\nAnd so on…\n\n\n7.5.1 Histograms via geom_histogram\nLet’s now present the ggplot() code to plot your first histogram! Unlike with scatterplots and linegraphs, there is now only one variable being mapped in aes(): the single numerical variable pts. The y-aesthetic of a histogram, the count of the observations in each bin, gets computed for you automatically. Furthermore, the geometric object layer is now a geom_histogram(). After running the following code, you’ll see the histogram in Figure 7.8 as well as a warning message. We’ll discuss the warning message first.\n\nggplot(data = nba %&gt;%\n        group_by(player_name) %&gt;%\n        summarize(pts = mean(pts)),\n       mapping = aes(x = pts)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 7.8: Histogram of average pts per game.\n\n\n\n\nThe warning is telling us that the histogram was constructed using bins = 30 for 30 equally spaced bins. This is known the default value for this argument (Section 2.4.1). We’ll see in the next section how to change the number of bins to another value than the default.\nNow let’s unpack the resulting histogram in Figure 7.9. Observe that values greater than 20 are rather rare. However, because of the large number of bins, it’s hard to get a sense for which range of temperatures is spanned by each bin; everything is one giant amorphous blob. So let’s add white vertical borders demarcating the bins by adding a color = \"white\" argument to geom_histogram() and ignore the warning about setting the number of bins to a better value:\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 7.9: Histogram of average pts per game.\n\n\n\n\nWe now have an easier time associating ranges of temperatures to each of the bins in Figure 7.10. We can also vary the color of the bars by setting the fill argument. For example, you can set the bin colors to be “blue steel” by setting fill = \"steelblue\":\n\nggplot(data = nba %&gt;%\n        group_by(player_name) %&gt;%\n        summarize(pts = mean(pts)),\n       mapping = aes(x = pts)) +\n  geom_histogram(color = \"white\", fill = \"steelblue\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 7.10: Histogram of average pts per game.\n\n\n\n\nIf you’re curious, run colors() to see all 657 possible choice of colors in R!\n\n\n7.5.2 Adjusting the bins\nObserve in Figure 7.10 that in the 10-20 range there appear to be roughly 111 bins. Thus each bin has width 20-10 divided by 11, or 0.91 points, which is not a very easily interpretable range to work with. Let’s improve this by adjusting the number of bins in our histogram in one of two ways:\n\nBy adjusting the number of bins via the bins argument to geom_histogram().\nBy adjusting the width of the bins via the binwidth argument to geom_histogram().\n\nUsing the first method, we have the power to specify how many bins we would like to cut the x-axis up in. As mentioned in the previous section, the default number of bins is 30. We can override this default, to say 25 bins, as follows:\n\nggplot(data = nba %&gt;%\n        group_by(player_name) %&gt;%\n        summarize(pts = mean(pts)),\n       mapping = aes(x = pts)) +\n  geom_histogram(bins = 25, color = \"white\")\n\nUsing the second method, instead of specifying the number of bins, we specify the width of the bins by using the binwidth argument in the geom_histogram() layer. For example, let’s set the width of each bin to be 5 points.\n\nggplot(data = nba %&gt;%\n        group_by(player_name) %&gt;%\n        summarize(pts = mean(pts)),\n       mapping = aes(x = pts)) +\n  geom_histogram(binwidth = 5, color = \"white\")\n\nWe compare both resulting histograms side-by-side in Figure 7.11.\n\n\n\n\n\nFigure 7.11: Setting histogram bins in two ways.\n\n\n\n\n\n\n7.5.3 Summary\nHistograms, unlike scatterplots and linegraphs, present information on only a single numerical variable. Specifically, they are visualizations of the distribution of the numerical variable in question."
  },
  {
    "objectID": "ggplot2.html#sec-facets",
    "href": "ggplot2.html#sec-facets",
    "title": "7  Visualizing Data",
    "section": "7.6 Facets",
    "text": "7.6 Facets\nBefore continuing with the next of the 5NG, let’s briefly introduce a new concept called faceting. Faceting is used when we’d like to split a particular visualization by the values of another variable. This will create multiple copies of the same type of plot with matching x and y axes, but whose content will differ.\nFor example, suppose we were interested in looking at how the histogram of players’ average points per game changed across seasons. We could “split” this histogram so that we had a separate histogram of pts for each of several values of season_int. We do this by adding facet_wrap(~ season_int) layer. Note the ~ is a “tilde” and can generally be found on the key next to the “1” key on US keyboards. The tilde is required and you’ll receive the error Error in as.quoted(facets) : object 'season_int' not found if you don’t include it here.\n\nggplot(\n    data = nba %&gt;%\n        group_by(player_name, season_int) %&gt;%\n        summarize(pts = mean(pts), season_int=first(season_int)),\n    mapping = aes(x = pts)\n) +\n    geom_histogram(binwidth = 5, color = \"white\") +\n    facet_wrap( ~ season_int)\n\n`summarise()` has grouped output by 'player_name'. You can override using the\n`.groups` argument.\n\n\n\n\n\nFigure 7.12: Faceted histogram of points per game.\n\n\n\n\nWe can also specify the number of rows and columns in the grid by using the nrow and ncol arguments inside of facet_wrap(). For example, say we would like our faceted histogram to have 4 rows instead of 3. We simply add an nrow = 4 argument to facet_wrap(~ season_int).\n\nggplot(\n    data = nba %&gt;%\n        group_by(player_name, season_int) %&gt;%\n        summarize(pts = mean(pts), season_int=first(season_int)),\n    mapping = aes(x = pts)\n) +\n    geom_histogram(binwidth = 5, color = \"white\") +\n    facet_wrap( ~ season_int, nrow = 4)\n\n`summarise()` has grouped output by 'player_name'. You can override using the\n`.groups` argument.\n\n\n\n\n\nFigure 7.13: Faceted histogram of points per game."
  },
  {
    "objectID": "ggplot2.html#sec-boxplots",
    "href": "ggplot2.html#sec-boxplots",
    "title": "7  Visualizing Data",
    "section": "7.7 5NG#4: Boxplots",
    "text": "7.7 5NG#4: Boxplots\nThough faceted histograms are one type of visualization used to compare the distribution of a numerical variable split by the values of another variable, another type of visualization that achieves a similar goal is a side-by-side boxplot. A boxplot is constructed from the information provided in the five-number summary of a numerical variable.\nLet’s again consider the distribution of points. For now, let’s confine ourselves to the 1996 season to keep things simple.\n\nbase_plot &lt;- nba %&gt;%\n        filter(season_int %in% c(1996)) %&gt;%\n    ggplot(mapping = aes(x = factor(season_int), y=pts))\nbase_plot + geom_jitter(width = 0.1, height = 0, alpha = 0.3)\n\n\n\n\nFigure 7.14: Points from 1996 represented as jittered points.\n\n\n\n\nThese observations have the following five-number summary:\n\nMinimum: 0\nFirst quartile (25th percentile): 3 points\nMedian (second quartile, 50th percentile): 6 points\nThird quartile (75th percentile): 12 points\nMaximum: 29.6\n\nIn the leftmost plot of Figure 7.15, let’s mark these 5 values with dashed horizontal lines on top of the actual data points. In the middle plot of Figure 7.15 let’s add the boxplot. In the rightmost plot of Figure 7.15, let’s remove the points and the dashed horizontal lines for clarity’s sake.\n\n\n\n\n\nFigure 7.15: Building up a boxplot of points\n\n\n\n\nWhat the boxplot does is visually summarize the points by cutting them into quartiles at the dashed lines, where each quartile contains four equally-size groups of observations. Thus\n\n25% of points fall below the bottom edge of the box, which is the first quartile of 3 points. In other words, 25% of observations were below 3 points.\n25% of points fall between the bottom edge of the box and the solid middle line, which is the median of 6 points. Thus, 25% of observations were between 3 points and 6 points and 50% of observations were below 6 points.\n25% of points fall between the solid middle line and the top edge of the box, which is the third quartile of 12 points. It follows that 25% of observations were between 6 points and 12 points and 75% of observations were below 12 points.\n25% of points fall above the top edge of the box. In other words, 25% of observations were above 12 points.\nThe middle 50% of points lie within the interquartile range (IQR) between the first and third quartile. Thus, the IQR for this example is 3 - 12 = -9 point. The interquartile range is one measure of a numerical variable’s spread.\n\nFurthermore, in the rightmost plot of Figure 7.15, we see the whiskers of the boxplot. The whiskers stick out from either end of the box all the way to the minimum and maximum observations of 0 and 29.6 points, respectively. However, the whiskers don’t always extend to the smallest and largest observed values as they do here. They in fact extend no more than 1.5 \\(\\times\\) the interquartile range from either end of the box. In this case, we see a small number of observations that lie more than 1.5 \\(\\times\\) -9 points = -13.5 points the top of the box. These observations are are called outliers.\n\n7.7.1 Boxplots via geom_boxplot\nLet’s now create a side-by-side boxplot of players’ average points per game split by the different seasons as we did previously with the faceted histograms. We do this by mapping the season_int variable to the x-position aesthetic, the pts variable to the y-position aesthetic, and by adding a geom_boxplot() layer:\n\nggplot(data = nba, mapping = aes(x = season_int, y = pts)) +\n  geom_boxplot()\n\nWarning: Continuous x aesthetic\nℹ did you forget `aes(group = ...)`?\n\n\n\n\n\nFigure 7.16: Invalid boxplot specification.\n\n\n\n\nObserve in Figure 7.16 that this plot does not provide information about points separated by season. The warning message clues us in as to why. It is telling us that we have a “continuous”, or numerical variable, on the x-position aesthetic. Boxplots, however, require a categorical variable to be mapped to the x-position aesthetic.\nWe can convert the numerical variable season_int into a factor categorical variable by using the factor() function. So after applying factor(season_int), season_int goes from having numerical values just the 1995, 1996, etc. to having an associated ordering. With this ordering, ggplot() now knows how to work with this variable to produce the needed plot.\n\nggplot(data = nba, mapping = aes(x = factor(season_int), y = pts)) +\n  geom_boxplot()\n\n\n\n\nFigure 7.17: Side-by-side boxplot of average points per game split by season.\n\n\n\n\nThe resulting Figure 7.17 shows 26 separate “box and whiskers” plots similar to the rightmost plot of Figure 7.15 of only data from 1996. Thus the different boxplots are shown “side-by-side.”\n\nThe “box” portions of the visualization represent the 1st quartile, the median (the 2nd quartile), and the 3rd quartile.\nThe height of each box (the value of the 3rd quartile minus the value of the 1st quartile) is the interquartile range (IQR). It is a measure of the spread of the middle 50% of values, with longer boxes indicating more variability.\nThe “whisker” portions of these plots extend out from the bottoms and tops of the boxes and represent points less than the 25th percentile and greater than the 75th percentiles, respectively. They’re set to extend out no more than \\(1.5 \\times IQR\\) units away from either end of the boxes. We say “no more than” because the ends of the whiskers have to correspond to observed points per game averages The length of these whiskers show how the data outside the middle 50% of values vary, with longer whiskers indicating more variability.\nThe dots representing values falling outside the whiskers are called outliers. These can be thought of as potentially anomalous (“out-of-the-ordinary”) values.\n\nIt is important to keep in mind that the definition of an outlier is somewhat arbitrary and not absolute. In this case, they are defined by the length of the whiskers, which are no more than \\(1.5 \\times IQR\\) units long for each boxplot. Looking at this side-by-side plot we can easily compare scorring distributions across seasons by drawing imaginary horizontal lines across the plot. Furthermore, the heights of the boxes as quantified by the interquartile ranges are informative too; they tell us about variability, or spread, of the different players’ averages recorded in a given season.\n\n\n7.7.2 Summary\nSide-by-side boxplots provide us with a way to compare the distribution of a numerical variable across multiple values of another variable. One can see where the median falls across the different groups by comparing the solid lines in the center of the boxes.\nTo study the spread of a numerical variable within one of the boxes, look at both the length of the box and also how far the whiskers extend from either end of the box. Outliers are even more easily identified when looking at a boxplot than when looking at a histogram as they are marked with distinct points."
  },
  {
    "objectID": "ggplot2.html#sec-geombar",
    "href": "ggplot2.html#sec-geombar",
    "title": "7  Visualizing Data",
    "section": "7.8 5NG#5: Barplots",
    "text": "7.8 5NG#5: Barplots\nBoth histograms and boxplots are tools to visualize the distribution of numerical variables. Another commonly desired task is to visualize the distribution of a categorical variable. This is a simpler task, as we are simply counting different categories within a categorical variable, also known as the levels of the categorical variable. Often the best way to visualize these different counts, also known as frequencies, is with barplots (also called barcharts).\nOne complication, however, is how your data is represented. Is the categorical variable of interest “pre-counted” or not? For example, run the following code that manually creates two data frames representing a collection of fruit: 3 apples and 2 oranges.\n\nfruits &lt;- tibble(\n  fruit = c(\"apple\", \"apple\", \"orange\", \"apple\", \"orange\")\n)\nfruits_counted &lt;- tibble(\n  fruit = c(\"apple\", \"orange\"),\n  number = c(3, 2)\n)\n\nWe see both the fruits and fruits_counted data frames represent the same collection of fruit. Whereas fruits just lists the fruit individually…\n\n\n# A tibble: 5 × 1\n  fruit \n  &lt;chr&gt; \n1 apple \n2 apple \n3 orange\n4 apple \n5 orange\n\n\n… fruits_counted has a variable count which represent the “pre-counted” values of each fruit.\n\n\n# A tibble: 2 × 2\n  fruit  number\n  &lt;chr&gt;   &lt;dbl&gt;\n1 apple       3\n2 orange      2\n\n\nDepending on how your categorical data is represented, you’ll need to add a different geometric layer type to your ggplot() to create a barplot, as we now explore.\n\n7.8.1 Barplots via geom_bar or geom_col\nLet’s generate barplots using these two different representations of the same basket of fruit: 3 apples and 2 oranges. Using the fruits data frame where all 5 fruits are listed individually in 5 rows, we map the fruit variable to the x-position aesthetic and add a geom_bar() layer:\n\nggplot(data = fruits, mapping = aes(x = fruit)) +\n  geom_bar()\n\n\n\n\nFigure 7.18: Barplot when counts are not pre-counted.\n\n\n\n\nHowever, using the fruits_counted data frame where the fruits have been “pre-counted”, we once again map the fruit variable to the x-position aesthetic, but here we also map the count variable to the y-position aesthetic, and add a geom_col() layer instead.\n\nggplot(data = fruits_counted, mapping = aes(x = fruit, y = number)) +\n  geom_col()\n\n\n\n\nFigure 7.19: Barplot when counts are pre-counted.\n\n\n\n\nCompare the barplots in Figures Figure 7.18 and Figure 7.19. They are identical because they reflect counts of the same five fruits. However, depending on how our categorical data is represented, either “pre-counted” or not, we must add a different geom layer. When the categorical variable whose distribution you want to visualize\n\nIs not pre-counted in your data frame, we use geom_bar().\nIs pre-counted in your data frame, we use geom_col() with the y-position aesthetic mapped to the variable that has the counts.\n\nLet’s now go back to the nba data frame and visualize the distribution of the categorical variable college. Specifically, we’ll visualize the number of players who graduated from different colleges. We’ll focus on the New York Knicks (team_abbreviation==NYK) and data from the 2006-2009 seasons.\nRecall from Chapter 6, you saw that each row in the nba data set corresponds to a player in a given year In other words, the nba data frame is more like the fruits data frame than the fruits_counted data frame because the numbers of players from each college have not been pre-counted. Thus we should use geom_bar() instead of geom_col() to create a barplot. Much like a geom_histogram(), there is only one variable in the aes() aesthetic mapping: the variable college gets mapped to the x-position. As a difference though, histograms typically have bars that touch whereas bar graphs typically have space between the bars.\n\nnba %&gt;%\n    group_by(player_name, college) %&gt;%\n    filter(team_abbreviation %in% c(\"NYK\"),\n           season_int &lt; 2010,\n       season_int &gt; 2005) %&gt;%\n    ggplot(mapping = aes(x = college)) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90))\n\n\n\n\nFigure 7.20: Number of players by colege using geom_bar().\n\n\n\n\nObserve in Figure 7.20 that there are many Knicks players who either did not attend any college (“None”), attended Arizona State, or attended Florida. If you don’t know which airlines correspond to which carrier codes, then run View(airlines) to see a directory of airlines. For example, B6 is JetBlue Airways. Alternatively, say you had a data frame where the number of flights for each carrier was pre-counted as in Table @ref(tab:flights-counted).\n\n\n\n\n\nNumber of players, pre-counted for each college\n\n\ncollege\nnumber\n\n\n\n\nArizona\n1\n\n\nArizona State\n1\n\n\nDePaul\n6\n\n\nDrexel\n2\n\n\nDuke\n2\n\n\nFlorida\n4\n\n\nFlorida A&M\n3\n\n\nFlorida State\n1\n\n\nGeorgia Tech\n2\n\n\nIndiana\n3\n\n\nIowa State\n1\n\n\nKansas State\n1\n\n\nKentucky\n3\n\n\nMaryland\n2\n\n\nMemphis\n1\n\n\nMichigan\n3\n\n\nMichigan State\n1\n\n\nNew Mexico\n1\n\n\nNone\n13\n\n\nOregon\n1\n\n\nSaint Louis\n1\n\n\nSouth Carolina\n2\n\n\nSyracuse\n1\n\n\nTemple\n2\n\n\nWashington\n3\n\n\n\n\n\n\nFigure 7.21: ?(caption)\n\n\n\nIn order to create a barplot visualizing the distribution of the categorical variable college in this case, we would now use geom_col() instead of geom_bar(), with an additional y = number in the aesthetic mapping on top of the x = carrier. The resulting barplot would be identical to Figure 7.20.\n\n\n7.8.2 Must avoid pie charts!\nOne of the most common plots used to visualize the distribution of categorical data is the pie chart. Though they may seem harmless, pie charts actually present a problem in that humans are unable to judge angles well. As Naomi Robbins describes in her book, Creating More Effective Graphs (robbins2013?), people tend to overestimate angles greater than 90 degrees and underestimate angles less than 90 degrees. In other words, it is difficult to determine the relative size of one piece of the pie compared to another. So stay away!\n\n\n7.8.3 Two categorical variables\nBarplots are a very common way to visualize the frequency of different categories, or levels, of a single categorical variable. Another use of barplots is to visualize the joint distribution of two categorical variables at the same time. Let’s examine the joint distribution of players by college as well as season. In other words, the number of players for each combination of college and season.\nFor example, the number of WestJet flights from JFK, the number of WestJet flights from LGA, the number of WestJet flights from EWR, the number of American Airlines flights from JFK, and so on. Recall the ggplot() code that created the barplot of carrier frequency in Figure @ref(fig:flightsbar):\nWe can now map the additional variable origin by adding a fill = origin inside the aes() aesthetic mapping.\n\nnba %&gt;%\n    group_by(player_name, college) %&gt;%\n    filter(team_abbreviation %in% c(\"NYK\"),\n           season_int &lt; 2010,\n           season_int &gt; 2005) %&gt;%\n    ggplot(mapping = aes(x = college, fill = factor(season_int))) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90))\n\n\n\n\nFigure 7.22: ?(caption)\n\n\n\n\nFigure 7.22 is an example of a stacked barplot. Though simple to make, in certain aspects it is not ideal. For example, it is not particularly easy to compare the heights of the different colors between the bars, corresponding to comparing the number of players from each season_int between the different teams.\nBefore we continue, let’s address some common points of confusion among new R users. First, the fill aesthetic corresponds to the color used to fill the bars, whereas the color aesthetic corresponds to the color of the outline of the bars. This is identical to how we added color to our histogram in Section 7.5.1: we set the outline of the bars to white by setting color = \"white\" and the colors of the bars to blue steel by setting fill = \"steelblue\". Observe in Figure 7.23 that mapping season_int to color and not fill yields grey bars with different colored outlines.\n\nnba %&gt;%\n    group_by(player_name, college) %&gt;%\n    filter(team_abbreviation %in% c(\"NYK\"),\n           season_int &lt; 2010,\n           season_int &gt; 2005) %&gt;%\n    ggplot(mapping = aes(x = college, color = factor(season_int))) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90))\n\n\n\n\nFigure 7.23: ?(caption)\n\n\n\n\nSecond, note that fill is another aesthetic mapping much like x-position; thus we were careful to include it within the parentheses of the aes() mapping. The following code, where the fill aesthetic is specified outside the aes() mapping will yield an error. This is a fairly common error that new ggplot users make:\n\n...\n    ggplot(mapping = aes(x = college), color = factor(season_int)) +\n    geom_bar() +\n\nAn alternative to stacked barplots are side-by-side barplots, also known as dodged barplots, as seen in Figure 7.24. The code to create a side-by-side barplot is identical to the code to create a stacked barplot, but with a position = \"dodge\" argument added to geom_bar(). In other words, we are overriding the default barplot type, which is a stacked barplot, and specifying it to be a side-by-side barplot instead.\n\nnba %&gt;%\n    group_by(player_name, college) %&gt;%\n    filter(team_abbreviation %in% c(\"NYK\"),\n           season_int &lt; 2010,\n           season_int &gt; 2005) %&gt;%\n    ggplot(mapping = aes(x = college, fill = factor(season_int))) +\n    geom_bar(position = \"dodge\") +\n    theme(axis.text.x = element_text(angle = 90))\n\n\n\n\nFigure 7.24: ?(caption)\n\n\n\n\nHere, the width of the bars for DuPaul, Florida, and None is different than the width of the bars for Arizona and Iowa State. We can make one tweak to the position argument to get them to be the same size in terms of width as the other bars by using the more robust position_dodge() function.\n\nnba %&gt;%\n    group_by(player_name, college) %&gt;%\n    filter(team_abbreviation %in% c(\"NYK\"),\n           season_int &lt; 2010,\n           season_int &gt; 2005) %&gt;%\n    ggplot(mapping = aes(x = college, fill = factor(season_int))) +\n    geom_bar(position = position_dodge(preserve = \"single\")) +\n    theme(axis.text.x = element_text(angle = 90))\n\n\n\n\nLastly, another type of barplot is a faceted barplot. Recall in Section 7.6 we visualized the distribution of players’ points split by season using facets. We apply the same principle to our barplot visualizing the frequency of college split by season_int: instead of mapping college to fill we include it as the variable to create small multiples of the plot across the levels of college.\n\nggplot(data = flights, mapping = aes(x = carrier)) +\n  geom_bar() +\n  facet_wrap(~ origin, ncol = 1)\n\n\nnba %&gt;%\n    group_by(player_name, college) %&gt;%\n    filter(team_abbreviation %in% c(\"NYK\"),\n           season_int &lt; 2010,\n           season_int &gt; 2005) %&gt;%\n    ggplot(mapping = aes(x = college)) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90)) +\n    facet_wrap(~season_int, ncol = 1)\n\n\n\n\nFigure 7.25: Faceted barplot comparing the number of flights by carrier and origin.\n\n\n\n\n\n\n7.8.4 Summary\nBarplots are a common way of displaying the distribution of a categorical variable, or in other words the frequency with which the different categories (also called levels) occur. They are easy to understand and make it easy to make comparisons across levels. Furthermore, when trying to visualize the relationship of two categorical variables, you have many options: stacked barplots, side-by-side barplots, and faceted barplots. Depending on what aspect of the relationship you are trying to emphasize, you will need to make a choice between these three types of barplots and own that choice."
  },
  {
    "objectID": "ggplot2.html#sec-data-vis-conclusion",
    "href": "ggplot2.html#sec-data-vis-conclusion",
    "title": "7  Visualizing Data",
    "section": "7.9 Conclusion",
    "text": "7.9 Conclusion\n\n7.9.1 Summary table\nLet’s recap all five of the five named graphs (5NG) in Table 7.1 summarizing their differences. Using these 5NG, you’ll be able to visualize the distributions and relationships of variables contained in a wide array of datasets. This will be even more the case as we start to map more variables to more of each geometric object’s aesthetic attribute options, further unlocking the awesome power of the ggplot2 package.\n\n\nTable 7.1: My Caption\n\n\n\n\n\n\n\n\nNamed graph\nShows\nGeometric object\nNotes\n\n\n\n\nScatterplot\nRelationship between 2 numerical variables\ngeom_point()\n\n\n\nLinegraph\nRelationship between 2 numerical variables\ngeom_line()\nUsed when there is a sequential order to x-variable, e.g., time\n\n\nHistogram\nDistribution of 1 numerical variable\ngeom_histogram()\nFacetted histograms show the distribution of 1 numerical variable split by the values of another variable\n\n\nBoxplot\nDistribution of 1 numerical variable split by the values of another variable\ngeom_boxplot()\nC\n\n\nBarplot\nDistribution of 1 categorical variable\ngeom_bar() when counts are not pre-counted, geom_col() when counts are pre-counted `\nStacked, side-by-side, and faceted barplots show the joint distribution of 2 categorical variables\n\n\n\n\n\n\n7.9.2 Function argument specification\nLet’s go over some important points about specifying the arguments (i.e., inputs) to functions. Run the following two segments of code:\n\n# Segment 1:\nggplot(data = nba, mapping = aes(x = team_abbreviation)) +\n  geom_bar()\n\n# Segment 2:\nggplot(flights, aes(x = team_abbreviation)) +\n  geom_bar()\n\nYou’ll notice that both code segments create the same barplot, even though in the second segment we omitted the data = and mapping = code argument names. This is because the ggplot() function by default assumes that the data argument comes first and the mapping argument comes second. As long as you specify the data frame in question first and the aes() mapping second, you can omit the explicit statement of the argument names data = and mapping =. That being said, explicit is better than implicit. Given the uniformity of the tidyverse packages, you will often see the data= argument name omitted (because the first argument of all tidyverse function is a tibble), but it is good practice to include the names of other arguments for readability and clarity purposes.\n\n\n\n\nWilkinson, Leland. 2012. The Grammar of Graphics. Springer."
  },
  {
    "objectID": "descriptives.html#sec-centraltendency",
    "href": "descriptives.html#sec-centraltendency",
    "title": "8  Descriptive Statistics",
    "section": "8.1 Measures of central tendency",
    "text": "8.1 Measures of central tendency\nDrawing pictures of the data, as we did in Section 7.5 is an excellent way to convey the “gist” of what the data is trying to tell you, it’s often extremely useful to try to condense the data into a few simple “summary” statistics. In most situations, the first thing that you’ll want to calculate is a measure of central tendency. That is, you’d like to know something about the “average” or “middle” of your data lies. The most commonly used measures are the mean, median and mode.\n\n8.1.1 The mean\nThe mean of a set of observations is just a normal, old-fashioned average: add all of the values up, and then divide by the total number of values. If we have five observations (56, 31, 56, 8 and 32), the mean of these observations is: \\[\n\\frac{56 + 31 + 56 + 8 + 32}{5} = \\frac{183}{5} = 36.60\n\\] Of course, this definition of the mean isn’t news to anyone: averages (i.e., means) are used so often in everyday life that this is pretty familiar stuff. However, since the concept of a mean is something that everyone already understands, I’ll use this as an excuse to start introducing some of the mathematical notation that statisticians use to describe this calculation, and talk about how the calculations would be done in R.\nHow do we get the magic computing box to do all this work for us? If you really wanted to, you could do this calculation directly in R.\n\n(56 + 31 + 56 + 8 + 32) / 5\n\n[1] 36.6\n\n\nHowever, that’s not the only way to do the calculations, and when the number of observations starts to become large, it’s easily the most tedious. We could use the sum() function to get the numerator for us\n\nsum(c(56, 31, 56, 8, 32)) / 5\n\n[1] 36.6\n\n\nThis is ok, but we can do it even more easily using the mean() function.\n\nmean(c(56, 31, 56, 8, 32))\n\n[1] 36.6\n\n\nAs you can see, this gives exactly the same answers as the previous calculations.\n\n\n8.1.2 The median\nThe second measure of central tendency that people use a lot is the median, and it’s even easier to describe than the mean. The median of a set of observations is just the middle value. As before let’s imagine we were interested only in the five values from before. To figure out the median, we sort these numbers into ascending order: \\[\n8, 31, \\mathbf{32}, 56, 56\n\\] From inspection, it’s obvious that the median value of these 5 observations is 32, since that’s the middle one in the sorted list (I’ve put it in bold to make it even more obvious). Easy stuff. But what should we do if we were interested in the first 6 games rather than the first 5? Since the sixth game in the season had a winning margin of 14 points, our sorted list is now \\[\n8, 14, \\mathbf{31}, \\mathbf{32}, 56, 56\n\\] and there are two middle numbers, 31 and 32. The median is defined as the average of those two numbers, which is of course 31.5. As before, it’s very tedious to do this by hand when you’ve got lots of numbers. To illustrate this, here’s what happens when you use R to sort the values. First, I’ll use the sort() function to display the values in increasing numerical order:\n\nsort(c(56, 31, 56, 8, 32, 14))\n\n[1]  8 14 31 32 56 56\n\n\nThe middle values are 30 and 31, so the median winning margin for 2010 was 30.5 points. In real life, of course, no-one actually calculates the median by sorting the data and then looking for the middle value. In real life, we use the median command:\n\nmedian(c(56, 31, 56, 8, 32, 14))\n\n[1] 31.5\n\n\nwhich outputs the value we expect.\n\n\n8.1.3 Mean or median?\nKnowing how to calculate means and medians is only a part of the story. You also need to understand what each one is saying about the data, and what that implies for when you should use each one. The mean is kind of like the “center of gravity” of the data set, whereas the median is the “middle value” in the data. What this implies that the appropriate measure of central tendency depends on what type of data you’ve got and what you’re trying to achieve. As a rough guide:\n\nIf your data are nominal scale, you probably shouldn’t be using either the mean or the median. Both the mean and the median rely on the idea that the numbers assigned to values are meaningful. If the numbering scheme is arbitrary, then it’s probably best to use the mode (Section 8.1.4) instead.\nIf your data are ordinal scale, you’re more likely to want to use the median than the mean. The median only makes use of the order information in your data (i.e., which numbers are bigger), but doesn’t depend on the precise numbers involved. That’s exactly the situation that applies when your data are ordinal scale. The mean, on the other hand, makes use of the precise numeric values assigned to the observations, so it’s not really appropriate for ordinal data.\nFor interval and ratio scale data, either one is generally acceptable. Which one you pick depends a bit on what you’re trying to achieve. The mean has the advantage that it uses all the information in the data (which is useful when you don’t have a lot of data), but it’s very sensitive to extreme values. Let’s expand on that last part a little. One consequence is that there’s systematic differences between the mean and the median when the histogram is asymmetric (or skewed, see Section 8.3). Many, many measurements are highly asymmetric, including those measured in dollars (e.g., home prices, income, etc.). If you’re interested in looking at the overall income among a group of people, the mean might be a good answer; but if you’re interested in a typical income among those same people, the median may be a better choice.\n\n\n\n8.1.4 Mode\nThe mode of a sample is very simple: it is the value that occurs most frequently within a set of measurements. To illustrate the mode using the nba data, let’s examine the teams represented in the data set Let’s take a look at the first few values of this variable.\n\nhead(nba$team_abbreviation)\n\n[1] \"CHI\" \"LAC\" \"TOR\" \"DAL\" \"MIA\" \"HOU\"\n\n\nThese are strings (or character vectors) that represent the team each player is plays for. We could read through all 400, and count the number of occasions on which each team name appears in our list of finalists, thereby producing a frequency table. However, that would be mindless and boring: exactly the sort of task that computers are great at. So let’s use the table() function (discussed in more detail in Section @ref(freqtables)) to do this task for us:\n\ntable(nba$team_abbreviation)\n\n\nATL BKN BOS CHA CHH CHI CLE DAL DEN DET GSW HOU IND LAC LAL MEM MIA MIL MIN NJN \n421 180 407 288  89 406 433 422 412 400 409 418 410 424 411 352 426 409 399 257 \nNOH NOK NOP NYK OKC ORL PHI PHX POR SAC SAS SEA TOR UTA VAN WAS \n143  32 159 410 239 411 420 399 407 398 413 182 428 397  72 422 \n\n\nIf you spend enough time staring at this frequency table, you may be able to figure out that there are more rows associated with “CLE” (the Cleveland Cavaliers) than any other team. Thus, the mode of the team_abbreviation data is \"CLE\".\nSomewhat surprisingly, neither the core functionality of R nor the tidyverse has a function for calculating the mode. However, you can find packages with such functionality (e.g., modeest). You can also find mode-calculating function that others have put together (e.g., on StackOverflow). Refer back to Section 2.4 to remind yourself how functions work."
  },
  {
    "objectID": "descriptives.html#sec-var",
    "href": "descriptives.html#sec-var",
    "title": "8  Descriptive Statistics",
    "section": "8.2 Measures of variability",
    "text": "8.2 Measures of variability\nThe statistics that we’ve discussed so far all relate to central tendency. That is, they all talk about which values are “in the middle” or “popular” in the data. However, central tendency is not the only type of summary statistic that we want to calculate. The second thing that we really want is a measure of the variability of the data. That is, how “spread out” are the data? How “far” away from the mean or median do the observed values tend to be? For now, let’s assume that the data are interval or ratio scale, so we’ll use the age variable in our nba data set. We’ll use this data to discuss several different measures of spread, each with different strengths and weaknesses.\n\n8.2.1 Range\nThe range of a variable is very simple: it’s the biggest value minus the smallest value. For the data, the maximum value of age is 44, and the minimum value is 18. We can calculate these values in R using the max() and min() functions:\n\nmax(nba$age)\n\n[1] 44\n\nmin(nba$age)\n\n[1] 18\n\n\nThe other possibility is to use the range() function; which outputs both the minimum value and the maximum value in a vector, like this:\n\nrange(nba$age)\n\n[1] 18 44\n\n\nAlthough the range is the simplest way to quantify the notion of “variability”, it’s one of the worst. Recall from our discussion of the mean that we want our summary measure to be robust. If the data set has one or two extremely bad values in it, we’d like our statistics not to be unduly influenced by these cases. If we look once again at our toy example of a data set containing very extreme outliers…\n\\[\n-100,2,3,4,5,6,7,8,9,10\n\\] … it is clear that the range is not robust, since this has a range of 110, but if the outlier were removed we would have a range of only 8. The problem with the range is that it only considers two numbers: the mininum and the maximum. It completely ignores every other value.\n\n\n8.2.2 Interquartile range\nThe interquartile range (IQR) is like the range, but instead of calculating the difference between the biggest and smallest value, it calculates the difference between the 25th quantile and the 75th quantile. Probably you already know what a quantile is (they’re more commonly called percentiles), but if not: the 10th percentile of a data set is the smallest number \\(x\\) such that 10% of the data is less than \\(x\\). In fact, we’ve already come across the idea: the median of a data set is its 50th quantile / percentile! R actually provides you with a way of calculating quantiles, using the (surprise, surprise) quantile() function. Let’s use it to calculate the median AFL winning margin:\n\nquantile(x=nba$age, probs=.5)\n\n50% \n 26 \n\n\nAnd not surprisingly, this agrees with the answer that we saw earlier with the median() function. Now, we can actually input lots of quantiles at once, by specifying a vector for the probs argument. So lets do that, and get the 25th and 75th percentile:\n\nquantile(x=nba$age, probs=c(.25,.75))\n\n25% 75% \n 24  30 \n\n\nAnd, by noting that \\(30 - 24 = 6\\), we can see that the interquartile range for the age variable is 6. Of course, that seems like too much work to do all that typing, so R has a built in function called IQR() that we can use:\n\nIQR(x=nba$age)\n\n[1] 6\n\n\nThough it’s obvious how to interpret the range, it’s a little less obvious how to interpret the IQR. The simplest way to think about it is like this: the interquartile range is the range spanned by the “middle half” of the data. That is, one quarter of the data falls below the 25th percentile, one quarter of the data is above the 75th percentile, leaving the “middle half” of the data lying in between the two. And the IQR is the range covered by that middle half.\n\n\n8.2.3 Mean absolute deviation\nThe two measures we’ve looked at so far, the range and the interquartile range, both rely on the idea that we can measure the spread of the data by looking at the quantiles of the data. However, this isn’t the only way to think about the problem. A different approach is to select a meaningful reference point (usually the mean or the median) and then report the “typical” deviations from that reference point. What do we mean by “typical” deviation? Usually, the mean or median value of these deviations! In practice, this leads to two different measures, the “mean absolute deviation (from the mean)” and the “median absolute deviation (from the median)”. From what I’ve read, the measure based on the median seems to be used in statistics, and does seem to be the better of the two, but to be honest I don’t think I’ve seen it used much in psychology. The measure based on the mean does occasionally show up in psychology though. In this section I’ll talk about the first one, and I’ll come back to talk about the second one later.\nSince the previous paragraph might sound a little abstract, let’s go through the mean absolute deviation from the mean a little more slowly. One useful thing about this measure is that the name actually tells you exactly how to calculate it. Let’s think about values of56, 31, 56, 8 and 32 again. Since our calculations rely on an examination of the deviation from some reference point (in this case the mean), the first thing we need to calculate is the mean. For these five observations, our mean is 36.6. The next step is to convert each of our observations into a deviation score. We do this by calculating the difference between the observation and the mean. For the first observation in our sample, this is equal to \\(56 - 36.6 = 19.4\\). Okay, that’s simple enough. The next step in the process is to convert these deviations to absolute deviations. We do this by converting any negative values to positive ones. Mathematically, we would denote the absolute value of \\(-3\\) as \\(|-3|\\), and so we say that \\(|-3| = 3\\). We use the absolute value function (abs() in R) here because we don’t really care whether the value is higher than the mean or lower than the mean, we’re just interested in how close it is to the mean. To help make this process as obvious as possible, the table below shows these calculations for all five observations:\nNow that we have calculated the absolute deviation score for every observation in the data set, all that we have to do to calculate the mean of these scores. Let’s do that:\n\\[\n\\frac{19.4 + 5.6 + 19.4 + 28.6 + 4.6}{5} = 15.52\n\\]\nAnd we’re done. The mean absolute deviation for these five scores is 15.52.\nHowever, while our calculations for this little example are at an end, we do have a couple of things left to talk about. Firstly, we should really try to write down a proper mathematical formula. But in order do to this I need some mathematical notation to refer to the mean absolute deviation. Irritatingly, “mean absolute deviation” and “median absolute deviation” have the same acronym (MAD), which leads to a certain amount of ambiguity, and since R tends to use MAD to refer to the median absolute deviation, I’d better come up with something different for the mean absolute deviation. Sigh. What I’ll do is use AAD instead, short for average absolute deviation. Now that we have some unambiguous notation, here’s the formula that describes what we just calculated: \\[\n\\mbox{}(X) = \\frac{1}{N} \\sum_{i = 1}^N |X_i - \\bar{X}|\n\\]\nThe last thing we need to talk about is how to calculate AAD in R. One possibility would be to do everything using low level commands, laboriously following the same steps that I used when describing the calculations above. However, that’s pretty tedious. You’d end up with a series of commands that might look like this:\n\nX &lt;- c(56, 31,56,8,32)   # enter the data\nX.bar &lt;- mean( X )       # step 1. the mean of the data\nAD &lt;- abs( X - X.bar )   # step 2. the absolute deviations from the mean\nAAD &lt;- mean( AD )        # step 3. the mean absolute deviations\nprint( AAD )             # print the results\n\n[1] 15.52\n\n\nEach of those commands is pretty simple, but there’s just too many of them. And because I find that to be too much typing. If you find youself needing to invoke these commands repeatedly, you might consider building function to do so (Section 2.4).\n\n\n8.2.4 Variance\nAlthough the mean absolute deviation measure has its uses, it’s not the best measure of variability to use. From a purely mathematical perspective, there are some solid reasons to prefer squared deviations rather than absolute deviations. If we do that, we obtain a measure is called the variance, which has a lot of really nice statistical properties that we’ll ignore for now and one massive psychological flaw that we’ll make a big deal out of in a moment. The variance of a data set is sometimes written as \\(\\mbox{Var}(X)\\), but it’s more commonly denoted \\(s^2\\) (the reason for this will become clearer shortly). The formula that we use to calculate the variance of a set of observations is as follows:\n\\[\n\\mbox{Var}(X) = \\frac{1}{N} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right)^2\n\\] \\[\\mbox{Var}(X) = \\frac{\\sum_{i=1}^N \\left( X_i - \\bar{X} \\right)^2}{N}\\]\nAs you can see, it’s basically the same formula that we used to calculate the mean absolute deviation, except that instead of using “absolute deviations” we use “squared deviations”. It is for this reason that the variance is sometimes referred to as the “mean square deviation”.\nNow that we’ve got the basic idea, let’s have a look at a concrete example. Once again, let’s use the five values we have been working with so far. If we follow the same approach that we took last time, we end up with the following table:\n\n\n\nBasic arithmetic operations in R. These five operators are used very frequently throughout the text, so it’s important to be familiar with them at the outset.\n\n\n\n\n\n\n\n\n\nNotation [English]\n\\(i\\) [which game]\n\\(X_i\\) [value]\n\\(X_i - \\bar{X}\\) [deviation from mean]\n\\((X_i - \\bar{X})^2\\) [absolute deviation]\n\n\n\n\n\n1\n56\n19.4\n376.36\n\n\n\n2\n31\n-5.6\n31.36\n\n\n\n3\n56\n19.4\n376.36\n\n\n\n4\n8\n-28.6\n817.96\n\n\n\n5\n32\n-4.6\n21.16\n\n\n\n\n\nThat last column contains all of our squared deviations, so all we have to do is average them. If we do that by typing all the numbers into R by hand…\n\n(376.36 + 31.36 + 376.36 + 817.96 + 21.16) / 5\n\n[1] 324.64\n\n\n… we end up with a variance of 324.64. Exciting, isn’t it? For the moment, let’s ignore the burning question that you’re all probably thinking (i.e., what the heck does a variance of 324.64 actually mean?) and instead talk a bit more about how to do the calculations in R, because this will reveal something very weird.\nAs always, we want to avoid having to type in a whole lot of numbers ourselves. And as it happens, we have the vector X lying around, which we created in the previous section. With this in mind, we can calculate the variance of X by using the following command,\n\nmean( (X - mean(X) )^2)\n\n[1] 324.64\n\n\nand as usual we get the same answer as the one that we got when we did everything by hand. However, I still think that this is too much typing. Fortunately, R has a built in function called var() which does calculate variances. So we could also do this…\n\nvar(X)\n\n[1] 405.8\n\n\nand you get the same… no, wait… you get a completely different answer. That’s just weird. Is R broken? Is this a typo?\nIt’s not a typo, and R is not making a mistake. To get a feel for what’s happening, let’s stop using the tiny data set containing only 5 data points, and switch to the full set ages that we’ve got stored in nba$age vector. First, let’s calculate the variance by using the formula that I described above:\n\nmean( (nba$age - mean(nba$age) )^2)\n\n[1] 18.79822\n\n\nNow let’s use the var() function:\n\nvar(nba$age)\n\n[1] 18.79975\n\n\nHm. These two numbers are very similar this time. That seems like too much of a coincidence to be a mistake. And of course it isn’t a mistake. In fact, it’s very simple to explain what R is doing here, but slightly trickier to explain why R is doing it. So let’s start with the “what”. What R is doing is evaluating a slightly different formula to the one I showed you above. Instead of averaging the squared deviations, which requires you to divide by the number of data points \\(N\\), R has chosen to divide by \\(N-1\\). In other words, the formula that R is using is this one\n\\[\n\\frac{1}{N-1} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right)^2\n\\] It’s easy enough to verify that this is what’s happening, as the following command illustrates:\n\nsum( (nba$age-mean(nba$age))^2 ) / (length(nba$age) - 1)\n\n[1] 18.79975\n\n\nThis is the same answer that R gave us originally when we calculated var(X) originally. So that’s the what. The real question is why R is dividing by \\(N-1\\) and not by \\(N\\). After all, the variance is supposed to be the mean squared deviation, right? So shouldn’t we be dividing by \\(N\\), the actual number of observations in the sample? Well, yes, we should. However, as we’ll discuss in Chapter @ref(estimation), there’s a subtle distinction between “describing a sample” and “making guesses about the population from which the sample came”. Up to this point, it’s been a distinction without a difference. Regardless of whether you’re describing a sample or drawing inferences about the population, the mean is calculated exactly the same way. Not so for the variance, or the standard deviation, or for many other measures besides. What I outlined to you initially (i.e., take the actual average, and thus divide by \\(N\\)) assumes that you literally intend to calculate the variance of the sample. Most of the time, however, you’re not terribly interested in the sample in and of itself. Rather, the sample exists to tell you something about the world. If so, you’re actually starting to move away from calculating a “sample statistic”, and towards the idea of estimating a “population parameter”. However, I’m getting ahead of myself. For now, let’s just take it on faith that R knows what it’s doing, and we’ll revisit the question later on.\nOkay, one last thing. This section so far has read a bit like a mystery novel. I’ve shown you how to calculate the variance, described the weird “\\(N-1\\)” thing that R does and hinted at the reason why it’s there, but I haven’t mentioned the single most important thing…how do you interpret the variance? Descriptive statistics are supposed to describe things, after all, and right now the variance is really just a gibberish number. Unfortunately, the reason why I haven’t given you the human-friendly interpretation of the variance is that there really isn’t one. This is the most serious problem with the variance. Although it has some elegant mathematical properties that suggest that it really is a fundamental quantity for expressing variation, it’s completely useless if you want to communicate with an actual human…variances are completely uninterpretable in terms of the original variable! All the numbers have been squared, and they don’t mean anything anymore. This is a huge issue. For instance, according to our calculations earlier, first value (i.e., 56) is “376.36 points-squared higher than the average”. This is exactly as stupid as it sounds; and so when we calculate a variance of 324.64, we’re in the same situation. You are probably familiar with lots of measurements (height, weight, dollars earned, etc.), but you rarely refer to “pounds squared” or “dollars squared”. It’s not a real unit of measurement, and since the variance is expressed in terms of this gibberish unit, it is totally meaningless to a human.\n\n\n8.2.5 Standard deviation\nOkay, suppose that you like the idea of using the variance because of those nice mathematical properties that I haven’t talked about, but – since you’re a human and not a robot – you’d like to have a measure that is expressed in the same units as the data itself (i.e., points, not points-squared). What should you do? The solution to the problem is obvious: take the square root of the variance, known as the standard deviation, also called the “root mean squared deviation”, or RMSD. This solves out problem fairly neatly: while nobody has a clue what “a variance of XYZ dollars-squared” really means, it’s much easier to understand “a standard deviation of $12”, since it’s expressed in the units the measurements were originally made in. It is traditional to refer to the standard deviation of a sample of data as \\(s\\), though “SD” and “std dev.” are also used at times. Because the standard deviation is equal to the square root of the variance, you probably won’t be surprised to see that the formula is:\n\\[\ns = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right)^2 }\n\\]\nand the R function that we use to calculate it is sd(). However, as you might have guessed from our discussion of the variance, what R actually calculates is slightly different to the formula given above. Just like the we saw with the variance, what R calculates is a version that divides by \\(N-1\\) rather than \\(N\\). For reasons that will make sense when we return to this topic in Chapter@refch:estimation I’ll refer to this new quantity as \\(\\hat\\sigma\\) (read as: “sigma hat”), and the formula for this is\n\\[\n\\hat\\sigma = \\sqrt{ \\frac{1}{N-1} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right)^2 }\n\\]\nWith that in mind, calculating standard deviations in R is simple:\n\nsd(X)\n\n[1] 20.14448\n\n\nInterpreting standard deviations is slightly more complex. Because the standard deviation is derived from the variance, and the variance is a quantity that has little to no meaning that makes sense to us humans, the standard deviation doesn’t have a simple interpretation. As a consequence, most of us just rely on a simple rule of thumb: in general, you should expect 68% of the data to fall within 1 standard deviation of the mean, 95% of the data to fall within 2 standard deviation of the mean, and 99.7% of the data to fall within 3 standard deviations of the mean. This rule tends to work pretty well most of the time, but it’s not exact: it’s actually calculated based on an assumption that the histogram is symmetric and “bell-shaped” (strictly speaking, it assumes measurements are normally-distributed). When data is not symmetric and/or not bell-shaped, the rule is approximately correct. As it turns out, 65.3% of the AFL margins data fall within one standard deviation of the mean. This is shown visually in Figure @ref(fig:aflsd).\n\n\nWarning: `position_stack()` requires non-overlapping x intervals\n\n\n\n\n\nFigure 8.1: An illustration of the standard deviation, applied to nba$age. The red bars in the histogram show how much of the data fall within one standard deviation of the mean.\n\n\n\n\n\n\n8.2.6 Median absolute deviation\nThe last measure of variability that I want to talk about is the median absolute deviation (MAD). The basic idea behind MAD is very simple, and is pretty much identical to the idea behind the mean absolute deviation (Section 8.2.3). The difference is that you use the median everywhere. If we were to frame this idea as a pair of R commands, they would look like this:\n\n# mean absolute deviation from the mean:\nmean( abs(nba$age - mean(nba$age)) )\n\n[1] 3.553141\n\n# *median* absolute deviation from the *median*:\nmedian( abs(nba$age - median(nba$age)) )\n\n[1] 3\n\n\nThis has a straightforward interpretation: every observation in the data set lies some distance away from the typical value (the median). So the MAD is an attempt to describe a typical deviation from a typical value in the data set. It wouldn’t be unreasonable to interpret the MAD value of 3 for our age variable by saying something like this:\n\nThe median age in our data is 26, indicating that a typical player was 26 years old. However, there was a fair amount of variation from player to player: the MAD value was 3, indicating that a typical player’s age would differ from this median value by about 3 years.\n\nAs you’d expect, R has a built in function for calculating MAD, and you will be shocked no doubt to hear that it’s called mad(). However, it’s a little bit more complicated than the functions that we’ve been using previously. If you want to use it to calculate MAD in the exact same way that I have described it above, the command that you need to use specifies two arguments: the data set itself x, and a constant that I’ll explain in a moment. For our purposes, the constant is 1, so our command becomes\n\nmad( x = nba$age, constant = 1 )\n\n[1] 3\n\n\nApart from the weirdness of having to type that constant = 1 part, this is pretty straightforward.\nOkay, so what exactly is this constant = 1 argument? I won’t go into all the details here, but here’s the gist. Although the “raw” MAD value that I’ve described above is completely interpretable on its own terms, that’s not actually how it’s used in a lot of real world contexts. Instead, what happens a lot is that the researcher actually wants to calculate the standard deviation. However, in the same way that the mean is very sensitive to extreme values, the standard deviation is vulnerable to the exact same issue. So, in much the same way that people sometimes use the median as a “robust” way of calculating “something that is like the mean”, it’s not uncommon to use MAD as a method for calculating “something that is like the standard deviation”. Unfortunately, the raw MAD value doesn’t do this. Our raw MAD value is 19.5, and our standard deviation was 26.07. However, what some clever person has shown is that, under certain assumptions (normally-distributed), you can multiply the raw MAD value by 1.4826 and obtain a number that is directly comparable to the standard deviation. As a consequence, the default value of constant is 1.4826, and so when you use the mad() command without manually setting a value, here’s what you get:\n\nmad(nba$age)\n\n[1] 4.4478\n\n\nI should point out, though, that if you want to use this “corrected” MAD value as a robust version of the standard deviation, you really are relying on the assumption that the data are (or at least, are “supposed to be” in some sense) symmetric and basically shaped like a bell curve. That’s really not true for our ages data, so in this case I wouldn’t try to use the MAD value this way.\n\n\n8.2.7 Which measure to use?\nWe’ve discussed quite a few measures of spread (range, IQR, MAD, variance and standard deviation), and hinted at their strengths and weaknesses. Here’s a quick summary:\n\nRange. Gives you the full spread of the data. It’s very vulnerable to outliers, and as a consequence it isn’t often used unless you have good reasons to care about the extremes in the data.\nInterquartile range. Tells you where the “middle half” of the data sits. It’s pretty robust, and complements the median nicely. This is used a lot.\nMean absolute deviation. Tells you how far “on average” the observations are from the mean. It’s very interpretable, but has a few minor issues (not discussed here) that make it less attractive to statisticians than the standard deviation. Used sometimes, but not often.\nVariance. Tells you the average squared deviation from the mean. It’s mathematically elegant, and is probably the “right” way to describe variation around the mean, but it’s completely uninterpretable because it doesn’t use the same units as the data. Almost never used except as a mathematical tool; but it’s buried “under the hood” of a very large number of statistical tools.\nStandard deviation. This is the square root of the variance. It’s fairly elegant mathematically, and it’s expressed in the same units as the data so it can be interpreted pretty well. In situations where the mean is the measure of central tendency, this is the default. This is by far the most popular measure of variation.\nMedian absolute deviation. The typical (i.e., median) deviation from the median value. In the raw form it’s simple and interpretable; in the corrected form it’s a robust way to estimate the standard deviation, for some kinds of data sets. Not used very often, but it does get reported sometimes.\n\nIn short, the IQR and the standard deviation are easily the two most common measures used to report the variability of the data; but there are situations in which the others are used. I’ve described all of them in this book because there’s a fair chance you’ll run into most of these somewhere."
  },
  {
    "objectID": "descriptives.html#sec-skewandkurtosis",
    "href": "descriptives.html#sec-skewandkurtosis",
    "title": "8  Descriptive Statistics",
    "section": "8.3 Skew and kurtosis",
    "text": "8.3 Skew and kurtosis\nThere are two more descriptive statistics that you will sometimes see reported in the psychological literature, known as skew and kurtosis. In practice, neither one is used anywhere near as frequently as the measures of central tendency and variability that we’ve been talking about. Skew is pretty important, so you do see it mentioned a fair bit; but I’ve actually never seen kurtosis reported in a scientific article to date.\n\n\n\n\n\nFigure 8.2: An illustration of skewness. On the left we have a right skewed variable, in the middle we have a data set with no skew, and on the right we have a left skewed data set.\n\n\n\n\nSince it’s the more interesting of the two, let’s start by talking about the skewness. Skewness is basically a measure of asymmetry, and the easiest way to explain it is by drawing some pictures. As Figure 8.2 illustrates, if the data tend to have a lot of extreme large values (i.e., the right tail is “longer” than the left tail) and not so many extremely small values (blue line), then we say that the data are left skewed (or negatively skewed). On the other hand, if there are more extremely small values than extremely large ones (red line) we say that the data are right skewed (or positively skewed). That’s the qualitative idea behind skewness. The actual formula for the skewness of a data set is as follows\n\\[\n\\mbox{skewness}(X) = \\frac{1}{N \\hat{\\sigma}^3} \\sum_{i=1}^N (X_i - \\bar{X})^3\n\\] where \\(N\\) is the number of observations, \\(\\bar{X}\\) is the sample mean, and \\(\\hat{\\sigma}\\) is the standard deviation (the “divide by \\(N-1\\)” version, that is).\nThe final measure that is sometimes referred to, though very rarely in practice, is the kurtosis of a data set. Put simply, kurtosis is a measure of the “pointiness” of a data set, or, equivalently, how thick the tails of the distribution are. We won’t go into it deeply here, but will instead point you to the very nice Wikipedia entry for more details about both the concept of kurtosis and how to quantify it."
  },
  {
    "objectID": "descriptives.html#sec-summary",
    "href": "descriptives.html#sec-summary",
    "title": "8  Descriptive Statistics",
    "section": "8.4 Getting an overall summary of a variable",
    "text": "8.4 Getting an overall summary of a variable\nUp to this point in the chapter I’ve explained several different summary statistics that are commonly used when analysing data, along with specific functions that you can use in R to calculate each one. However, it’s kind of annoying to have to separately calculate means, medians, standard deviations, skews etc. Wouldn’t it be nice if R had some helpful functions that would do all these tedious calculations at once? Something like summary() or describe(), perhaps? Why yes, yes it would. So much so that both of these functions exist. The summary() function is in the base package, so it comes with every installation of R. The describe() function is part of the psych package, which we loaded earlier in the chapter.\n\n8.4.1 “Summarising” a variable\nThe summary() function is an easy thing to use. The basic idea behind the summary() function is that it prints out some useful information about whatever object you specify as the object argument. Because of how R functions work, the behavior of the summary() function differs quite dramatically depending on the type of the object that you give it. Let’s start by giving it a numeric object:\n\nsummary( object = nba$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  18.00   24.00   26.00   27.08   30.00   44.00 \n\n\nFor numeric variables, we get a whole bunch of useful descriptive statistics. It gives us the minimum and maximum values (i.e., the range), the first and third quartiles (25th and 75th percentiles; i.e., the IQR), the mean and the median. In other words, it gives us a pretty good collection of descriptive statistics related to the central tendency and the spread of the data.\nOkay, what about if we feed it a logical vector instead? Let’s say I want to know something about how many “superstars” are in our data set. We might operationalize the concept of a superstar as a player averaging more than 30 points per game for a season. Let’s create a logical variable superstar in which each element is TRUE if the corresponding player was is a superstar according to this definition.\n\nsuperstar &lt;-  nba$pts &gt; 30\nhead(superstar, 10)\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nSo that’s what the superstar variable looks like. Now let’s ask R for a summary()\n\nsummary(object = superstar)\n\n   Mode   FALSE    TRUE \nlogical   12284      21 \n\n\nIn this context, the summary() function gives us a count of the number of TRUE values, the number of FALSE values, and the number of missing values (i.e., the NAs). Pretty reasonable.\nNext, let’s try to give it a factor. If you recall, I’ve previously transformed both the colleg and team_abbreviations variables to factors when plotting. Let’s use team_abbreviations here.\n\nsummary( object = factor(nba$team_abbreviation))\n\nATL BKN BOS CHA CHH CHI CLE DAL DEN DET GSW HOU IND LAC LAL MEM MIA MIL MIN NJN \n421 180 407 288  89 406 433 422 412 400 409 418 410 424 411 352 426 409 399 257 \nNOH NOK NOP NYK OKC ORL PHI PHX POR SAC SAS SEA TOR UTA VAN WAS \n143  32 159 410 239 411 420 399 407 398 413 182 428 397  72 422 \n\n\nFor factors, we get a frequency table, just like we got when we used the table() function. Interestingly, if we leave the team_abbreviation as a column of character vectors, we don’t get the same results:\n\nsummary( object = nba$team_abbreviation)\n\n   Length     Class      Mode \n    12305 character character \n\n\nWe briefly touched on factors in Chapter 5, but this example illustrates in why it is often a good idea to declare your nominal scale variables as factors rather than a character vector. Treating nba$team_abbreviations as a factor, R knows that it should treat it as a nominal scale variable, and so it gives you a much more detailed (and helpful) summary than it would have if I’d left it as a character vector.\n\n\n8.4.2 “Summarising” a data frame\nOkay what about data frames (tibbles)? When you pass a data frame to the summary() function, it produces a slightly condensed summary of each variable inside the data frame. To give you a sense of how this can be useful, let’s try this for nba:\n\nsummary(nba)\n\n      ...1       player_name        team_abbreviation       age       \n Min.   :    0   Length:12305       Length:12305       Min.   :18.00  \n 1st Qu.: 3076   Class :character   Class :character   1st Qu.:24.00  \n Median : 6152   Mode  :character   Mode  :character   Median :26.00  \n Mean   : 6152                                         Mean   :27.08  \n 3rd Qu.: 9228                                         3rd Qu.:30.00  \n Max.   :12304                                         Max.   :44.00  \n                                                                      \n player_height   player_weight      college            country         \n Min.   :160.0   Min.   : 60.33   Length:12305       Length:12305      \n 1st Qu.:193.0   1st Qu.: 90.72   Class :character   Class :character  \n Median :200.7   Median : 99.79   Mode  :character   Mode  :character  \n Mean   :200.6   Mean   :100.37                                        \n 3rd Qu.:208.3   3rd Qu.:108.86                                        \n Max.   :231.1   Max.   :163.29                                        \n                                                                       \n   draft_year    draft_round     draft_number          gp       \n Min.   :1963   Min.   :0.000   Min.   :  0.00   Min.   : 1.00  \n 1st Qu.:1997   1st Qu.:1.000   1st Qu.:  9.00   1st Qu.:31.00  \n Median :2004   Median :1.000   Median : 19.00   Median :57.00  \n Mean   :2004   Mean   :1.304   Mean   : 21.86   Mean   :51.29  \n 3rd Qu.:2010   3rd Qu.:2.000   3rd Qu.: 33.00   3rd Qu.:73.00  \n Max.   :2021   Max.   :8.000   Max.   :165.00   Max.   :85.00  \n NA's   :2224   NA's   :2274    NA's   :2277                    \n      pts              reb              ast           net_rating      \n Min.   : 0.000   Min.   : 0.000   Min.   : 0.000   Min.   :-250.000  \n 1st Qu.: 3.600   1st Qu.: 1.800   1st Qu.: 0.600   1st Qu.:  -6.400  \n Median : 6.700   Median : 3.000   Median : 1.200   Median :  -1.300  \n Mean   : 8.173   Mean   : 3.559   Mean   : 1.814   Mean   :  -2.256  \n 3rd Qu.:11.500   3rd Qu.: 4.700   3rd Qu.: 2.400   3rd Qu.:   3.200  \n Max.   :36.100   Max.   :16.300   Max.   :11.700   Max.   : 300.000  \n                                                                      \n    oreb_pct          dreb_pct        usg_pct           ts_pct      \n Min.   :0.00000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.02100   1st Qu.:0.096   1st Qu.:0.1490   1st Qu.:0.4800  \n Median :0.04100   Median :0.131   Median :0.1810   Median :0.5240  \n Mean   :0.05447   Mean   :0.141   Mean   :0.1849   Mean   :0.5111  \n 3rd Qu.:0.08400   3rd Qu.:0.180   3rd Qu.:0.2170   3rd Qu.:0.5610  \n Max.   :1.00000   Max.   :1.000   Max.   :1.0000   Max.   :1.5000  \n                                                                    \n    ast_pct          season         \n Min.   :0.0000   Length:12305      \n 1st Qu.:0.0660   Class :character  \n Median :0.1030   Mode  :character  \n Mean   :0.1314                     \n 3rd Qu.:0.1780                     \n Max.   :1.0000                     \n                                    \n\n\nHere, we see many descriptive statistics for the various columns in our data frame. We can also see that many of our variables (e.g., player_name, team_abbreviation, college, country, etc.) are character vectors and should, if we wish to do more processing, be converted to factors."
  },
  {
    "objectID": "descriptives.html#sec-zscore",
    "href": "descriptives.html#sec-zscore",
    "title": "8  Descriptive Statistics",
    "section": "8.5 Standard scores",
    "text": "8.5 Standard scores\nSuppose my friend is putting together a new questionnaire intended to measure “grumpiness”. The survey has 50 questions, which you can answer in a grumpy way or not. Across a big sample (hypothetically, let’s imagine a million people or so!) the data are fairly normally distributed, with the mean grumpiness score being 17 out of 50 questions answered in a grumpy way, and the standard deviation is 5. In contrast, when I take the questionnaire, I answer 35 out of 50 questions in a grumpy way. So, how grumpy am I? One way to think about would be to say that I have grumpiness of 35/50, so you might say that I’m 70% grumpy. But that’s a bit weird, when you think about it. If my friend had phrased her questions a bit differently, people might have answered them in a different way, so the overall distribution of answers could easily move up or down depending on the precise way in which the questions were asked. So, I’m only 70% grumpy with respect to this set of survey questions. Even if it’s a very good questionnaire, this isn’t very a informative statement.\nA simpler way around this is to describe my grumpiness by comparing me to other people. Shockingly, out of my friend’s sample of 1,000,000 people, only 159 people were as grumpy as me, suggesting that I’m in the top 0.016% of people for grumpiness. This makes much more sense than trying to interpret the raw data. This idea – that we should describe my grumpiness in terms of the overall distribution of the grumpiness of humans – is the qualitative idea that standardisation attempts to get at. One way to do this is to do exactly what I just did, and describe everything in terms of percentiles. However, the problem with doing this is that “it’s lonely at the top”. Suppose that my friend had only collected a sample of 1000 people (still a pretty big sample for the purposes of testing a new questionnaire, I’d like to add), and this time gotten a mean of 16 out of 50 with a standard deviation of 5, let’s say. The problem is that almost certainly, not a single person in that sample would be as grumpy as me.\nHowever, all is not lost. A different approach is to convert my grumpiness score into a standard score, also referred to as a \\(z\\)-score. The standard score is defined as the number of standard deviations above the mean that my grumpiness score lies. To phrase it in “pseudo-maths” the standard score is calculated like this:\n\\[\n\\mbox{standard score} = \\frac{\\mbox{raw score} - \\mbox{mean}}{\\mbox{standard deviation}}\n\\]\nIn actual math, the equation for the \\(z\\)-score is\n\\[\nz_i = \\frac{X_i - \\bar{X}}{\\hat\\sigma}\n\\] So, going back to the grumpiness data, we can now transform my raw grumpiness into a standardized grumpiness score. If the mean is 17 and the standard deviation is 5 then my standardized grumpiness score would be:\n\\[\nz = \\frac{35 - 17}{5} = 3.6\n\\] To interpret this value, recall the rough heuristic that I provided in Section 8.2.5, in which I noted that 99.7% of values are expected to lie within 3 standard deviations of the mean. So the fact that my grumpiness corresponds to a \\(z\\) score of 3.6 indicates that I’m very grumpy indeed. Without worrying too much about what it does we can use a function called pnorm() that allows us to be a bit more precise than this. Specifically, it allows us to calculate a theoretical percentile rank for my grumpiness:\n\npnorm( 3.6 )\n\n[1] 0.9998409\n\n\nAt this stage, this command doesn’t make too much sense, but don’t worry too much about it. It’s not important for now. But the output is fairly straightforward: it suggests that I’m grumpier than 99.98% of people. Sounds about right.\nIn addition to allowing you to interpret a raw score in relation to a larger population (and thereby allowing you to make sense of variables that lie on arbitrary scales), standard scores serve a second useful function. Standard scores can be compared to one another in situations where the raw scores can’t. Suppose, for instance, my friend also had another questionnaire that measured extroversion using a 24 items questionnaire. The overall mean for this measure turns out to be 13 with standard deviation 4; and I scored a 2. As you can imagine, it doesn’t make a lot of sense to try to compare my raw score of 2 on the extroversion questionnaire to my raw score of 35 on the grumpiness questionnaire. The raw scores for the two variables are “about” fundamentally different things, so this would be like comparing apples to oranges.\nWhat about the standard scores? Well, this is a little different. If we calculate the standard scores, we get \\(z = (35-17)/5 = 3.6\\) for grumpiness and \\(z = (2-13)/4 = -2.75\\) for extroversion. These two numbers can be compared to each other. I’m much less extroverted than most people (\\(z = -2.75\\)) and much grumpier than most people (\\(z = 3.6\\)): but the extent of my unusualness is much more extreme for grumpiness (since 3.6 is a bigger number than 2.75). Because each standardized score is a statement about where an observation falls relative to its own population, it is possible to compare standardized scores across completely different variables."
  },
  {
    "objectID": "descriptives.html#sec-correl",
    "href": "descriptives.html#sec-correl",
    "title": "8  Descriptive Statistics",
    "section": "8.6 Correlations",
    "text": "8.6 Correlations\nUp to this point we have focused entirely on how to construct descriptive statistics for a single variable. What we haven’t done is talked about how to describe the relationships between variables in the data. To do that, we want to talk mostly about the correlation between variables.\nTo give us a general sense of how closely related two variables are, we can draw scatterplots (Section 7.3). However, we might wish to say a bit more than that. For instance, let’s inspect the relationship between nba$pts and nba$ast (Figure 8.3).\n\n\n\n\n\nFigure 8.3: Scatterplot showing the relationship between pts and ast\n\n\n\n\nOr we can look at the relationship between players’ heights and weights.\n\n\n\n\n\nFigure 8.4: Scatterplot showing the relationship between player_height and player_weight\n\n\n\n\nIt’s clear that the relationship is qualitatively the same in both cases: more points means more assists (and vice versa) and taller means heavier (and vice versa). However, it’s also seems that the two relationship between height and weight is stronger than the relationship between points and assists.\n\n\nWarning: Removed 2277 rows containing missing values (`geom_point()`).\n\n\n\n\n\nFigure 8.5: Scatterplot showing the relationship between pts and draft_number\n\n\n\n\nIn contrast, let’s consider Figure 8.5). Here, the the direction of the relationship is different. As a player’s position goes up (i.e., teams apparently were not overly motivated to spend an early draft pick on him), the player also tends to average fewer points per game (a negative relationship).\n\n8.6.1 The correlation coefficient\nWe can make these ideas a bit more explicit by introducing the idea of a correlation coefficient (or, more specifically, Pearson’s correlation coefficient), which is traditionally denoted by \\(r\\). The correlation coefficient between two variables \\(X\\) and \\(Y\\) (sometimes denoted \\(r_{XY}\\)), which we’ll define more precisely in the next section, is a measure that varies from \\(-1\\) to \\(1\\). When \\(r = -1\\) it means that we have a perfect negative relationship, and when \\(r = 1\\) it means we have a perfect positive relationship. When \\(r = 0\\), there’s no relationship at all.\nThe formula for the Pearson’s correlation coefficient can be written in several different ways. I think the simplest way to write down the formula is to break it into two steps. Firstly, let’s introduce the idea of a covariance. The covariance between two variables \\(X\\) and \\(Y\\) is a generalization of the notion of the variance (Section 8.2.4); it’s a mathematically simple way of describing the relationship between two variables that isn’t terribly informative to humans:\n\\[\n\\mbox{Cov}(X,Y) = \\frac{1}{N-1} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right) \\left( Y_i - \\bar{Y} \\right)\n\\]\nBecause we’re multiplying (i.e., taking the “product” of) a quantity that depends on \\(X\\) by a quantity that depends on \\(Y\\) and then averaging (dividing by \\(N-1\\) rather than \\(N\\), just like we saw with the variance and the standard deviation), you can think of the formula for the covariance as an “average cross product” between \\(X\\) and \\(Y\\). The covariance has the nice property that, if \\(X\\) and \\(Y\\) are entirely unrelated, then the covariance is exactly zero. If the relationship between them is positive, then the covariance is also positive; and if the relationship is negative then the covariance is also negative. In other words, the covariance captures the basic qualitative idea of correlation. Unfortunately, the raw magnitude of the covariance isn’t easy to interpret: it depends on the units in which \\(X\\) and \\(Y\\) are expressed, and worse yet, the actual units that the covariance itself is expressed in are really weird. For instance, if \\(X\\) refers to the player_height variable (units: centimeters) and \\(Y\\) refers to the player_weight variable (units: kilograms), then the units for their covariance are “centimeters \\(\\times\\) kilograms”. And I have no idea what that would even mean.\nThe Pearson correlation coefficient \\(r\\) fixes this interpretation problem by standardising the covariance, in pretty much the exact same way that the \\(z\\)-score standardises a raw score: by dividing by the standard deviation. However, because we have two variables that contribute to the covariance, the standardisation only works if we divide by both standard deviations. In other words, the correlation between \\(X\\) and \\(Y\\) can be written as follows: \\[\nr_{XY}  = \\frac{\\mbox{Cov}(X,Y)}{ \\hat{\\sigma}_X \\ \\hat{\\sigma}_Y}\n\\] By doing this standardisation, not only do we keep all of the nice properties of the covariance discussed earlier, but the actual values of \\(r\\) are on a meaningful scale: \\(r= 1\\) implies a perfect positive relationship, and \\(r = -1\\) implies a perfect negative relationship. I’ll expand a little more on this point later, in Section@refsec:interpretingcorrelations. But before I do, let’s look at how to calculate correlations in R.\n\n\n8.6.2 Calculating correlations in R\nCalculating correlations in R can be done using the cor() command. The simplest way to use the command is to specify two input arguments x and y, each one corresponding to one of the variables. The following extract illustrates the basic usage of the function:\n\ncor(nba$pts, nba$ast)\n\n[1] 0.6609489\n\n\nLet’s take a quick look at the relationship between a player’s draft number and that player’s average points per game:\n\ncor(nba$draft_number, nba$pts)\n\n[1] NA\n\n\nHere we can see that appears to be a problem. cor() has returned the unhelpful result of NA. This is because there are missing values in nba$draft_number (we converted the “Undrafted” values to NA when we loaded the CSV file, Section 6.1). So the correlation between these variables is actually undefined. But this isn’t super helpful. We may wish to determine the correlation among players who have valid values for both variables, both draft_number and pts. To get this, we need to tell cor to use any draft_number-pts pairs that consist of two valid (non-NA) values. In other words, any “complete” observations.\n\ncor(nba$draft_number, nba$pts, use=\"complete.obs\")\n\n[1] -0.3733803\n\n\nThe cor() function is actually a bit more powerful than these simple examples suggest. For example, you can also calculate a complete “correlation matrix”, between all pairs of variables in a given data frame. Let’s do that for a selection of variables in our nba data frame. We have some missing values, so we will need to filter those out along the way.\n\nnba %&gt;%\n    select(pts, ast, player_height, player_weight, draft_number) %&gt;%\n    cor(use = \"complete.obs\") %&gt;%\n    round(2)\n\n                pts   ast player_height player_weight draft_number\npts            1.00  0.65         -0.10         -0.06        -0.37\nast            0.65  1.00         -0.49         -0.41        -0.24\nplayer_height -0.10 -0.49          1.00          0.82        -0.11\nplayer_weight -0.06 -0.41          0.82          1.00        -0.10\ndraft_number  -0.37 -0.24         -0.11         -0.10         1.00\n\n\nHere we can quickly see the positive relationships between pts and ast and between player_height and player_weight, as well as the negative relationship between draft_number and both pts and ast. You might also note the negative relationships between player_height (and player_weight) and both ast. This might may sense to you if you knew anything about basketball (but I don’t).\n\n\n8.6.3 Interpreting a correlation\nNaturally, in real life you don’t see many correlations of 1. So how should you interpret a correlation of, say \\(r= .4\\)? The honest answer is that it really depends on what you want to use the data for, and on how strong the correlations in your field tend to be. A friend of mine in engineering once argued that any correlation less than \\(.95\\) is completely useless (I think he was exaggerating, even for engineering). On the other hand there are real cases – even in psychology – where you should really expect correlations that strong. For instance, one of the benchmark data sets used to test theories of how people judge similarities is so clean that any theory that can’t achieve a correlation of at least \\(.9\\) really isn’t deemed to be successful. However, when looking for (say) elementary correlates of intelligence (e.g., inspection time, response time), if you get a correlation above \\(.3\\) you’re doing very very well. In short, the interpretation of a correlation depends a lot on the context.\nHowever, something that can never be stressed enough is that you should always look at the scatterplot before attaching any interpretation to a correlation. A correlation might not mean what you think it means. The classic illustration of this is “Anscombe’s Quartet” Anscombe (1973), which is a collection of four data sets. Each data set has two variables, an \\(X\\) and a \\(Y\\). For each data set, the mean value for \\(X\\) is 9 and the mean for \\(Y\\) is 7.5. The, standard deviations for all \\(X\\) variables are almost identical, as are those for the the \\(Y\\) variables. And in each case the correlation between \\(X\\) and \\(Y\\) is \\(r = 0.816\\). You can verify this yourself, since the data set comes distributed with R. The commands would be:\n\ncor( anscombe$x1, anscombe$y1 )\n\n[1] 0.8164205\n\ncor( anscombe$x2, anscombe$y2 )\n\n[1] 0.8162365\n\n\nand so on.\nYou’d think that these four data sets would look pretty similar to one another. They do not. If we draw scatterplots of \\(X\\) against \\(Y\\) for all four variables, as shown in Figure 8.6 we see that all four of these are spectacularly different to each other.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nFigure 8.6: Anscombe’s quartet. All four of these data sets have a Pearson correlation of \\(r = .816\\), but they are qualitatively different from one another.”\n\n\n\n\nThe lesson here, which so very many people seem to forget in real life is “always graph your raw data” (Chapter 7).\n\n\n8.6.4 Spearman’s rank correlations\nThe Pearson correlation coefficient is useful for a lot of things, but it does have shortcomings. One issue in particular stands out: what it actually measures is the strength of the linear relationship between two variables. In other words, what it gives you is a measure of the extent to which the data all tend to fall on a single, perfectly straight line. Often, this is a pretty good approximation to what we mean when we say “relationship”, and so the Pearson correlation is a good thing to calculation. Sometimes, it isn’t.\nOne very common situation where the Pearson correlation isn’t quite the right thing to use arises when an increase in one variable \\(X\\) really is reflected in an increase in another variable \\(Y\\), but the nature of the relationship isn’t necessarily linear. An example of this might be the relationship between effort and reward when studying for an exam. If you put in zero effort (\\(X\\)) into learning a subject, then you should expect a grade of 0% (\\(Y\\)). However, a little bit of effort will cause a massive improvement: just turning up to lectures means that you learn a fair bit, and if you just turn up to classes, and scribble a few things down so your grade might rise to 35%, all without a lot of effort. However, you just don’t get the same effect at the other end of the scale. As everyone knows, it takes a lot more effort to get a grade of 90% than it takes to get a grade of 55%. What this means is that, if I’ve got data looking at study effort and grades, there’s a pretty good chance that Pearson correlations will be misleading.\nHow should we address this? Actually, it’s really easy: if we’re looking for ordinal relationships, all we have to do is treat the data as if it were ordinal scale! So, instead of measuring effort in terms of “hours worked”, we can rank all 10 of our students in order of hours worked. That is, student 1 did the least work out of anyone (2 hours) so they get the lowest rank (rank = 1). Student 4 was the next laziest, putting in only 6 hours of work in over the whole semester, so they get the next lowest rank (rank = 2). Notice that I’m using “rank =1” to mean “low rank”. Sometimes in everyday language we talk about “rank = 1” to mean “top rank” rather than “bottom rank”. So be careful: you can rank “from smallest value to largest value” (i.e., small equals rank 1) or you can rank “from largest value to smallest value” (i.e., large equals rank 1). In this case, I’m ranking from smallest to largest, because that’s the default way that R does it. But in real life, it’s really easy to forget which way you set things up, so you have to put a bit of effort into remembering!\nWe can get R to construct these rankings using the rank() function, like this:\n\nhours_rank &lt;- rank(hours)   # rank students by hours worked\ngrade_rank &lt;- rank(grade)   # rank students by grade received\n\nAs the table above shows, these two rankings are identical, so if we now correlate them we get a perfect relationship:\n\ncor(hours_rank, grade_rank)\n\nWhat we’ve just re-invented is Spearman’s rank order correlation, usually denoted \\(\\rho\\) to distinguish it from the Pearson correlation \\(r\\). We can calculate Spearman’s \\(\\rho\\) using R in two different ways. Firstly we could do it the way I just showed, using the rank() function to construct the rankings, and then calculate the Pearson correlation on these ranks. However, that’s way too much effort to do every time. It’s much easier to just specify the method argument of the cor() function.\n\ncor(hours, grade, method = \"spearman\")\n\nThe default value of the method argument is \"pearson\", which is why we didn’t have to specify it earlier on when we were doing Pearson correlations.\n\n\n\n\nAnscombe, Francis J. 1973. “Graphs in Statistical Analysis.” The American Statistician 27 (1): 17–21."
  },
  {
    "objectID": "single.html",
    "href": "single.html",
    "title": "9  Simple",
    "section": "",
    "text": "What is regression"
  },
  {
    "objectID": "multiple.html",
    "href": "multiple.html",
    "title": "10  Multiple",
    "section": "",
    "text": "What is multiple regression"
  },
  {
    "objectID": "anova.html",
    "href": "anova.html",
    "title": "11  ANOVA",
    "section": "",
    "text": "What is ANOVA\nWhat is ANCOVA"
  },
  {
    "objectID": "sampling.html#sec-sampling-activity",
    "href": "sampling.html#sec-sampling-activity",
    "title": "12  Sampling",
    "section": "12.1 Sampling bowl activity",
    "text": "12.1 Sampling bowl activity\nLet’s start with a hands-on activity.\n\n12.1.1 What proportion of this bowl’s balls are red?\nTake a look at the bowl in Figure 12.1. It has a certain number of red and a certain number of white balls all of equal size. Furthermore, it appears the bowl has been mixed beforehand, as there does not seem to be any coherent pattern to the spatial distribution of the red and white balls.\nLet’s now ask ourselves, what proportion of this bowl’s balls are red?\n\n\n\nFigure 12.1: A bowl with red and white balls.\n\n\nOne way to answer this question would be to perform an exhaustive count: remove each ball individually, count the number of red balls and the number of white balls, and divide the number of red balls by the total number of balls. However, this would be a long and tedious process.\n\n\n12.1.2 Using the shovel once\nInstead of performing an exhaustive count, let’s insert a shovel into the bowl as seen in Figure 12.2. Using the shovel, let’s remove \\(5 \\cdot 10 = 50\\) balls, as seen in Figure Figure 12.3.\n\n\n\nFigure 12.2: Inserting a shovel into the bowl.\n\n\n\n\n\nFigure 12.3: Removing 50 balls from the bowl.\n\n\nObserve that 17 of the balls are red and thus 0.34 = 34% of the shovel’s balls are red. We can view the proportion of balls that are red in this shovel as a guess of the proportion of balls that are red in the entire bowl. While not as exact as doing an exhaustive count of all the balls in the bowl, our guess of 34% took much less time and energy to make.\nHowever, say, we started this activity over from the beginning. In other words, we replace the 50 balls back into the bowl and start over. Would we remove exactly 17 red balls again? In other words, would our guess at the proportion of the bowl’s balls that are red be exactly 34% again? Maybe?\nWhat if we repeated this activity several times following the process shown in Figure @ref(fig:sampling-exercise-3b)? Would we obtain exactly 17 red balls each time? In other words, would our guess at the proportion of the bowl’s balls that are red be exactly 34% every time? Surely not. Let’s repeat this exercise several times with the help of 33 groups of friends to understand how the value differs with repetition.\n\n\n12.1.3 Using the shovel 33 times\nEach of our 33 groups of friends will do the following:\n\nUse the shovel to remove 50 balls each.\nCount the number of red balls and thus compute the proportion of the 50 balls that are red.\nReturn the balls into the bowl.\nMix the contents of the bowl a little to not let a previous group’s results influence the next group’s.\n\nEach of our 33 groups of friends make note of their proportion of red balls from their sample collected. Each group then notes the proportion of their 50 balls that were red. If we entered each group’s measurements into a dta frame, it would look something like this:\n\n\nRows: 33 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): group\ndbl (3): replicate, red_balls, prop_red\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 33 × 4\n   group            replicate red_balls prop_red\n   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 Ilyas, Yohan             1        21     0.42\n 2 Morgan, Terrance         2        17     0.34\n 3 Martin, Thomas           3        21     0.42\n 4 Clark, Frank             4        21     0.42\n 5 Riddhi, Karina           5        18     0.36\n 6 Andrew, Tyler            6        19     0.38\n 7 Julia                    7        19     0.38\n 8 Rachel, Lauren           8        11     0.22\n 9 Daniel, Caroline         9        15     0.3 \n10 Josh, Maeve             10        17     0.34\n# ℹ 23 more rows\n\n\nObserve for each group that we have their names, the number of red_balls they obtained, and the corresponding proportion out of 50 balls that were red named prop_red. We also have a replicate variable enumerating each of the 33 groups. We chose this name because each row can be viewed as one instance of a replicated (in other words repeated) activity: using the shovel to remove 50 balls and computing the proportion of those balls that are red.\nFigure 12.4 visualizes the distribution of these 33 proportions using a histogram (see Section 7.5 for a refresher on histograms).\n\n\n\n\n\nFigure 12.4: ?(caption)\n\n\n\n\nObserve the following:\n\nAt the low end, two groups removed 50 balls from the bowl with proportion red between 0.20 and 0.25.\nAt the high end, a single group removed 50 balls from the bowl with proportion between 0.45 and 0.5 red.\nHowever, the most frequently occurring proportions are between 0.30 and 0.35 red, right in the middle of the distribution.\nThe shape of this distribution is approximately bell-shaped.\n\n\n\n12.1.4 What did we just do?\nWhat we just demonstrated in this activity is the statistical concept of sampling. We would like to know the proportion of the bowl’s balls that are red. Because the bowl has a large number of balls, performing an exhaustive count of the red and white balls would be time-consuming. We thus extracted a sample of 50 balls using the shovel to make an estimate. Using this sample of 50 balls, we estimated the proportion of the bowl’s balls that are red to be 34%.\nMoreover, because we mixed the balls before each use of the shovel, the samples were randomly drawn. Because each sample was drawn at random, the samples were different from each other. Because the samples were different from each other, we obtained the different proportions red observed in Figure 12.4. This is known as the concept of sampling variation.\nThe purpose of this sampling activity was to develop an understanding of two key concepts relating to sampling:\n\nUnderstanding the effect of sampling variation.\nUnderstanding the effect of sample size on sampling variation."
  },
  {
    "objectID": "sampling.html#sec-sampling-simulation",
    "href": "sampling.html#sec-sampling-simulation",
    "title": "12  Sampling",
    "section": "12.2 Virtual sampling",
    "text": "12.2 Virtual sampling\nIn the exercise above, we performed a tactile sampling activity by hand. In other words, we used a physical bowl of balls and a physical shovel. We performed this sampling activity by hand first so that we could develop a firm understanding of the root ideas behind sampling. In this section, we’ll mimic this tactile sampling activity with a virtual sampling activity using a computer. In other words, we’ll use a virtual analog to the bowl of balls and a virtual analog to the shovel.\n\n12.2.1 Using the virtual shovel once\nLet’s start by performing the virtual analog of the tactile sampling exercise. We first need a virtual analog of the bowl seen in Figure Figure 12.1. To this end, we included a data frame named bowl in the moderndive package. The rows of bowl correspond exactly with the contents of the actual bowl.\n\nbowl&lt;-read_csv(\"data/bowl.csv\")\n\nRows: 2400 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): color\ndbl (1): ball_ID\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(bowl)\n\n# A tibble: 6 × 2\n  ball_ID color\n    &lt;dbl&gt; &lt;chr&gt;\n1       1 white\n2       2 white\n3       3 white\n4       4 red  \n5       5 white\n6       6 white\n\n\nObserve that bowl has 2400 rows, telling us that the bowl contains 2400 equally sized balls. The first variable ball_ID is used as an identification variable; none of the balls in the actual bowl are marked with numbers. The second variable color indicates whether a particular virtual ball is red or white.\nNow that we have a virtual analog of our bowl, we next need a virtual analog to the shovel seen in Figure 12.2 to generate virtual samples of 50 balls. We’re going to put together a function called rep_sample_n() (a very similar function is available in the infer package). This function allows us to take repeated, or replicated, samples of size n.\n\nrep_sample_n &lt;- function(tbl,\n                         size,\n                         replace = FALSE,\n                         reps = 1) {\n    n &lt;- nrow(tbl)\n    i &lt;- unlist(replicate(reps,\n                          sample.int(n, size, replace = replace),\n                          simplify = FALSE))\n    \n    rep_tbl &lt;- cbind(replicate = rep(1:reps,\n                                     rep(size, reps)),\n                     tbl[i, ])\n    \n    group_by(rep_tbl, replicate)\n}\n\nLet’s show an example of this function in action. Let’s first use the tibble() function to manually create a data frame of five fruit called fruit_basket.\n\nfruit_basket &lt;- tibble(\n  fruit = c(\"Mango\", \"Tangerine\", \"Apricot\", \"Pamplemousse\", \"Lime\")\n)\n\nNow let’s try out rep_sample_n() by asking for a single sample from our fruit basket.\n\nrep_sample_n(fruit_basket, size=3)\n\n# A tibble: 3 × 2\n# Groups:   replicate [1]\n  replicate fruit    \n      &lt;int&gt; &lt;chr&gt;    \n1         1 Apricot  \n2         1 Mango    \n3         1 Tangerine\n\n\nNow let’s try asking for 4 different samples.\n\nrep_sample_n(fruit_basket, size=3, reps=4)\n\n# A tibble: 12 × 2\n# Groups:   replicate [4]\n   replicate fruit       \n       &lt;int&gt; &lt;chr&gt;       \n 1         1 Mango       \n 2         1 Apricot     \n 3         1 Tangerine   \n 4         2 Mango       \n 5         2 Pamplemousse\n 6         2 Lime        \n 7         3 Apricot     \n 8         3 Tangerine   \n 9         3 Mango       \n10         4 Tangerine   \n11         4 Apricot     \n12         4 Mango       \n\n\nNote that the replicate variable indicates which “use” of our virtual shovel that row corresponds to. So rows where replicate=1 correspond to our first “scoop” of fruit, rows where replicate=12 correspond to our second “scoop” of fruit, and so on.\nNow let’s return to our bowl and take a single scoop.\n\nvirtual_shovel &lt;- bowl %&gt;% \n  rep_sample_n(size = 50)\nvirtual_shovel\n\n# A tibble: 50 × 3\n# Groups:   replicate [1]\n   replicate ball_ID color\n       &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;\n 1         1    2086 white\n 2         1    1664 red  \n 3         1      57 white\n 4         1     498 white\n 5         1     323 red  \n 6         1    1260 white\n 7         1    1320 white\n 8         1    1700 white\n 9         1     245 white\n10         1    1964 white\n# ℹ 40 more rows\n\n\nObserve that virtual_shovel has 50 rows corresponding to our virtual sample of size 50. The ball_ID variable identifies which of the 2400 balls from bowl are included in our sample of 50 balls while color denotes its color, and replicate again indicates which “scoop” the row came from.\nLet’s compute the proportion of balls in our virtual sample that are red using the dplyr data wrangling verbs you learned in Chapter 6 First, for each of our 50 sampled balls, let’s identify if it is red or not using a test for equality with ==. Let’s create a new Boolean variable is_red using the mutate() function from Section 6.6:\n\nvirtual_shovel %&gt;% \n  mutate(is_red = (color == \"red\"))\n\n# A tibble: 50 × 4\n# Groups:   replicate [1]\n   replicate ball_ID color is_red\n       &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; \n 1         1    2086 white FALSE \n 2         1    1664 red   TRUE  \n 3         1      57 white FALSE \n 4         1     498 white FALSE \n 5         1     323 red   TRUE  \n 6         1    1260 white FALSE \n 7         1    1320 white FALSE \n 8         1    1700 white FALSE \n 9         1     245 white FALSE \n10         1    1964 white FALSE \n# ℹ 40 more rows\n\n\nObserve that for every row where color == \"red\", the Boolean (logical) value TRUE is returned and for every row where color is not equal to \"red\", the Boolean FALSE is returned.\nSecond, let’s compute the number of balls out of 50 that are red using the summarize() function. Recall from Section 6.4 that summarize() takes a data frame with many rows and returns a data frame with a single row containing summary statistics, like the mean() or median(). In this case, we use the sum():\n\nvirtual_shovel %&gt;% \n  mutate(is_red = (color == \"red\")) %&gt;% \n  summarize(num_red = sum(is_red))\n\n# A tibble: 1 × 2\n  replicate num_red\n      &lt;int&gt;   &lt;int&gt;\n1         1      15\n\n\nWhy does this work? Because R treats TRUE like the number 1 and FALSE like the number 0. So summing the number of TRUEs and FALSEs is equivalent to summing 1’s and 0’s. In the end, this operation counts the number of balls where color is red. In our case, 15 of the 50 balls were red. However, you might have gotten a different number red because of the randomness of the virtual sampling.\nThird and lastly, let’s compute the proportion of the 50 sampled balls that are red by dividing num_red by 50:\n\nvirtual_shovel %&gt;% \n  mutate(is_red = color == \"red\") %&gt;% \n  summarize(num_red = sum(is_red)) %&gt;% \n  mutate(prop_red = num_red / 50)\n\n# A tibble: 1 × 3\n  replicate num_red prop_red\n      &lt;int&gt;   &lt;int&gt;    &lt;dbl&gt;\n1         1      15      0.3\n\n\nIn other words, 30% of this virtual sample’s balls were red. Let’s make this code a little more compact and succinct by combining the first mutate() and the summarize() as follows:\n\nvirtual_shovel %&gt;% \n  summarize(num_red = sum(color == \"red\")) %&gt;% \n  mutate(prop_red = num_red / 50)\n\n# A tibble: 1 × 3\n  replicate num_red prop_red\n      &lt;int&gt;   &lt;int&gt;    &lt;dbl&gt;\n1         1      15      0.3\n\n\nGreat! 30% of virtual_shovel’s 50 balls were red! So based on this particular sample of 50 balls, our guess at the proportion of the bowl’s balls that are red is 30%. But remember from our earlier tactile sampling activity that if we repeat this sampling, we will not necessarily obtain the same value of 30% again. There will likely be some variation. In fact, our 33 groups of friends computed 33 such proportions whose distribution we visualized in Figure 12.4. We saw that these estimates varied. Let’s now perform the virtual analog of having 33 groups of students use the sampling shovel!\n\n\n12.2.2 Using the virtual shovel 33 times\nRecall that in our tactile sampling exercise in Section 12.1, we had 33 groups of students each use the shovel, yielding 33 samples of size 50 balls. We then used these 33 samples to compute 33 proportions. In other words, we repeated/replicated using the shovel 33 times. We can perform this repeated/replicated sampling virtually by once again using our virtual shovel function rep_sample_n(), but by adding the reps = 33 argument. This is telling R that we want to repeat the sampling 33 times.\nWe’ll save these results in a data frame called virtual_samples. Though we provide a preview of the first 10 rows of virtual_samples in what follows, we highly suggest you scroll through its contents using RStudio’s spreadsheet viewer by running View(virtual_samples).\n\nvirtual_samples &lt;- bowl %&gt;% \n  rep_sample_n(size = 50, reps = 33)\nvirtual_samples\n\n# A tibble: 1,650 × 3\n# Groups:   replicate [33]\n   replicate ball_ID color\n       &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;\n 1         1     158 red  \n 2         1    1575 red  \n 3         1    1297 white\n 4         1      89 red  \n 5         1    1190 red  \n 6         1    1923 white\n 7         1    1976 white\n 8         1     504 red  \n 9         1     150 white\n10         1    2255 red  \n# ℹ 1,640 more rows\n\n\nObserve in the spreadsheet viewer that the first 50 rows of replicate are equal to 1 while the next 50 rows of replicate are equal to 2. This is telling us that the first 50 rows correspond to the first sample of 50 balls while the next 50 rows correspond to the second sample of 50 balls. This pattern continues for all reps = 33 replicates and thus virtual_samples has 33 \\(\\cdot\\) 50 = 1650 rows.\nLet’s now take virtual_samples and compute the resulting 33 proportions red. We’ll use the same dplyr verbs as before, but this time with an additional group_by() of the replicate variable. Recall from Section Section 6.5 that by assigning the grouping variable “meta-data” before we summarize(), we’ll obtain 33 different proportions red. We display a preview of the first 10 out of 33 rows:\n\nvirtual_prop_red &lt;- virtual_samples %&gt;% \n  group_by(replicate) %&gt;% \n  summarize(red = sum(color == \"red\")) %&gt;% \n  mutate(prop_red = red / 50)\nvirtual_prop_red\n\n# A tibble: 33 × 3\n   replicate   red prop_red\n       &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;\n 1         1    20     0.4 \n 2         2    17     0.34\n 3         3    21     0.42\n 4         4    21     0.42\n 5         5    18     0.36\n 6         6    22     0.44\n 7         7    18     0.36\n 8         8    18     0.36\n 9         9    19     0.38\n10        10    25     0.5 \n# ℹ 23 more rows\n\n\nAs with our 33 groups of friends’ tactile samples, there is variation in the resulting 33 virtual proportions red. Let’s visualize this variation in a histogram in Figure 12.5. Note that we add binwidth = 0.05 and boundary = 0.4 arguments as well. Recall that setting boundary = 0.4 ensures a binning scheme with one of the bins’ boundaries at 0.4. Since the binwidth = 0.05 is also set, this will create bins with boundaries at 0.30, 0.35, 0.45, 0.5, etc. as well.\n\n\n\n\n\nFigure 12.5: Distribution of 33 proportions based on 33 samples of size 50.\n\n\n\n\nObserve that we occasionally obtained proportions red that are less than 30%. On the other hand, we occasionally obtained proportions that are greater than 45%. However, the most frequently occurring proportions were between 35% and 40%. Why do we have these differences in proportions red? Because of sampling variation.\nLet’s now compare our virtual results with our tactile results from the previous section in Figure @ref(fig:tactile-vs-virtual). Observe that both histograms are somewhat similar in their center and variation, although not identical. These slight differences are again due to random sampling variation. Furthermore, observe that both distributions are somewhat bell-shaped.\n\nbind_rows(\n    virtual_prop_red %&gt;%\n        mutate(type = \"Virtual sampling\"),\n    tactile_prop_red %&gt;%\n        select(replicate, red = red_balls, prop_red) %&gt;%\n        mutate(type = \"Tactile sampling\")\n) %&gt;%\n    mutate(type = factor(type, levels = c(\"Virtual sampling\", \"Tactile sampling\"))) %&gt;%\n    ggplot(aes(x = prop_red)) +\n    geom_histogram(binwidth = 0.05,\n                   boundary = 0.4,\n                   color = \"white\") +\n    facet_wrap( ~ type) +\n    labs(x = \"Proportion of 50 balls that were red\",\n         title = \"Comparing distributions\")\n\n\n\n\nFigure 12.6: Comparing 33 virtual and 33 tactile proportions red.\n\n\n\n\n\n\n12.2.3 Using the virtual shovel 1000 times\nNow say we want to study the effects of sampling variation not for 33 samples, but rather for a larger number of samples, say 1000. We have two choices at this point. We could have our groups of friends manually take 1000 samples of 50 balls and compute the corresponding 1000 proportions. However, this would be a tedious and time-consuming task. This is where computers excel: automating long and repetitive tasks while performing them quite quickly. Thus, at this point we will abandon tactile sampling in favor of only virtual sampling. Let’s once again use the rep_sample_n() function with sample size set to be 50 once again, but this time with the number of replicates reps set to 1000. Be sure to scroll through the contents of virtual_samples in RStudio’s viewer.\n\nvirtual_samples &lt;- bowl %&gt;% \n  rep_sample_n(size = 50, reps = 1000)\nvirtual_samples\n\n# A tibble: 50,000 × 3\n# Groups:   replicate [1,000]\n   replicate ball_ID color\n       &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;\n 1         1    2141 white\n 2         1     958 white\n 3         1    1700 white\n 4         1    1207 white\n 5         1    1289 red  \n 6         1    2290 white\n 7         1     897 white\n 8         1     487 red  \n 9         1    2035 white\n10         1    1672 white\n# ℹ 49,990 more rows\n\n\nObserve that now virtual_samples has 1000 \\(\\cdot\\) 50 = 50,000 rows, instead of the 33 \\(\\cdot\\) 50 = 1650 rows from earlier. Using the same data wrangling code as earlier, let’s take the data frame virtual_samples with 1000 \\(\\cdot\\) 50 = 50,000 rows and compute the resulting 1000 proportions of red balls.\n\nvirtual_prop_red &lt;- virtual_samples %&gt;% \n  group_by(replicate) %&gt;% \n  summarize(red = sum(color == \"red\")) %&gt;% \n  mutate(prop_red = red / 50)\nvirtual_prop_red\n\n# A tibble: 1,000 × 3\n   replicate   red prop_red\n       &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;\n 1         1    18     0.36\n 2         2    18     0.36\n 3         3    20     0.4 \n 4         4    13     0.26\n 5         5    12     0.24\n 6         6    16     0.32\n 7         7    22     0.44\n 8         8    16     0.32\n 9         9    18     0.36\n10        10    17     0.34\n# ℹ 990 more rows\n\n\nObserve that we now have 1000 replicates of prop_red, the proportion of 50 balls that are red. Using the same code as earlier, let’s now visualize the distribution of these 1000 replicates of prop_red in a histogram in Figure 12.7.\n\nggplot(virtual_prop_red, aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of 50 balls that were red\", \n       title = \"Distribution of 1000 proportions red\") \n\n\n\n\nFigure 12.7: Distribution of 1000 proportions based on 1000 samples of size 50.\n\n\n\n\nOnce again, the most frequently occurring proportions of red balls occur between 35% and 40%. Every now and then, we obtain proportions as less than 25% and others greater than 55%. These are rare, however. Furthermore, observe that we now have a much more symmetric and smoother bell-shaped distribution. This distribution is, in fact, well approximated by a normal distribution.\n\n\n12.2.4 Using different shovels\nNow say instead of just one shovel, you have three choices of shovels to extract a sample of balls with: shovels of size 25, 50, and 100.\n\n\n\nFigure 12.8: Three shovels to extract three different sample sizes.\n\n\nIf your goal is still to estimate the proportion of the bowl’s balls that are red, which shovel would you choose? In our experience, most people would choose the largest shovel with 100 slots because it would yield the “best” guess of the proportion of the bowl’s balls that are red. Let’s define some criteria for “best” in this subsection.\nUsing our newly developed tools for virtual sampling, let’s unpack the effect of having different sample sizes! In other words, let’s use rep_sample_n() with size set to 25, 50, and 100, respectively, while keeping the number of repeated/replicated samples at 1000:\n\nVirtually use the appropriate shovel to generate 1000 samples with size balls.\nCompute the resulting 1000 replicates of the proportion of the shovel’s balls that are red.\nVisualize the distribution of these 1000 proportions red using a histogram.\n\nRun each of the following code segments individually and then compare the three resulting histograms.\n\n# Segment 1: sample size = 25 ------------------------------\n# 1.a) Virtually use shovel 1000 times\nvirtual_samples_25 &lt;- bowl %&gt;% \n  rep_sample_n(size = 25, reps = 1000)\n\n# 1.b) Compute resulting 1000 replicates of proportion red\nvirtual_prop_red_25 &lt;- virtual_samples_25 %&gt;% \n  group_by(replicate) %&gt;% \n  summarize(red = sum(color == \"red\")) %&gt;% \n  mutate(prop_red = red / 25)\n\n# 1.c) Plot distribution via a histogram\nggplot(virtual_prop_red_25, aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of 25 balls that were red\", title = \"25\") \n\n\n# Segment 2: sample size = 50 ------------------------------\n# 2.a) Virtually use shovel 1000 times\nvirtual_samples_50 &lt;- bowl %&gt;% \n  rep_sample_n(size = 50, reps = 1000)\n\n# 2.b) Compute resulting 1000 replicates of proportion red\nvirtual_prop_red_50 &lt;- virtual_samples_50 %&gt;% \n  group_by(replicate) %&gt;% \n  summarize(red = sum(color == \"red\")) %&gt;% \n  mutate(prop_red = red / 50)\n\n# 2.c) Plot distribution via a histogram\nggplot(virtual_prop_red_50, aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of 50 balls that were red\", title = \"50\")  \n\n\n# Segment 3: sample size = 100 ------------------------------\n# 3.a) Virtually using shovel with 100 slots 1000 times\nvirtual_samples_100 &lt;- bowl %&gt;% \n  rep_sample_n(size = 100, reps = 1000)\n\n# 3.b) Compute resulting 1000 replicates of proportion red\nvirtual_prop_red_100 &lt;- virtual_samples_100 %&gt;% \n  group_by(replicate) %&gt;% \n  summarize(red = sum(color == \"red\")) %&gt;% \n  mutate(prop_red = red / 100)\n\n# 3.c) Plot distribution via a histogram\nggplot(virtual_prop_red_100, aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of 100 balls that were red\", title = \"100\") \n\nFor easy comparison, we present the three resulting histograms in a single row with matching x and y axes in Figure 12.9.\n\n\n\n\n\nFigure 12.9: Comparing the distributions of proportion red for different sample sizes.\n\n\n\n\nObserve that as the sample size increases, the variation of the 1000 replicates of the proportion of red decreases. In other words, as the sample size increases, there are fewer differences due to sampling variation and the distribution centers more tightly around the same value. Eyeballing Figure Figure 12.9, all three histograms appear to center around roughly 40%.\nWe can be numerically explicit about the amount of variability in our three sets of 1000 values of prop_red using the standard deviation (Section 8.2.5). For all three sample sizes, let’s compute the standard deviation of the 1000 proportions red by running the following data wrangling code that uses the sd() summary function.\n\n# n = 25\nvirtual_prop_red_25 %&gt;% \n  summarize(sd = sd(prop_red))\n\n# n = 50\nvirtual_prop_red_50 %&gt;% \n  summarize(sd = sd(prop_red))\n\n# n = 100\nvirtual_prop_red_100 %&gt;% \n  summarize(sd = sd(prop_red))\n\nLet’s compare these three measures of distributional variation in Table 12.1.\n\n\n\n\nTable 12.1: Comparing standard deviations of proportions red for three different shovels\n\n\nNumber of slots in shovel\nStandard deviation of proportions red\n\n\n\n\n25\n0.095\n\n\n50\n0.069\n\n\n100\n0.048\n\n\n\n\n\n\n\n\nAs we observed in Figure 12.9, as the sample size increases, the variation decreases. In other words, there is less variation in the 1000 values of the proportion red. So as the sample size increases, our guesses at the true proportion of the bowl’s balls that are red get more precise."
  },
  {
    "objectID": "sampling.html#sec-sampling-framework",
    "href": "sampling.html#sec-sampling-framework",
    "title": "12  Sampling",
    "section": "12.3 Sampling framework",
    "text": "12.3 Sampling framework\nIn both our tactile and our virtual sampling activities, we used sampling for the purpose of estimation. We extracted samples in order to estimate the proportion of the bowl’s balls that are red. We used sampling as a less time-consuming approach than performing an exhaustive count of all the balls. Our virtual sampling activity built up to the results shown in Figure 12.9 and Table 12.1: comparing 1000 proportions red based on samples of size 25, 50, and 100. This was our first attempt at understanding two key concepts relating to sampling for estimation:\n\nThe effect of sampling variation on our estimates.\nThe effect of sample size on sampling variation.\n\nNow that you have built some intuition relating to sampling, let’s now attach words and labels to the various concepts we’ve explored so far. Specifically in the next section, we’ll introduce terminology and notation as well as statistical definitions related to sampling. This will allow us to succinctly summarize and refer to the ideas behind sampling for the rest of this book.\n\n12.3.1 Terminology and notation\nLet’s now attach words and labels to the various sampling concepts we’ve seen so far by introducing some terminology and mathematical notation. While they may seem daunting at first, we’ll make sure to tie each of them to sampling bowl activities you performed earlier. Furthermore, throughout this book we’ll give you plenty of opportunity for practice, as the best method for mastering these terms is repetition.\nThe first set of terms and notation relate to populations:\n\nA population is a collection of individuals or observations we are interested in. This is also commonly denoted as a study population. We mathematically denote the population’s size using upper-case \\(N\\).\n\nA population parameter is some numerical summary about the population that is unknown but you wish you knew. For example, when this quantity is a mean like the average height of all Canadians, the population parameter of interest is the population mean.\nA census is an exhaustive enumeration or counting of all \\(N\\) individuals in the population. We do this in order to compute the population parameter’s value exactly. Of note is that as the number \\(N\\) of individuals in our population increases, conducting a census gets more expensive (in terms of time, energy, and money).\n\nSo in our sampling activities, the population is the collection of \\(N\\) = 2400 identically sized red and white balls in the bowl shown in Figure 12.1. Recall that we also represented the bowl “virtually” in the data frame bowl:\n\nhead(bowl)\n\n# A tibble: 6 × 2\n  ball_ID color\n    &lt;dbl&gt; &lt;chr&gt;\n1       1 white\n2       2 white\n3       3 white\n4       4 red  \n5       5 white\n6       6 white\n\n\nThe population parameter here is the proportion of the bowl’s balls that are red. Whenever we’re interested in a proportion of some value in a population, the population parameter has a specific name: the population proportion. We denote population proportions with the letter \\(p\\). We’ll see later on that we can also consider other types of population parameters, like population means and population regression slopes.\nIn order to compute this population proportion \\(p\\) exactly, we need to first conduct a census by going through all \\(N\\) = 2400 and counting the number that are red. We then divide this count by 2400 to obtain the proportion red.\nYou might be now asking yourself: “Wait. I understand that performing a census on the actual bowl would take a long time. But can’t we conduct a ‘virtual’ census using the virtual bowl?” You are absolutely correct! In fact when the authors of this book created the bowl data frame, they made its contents match the contents of actual bowl not by doing a census, but by reading the contents written on the box the bowl came in!\nLet’s conduct this “virtual” census by using the same dplyr verbs you used earlier to count the number of balls that are red:\n\nbowl %&gt;% \n  summarize(red = sum(color == \"red\")) \n\n# A tibble: 1 × 1\n    red\n  &lt;int&gt;\n1   900\n\n\nSince 900 of the 2400 are red, the proportion is 900/2400 = 0.375 = 37.5. So we know the value of the population parameter: in our case, the population proportion \\(p\\) is equal to 0.375.\nAt this point, you might be further asking yourself: “If we had a way of knowing that the proportion of the balls that are red is 37.5, then why did we do any sampling?” Great question! Normally, you wouldn’t do any sampling! However, the sampling activities we did this chapter are merely simulations of how sampling is done in real-life! We perform these simulations in order to study:\n\nThe effect of sampling variation on our estimates.\nThe effect of sample size on sampling variation.\n\nIn real-life sampling, not only will the population size \\(N\\) be very large making a census expensive, but sometimes we won’t even know how big the population is! For now however, we press on with our next set of terms and notation.\nThe second set of terms and notation relate to samples:\n\nSampling is the act of collecting a sample from the population, which we generally only do when we can’t perform a census. We mathematically denote the sample size using lower case \\(n\\), as opposed to upper case \\(N\\) which denotes the population’s size. Typically the sample size \\(n\\) is much smaller than the population size \\(N\\). Thus sampling is a much cheaper alternative than performing a census.\nA point estimate, also known as a sample statistic, is a summary statistic computed from a sample that estimates the unknown population parameter.\n\nSo previously we conducted sampling using a shovel with 50 slots to extract samples of size \\(n\\) = 50. To perform the virtual analog of this sampling, recall that we used the rep_sample_n() function as follows:\n\nvirtual_shovel &lt;- bowl %&gt;% \n  rep_sample_n(size = 50)\nvirtual_shovel\n\n# A tibble: 50 × 3\n# Groups:   replicate [1]\n   replicate ball_ID color\n       &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;\n 1         1    2261 red  \n 2         1     152 red  \n 3         1     472 white\n 4         1     224 white\n 5         1    1213 white\n 6         1    2094 red  \n 7         1    1550 white\n 8         1    1369 red  \n 9         1     329 white\n10         1     761 red  \n# ℹ 40 more rows\n\n\nUsing the sample of 50 balls contained in virtual_shovel, we generated an estimate of the proportion of the bowl’s balls that are red prop_red\n\nvirtual_shovel %&gt;% \n  summarize(num_red = sum(color == \"red\")) %&gt;% \n  mutate(prop_red = num_red / 50)\n\n# A tibble: 1 × 3\n  replicate num_red prop_red\n      &lt;int&gt;   &lt;int&gt;    &lt;dbl&gt;\n1         1      18     0.36\n\n\nSo in our case, the value of prop_red is the point estimate of the population proportion \\(p\\) since it estimates the latter’s value. Furthermore, this point estimate has a specific name when considering proportions: the sample proportion. It is denoted using \\(\\widehat{p}\\) because it is a common convention in statistics to use a “hat” symbol to denote point estimates.\nThe third set of terms relate to sampling methodology: the method used to collect samples. You’ll see here and throughout the rest of your book that the way you collect samples directly influences their quality.\n\nA sample is said to be representative if it roughly “looks like” the population. In other words, if the sample’s characteristics are a “good” representation of the population’s characteristics.\nWe say a sample is generalizable if any results based on the sample can generalize to the population. In other words, if we can make “good” guesses about the population using the sample.\nWe say a sampling procedure is biased if certain individuals in a population have a higher chance of being included in a sample than others. We say a sampling procedure is unbiased if every individual in a population has an equal chance of being sampled.\n\nWe say a sample of \\(n\\) balls extracted using our shovel is representative of the population if it’s contents “roughly resemble” the contents of the bowl. If so, then the proportion of the shovel’s balls that are red can generalize to the proportion of the bowl’s \\(N\\) = 2400 balls that are red. Or expressed differently, \\(\\widehat{p}\\) is a “good guess” of \\(p\\). Now say we cheated when using the shovel and removed a number of white balls in favor of red balls. Then this sample would be biased towards red balls, and thus the sample would no longer be representative of the bowl.\nThe fourth and final set of terms and notation relate to the goal of sampling:\n\nOne way to ensure that a sample is unbiased and representative of the population is by using random sampling\nInference is the act of “making a guess” about some unknown. Statistical inference is the act of making a guess about a population using a sample.\n\nIn our case, since the rep_sample_n() function uses your computer’s random number generator, we were in fact performing random sampling.\nLet’s now put all four sets of terms and notation together, keeping our sampling activities in mind:\n\nSince we extracted a sample of \\(n\\) = 50 balls at random, we mixed all of the equally sized balls before using the shovel, then\nthe contents of the shovel are unbiased and representative of the contents of the bowl, thus\nany result based on the shovel can generalize to the bowl, thus\nthe sample proportion \\(\\widehat{p}\\) of the \\(n\\) = 50 balls in the shovel that are red is a “good guess” of the population proportion \\(p\\) of the bowl’s \\(N\\) = 2400 balls that are red, thus\ninstead of conducting a census of the 2400 balls in the bowl, we can infer about the bowl using the sample from the shovel.\n\nWhat you have been performing is statistical inference. This is one of the most important concepts in all of statistics. So much so, we included this term in the title of our book: “Statistical Inference via Data Science”. More generally speaking,\n\nIf the sampling of a sample of size \\(n\\) is done at random, then\nthe sample is unbiased and representative of the population of size \\(N\\), thus\nany result based on the sample can generalize to the population, thus\nthe point estimate is a “good guess” of the unknown population parameter, thus\ninstead of performing a census, we can infer about the population using sampling.\n\n\n\n12.3.2 Statistical definitions\nTo further attach words and labels to the various sampling concepts we’ve seen so far, we also introduce some important statistical definitions related to sampling. As a refresher of our 1000 repeated/replicated virtual samples of size \\(n\\) = 25, \\(n\\) = 50, and \\(n\\) = 100 in Section Section 12.2, let’s display Figure 12.9 again as Figure 12.10.\n\n\n\n\n\nFigure 12.10: Previously seen three distributions of the sample proportion \\(\\widehat{p}\\).\n\n\n\n\nThese types of distributions have a special name: sampling distributions of point estimates. Their visualization displays the effect of sampling variation on the distribution of any point estimate, in this case, the sample proportion \\(\\widehat{p}\\). Using these sampling distributions, for a given sample size \\(n\\), we can make statements about what values we can typically expect. Unfortunately, the term sampling distribution is often confused with a sample’s distribution which is merely the distribution of the values in a single sample.\nFor example, observe the centers of all three sampling distributions: they are all roughly centered around 0.4 = 40%. Furthermore, observe that while we are somewhat likely to observe sample proportions of red balls of 0.2 = 20% when using the shovel with 25 slots, we will almost never observe a proportion of 20% when using the shovel with 100 slots. Observe also the effect of sample size on the sampling variation. As the sample size \\(n\\) increases from 25 to 50 to 100, the variation of the sampling distribution decreases and thus the values cluster more and more tightly around the same center of around 40%. We quantified this variation using the standard deviation of our sample proportions in Table 12.1, which we display again as Table 12.2:\n\n\nTable 12.2: ?(caption)\n\n\n\n\n(a) Previously seen comparing standard deviations of proportions red for three different shovels\n\n\nNumber of slots in shovel\nStandard deviation of proportions red\n\n\n\n\n25\n0.095\n\n\n50\n0.069\n\n\n100\n0.048\n\n\n\n\n\n\n\n\n\n\n\n\nSo as the sample size increases, the standard deviation of the proportion of red balls decreases. This type of standard deviation has another special name: standard error of a point estimate. Standard errors quantify the effect of sampling variation induced on our estimates. In other words, they quantify how much we can expect different proportions of a shovel’s balls that are red to vary from one sample to another sample to another sample, and so on. As a general rule, as sample size increases, the standard error decreases.\nSimilarly to confusion between sampling distributions with a sample’s distribution, people often confuse the standard error with the standard deviation. This is especially the case since a standard error is itself a kind of standard deviation. The best advice we can give is that a standard error is merely a kind of standard deviation: the standard deviation of any point estimate from sampling. In other words, all standard errors are standard deviations, but not every standard deviation is necessarily a standard error.\nTo help reinforce these concepts, let’s re-display Figure 12.9 but using our new terminology, notation, and definitions relating to sampling in Figure Figure 12.11.\n\n\n\n\n\nFigure 12.11: Three sampling distributions of the sample proportion \\(\\widehat{p}\\).\n\n\n\n\nFurthermore, let’s re-display Table 12.1 but using our new terminology, notation, and definitions relating to sampling in Table 12.3.\n\n\nTable 12.3: ?(caption)\n\n\n\n\n(a) Standard errors of the sample proportion based on sample sizes of 25, 50, and 100\n\n\nSample size (n)\nStandard error of $\\widehat{p}$\n\n\n\n\nn = 25\n0.095\n\n\nn = 50\n0.069\n\n\nn = 100\n0.048\n\n\n\n\n\n\n\n\n\n\n\n\nRemember the key message of this last table: that as the sample size \\(n\\) goes up, the “typical” error of your point estimate will go down, as quantified by the standard error. In fact, in Subsection @ref(theory-se) we’ll see that the standard error for the sample proportion \\(\\widehat{p}\\) can also be approximated via a mathematical theory-based formula, a formula that has \\(n\\) in the denominator.\n\n\n12.3.3 The moral of the story\nLet’s recap this section so far. We’ve seen that if a sample is generated at random, then the resulting point estimate is a “good guess” of the true unknown population parameter. In our sampling activities, since we made sure to mix the balls first before extracting a sample with the shovel, the resulting sample proportion \\(\\widehat{p}\\) of the shovel’s balls that were red was a “good guess” of the population proportion \\(p\\) of the bowl’s balls that were red.\nHowever, what do we mean by our point estimate being a “good guess”? Sometimes, we’ll get an estimate that is less than the true value of the population parameter, while at other times we’ll get an estimate that is greater. This is due to sampling variation. However, despite this sampling variation, our estimates will “on average” be correct and thus will be centered at the true value. This is because our sampling was done at random and thus in an unbiased fashion.\nIn our sampling activities, sometimes our sample proportion \\(\\widehat{p}\\) was less than the true population proportion \\(p\\), while at other times it was greater. This was due to the sampling variability. However, despite this sampling variation, our sample proportions \\(\\widehat{p}\\) were “on average” correct and thus were centered at the true value of the population proportion \\(p\\). This is because we mixed our bowl before taking samples and thus the sampling was done at random and thus in an unbiased fashion. This is also known as having an accurate estimate.\nRecall from earlier that the value of the population proportion \\(p\\) of the \\(N\\) = 2400 balls in the bowl was 900/2400 = 0.375 = 37.5. We computed this value by performing a virtual census of bowl. Let’s re-display our sampling distributions from Figure 12.9 and Figure 12.11, but now with a vertical red line marking the true population proportion \\(p\\) of balls that are red = 37.5 in Figure 12.12. We see that while there is a certain amount of error in the sample proportions \\(\\widehat{p}\\) for all three sampling distributions, on average the \\(\\widehat{p}\\) are centered at the true population proportion red \\(p\\).\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nFigure 12.12: Three sampling distributions with population proportion \\(p\\) marked by vertical line.\n\n\n\n\nWe also saw in this section that as your sample size \\(n\\) increases, your point estimates will vary less and less and be more and more concentrated around the true population parameter. This variation is quantified by the decreasing standard error. In other words, the typical error of your point estimates will decrease. In our sampling exercise, as the sample size increased, the variation of our sample proportions \\(\\widehat{p}\\) decreased. You can observe this behavior in Figure 12.12. This is also known as having a precise estimate.\nSo random sampling ensures our point estimates are accurate, while on the other hand having a large sample size ensures our point estimates are precise. While the terms “accuracy” and “precision” may sound like they mean the same thing, there is a subtle difference. Accuracy describes how “on target” our estimates are, whereas precision describes how “consistent” our estimates are. Figure 12.13 illustrates the difference.\n\n\n\nFigure 12.13: Comparing accuracy and precision.\n\n\nAt this point, you might be asking yourself: “Why did we take 1000 repeated samples of size n = 25, 50, and 100? Shouldn’t we be taking only one sample that’s as large as possible?”. If you did ask yourself these questions, your suspicion is correct! Recall from earlier when we asked ourselves “If we had a way of knowing that the proportion of the balls that are red is 37.5, then why did we do any sampling?” Similarly, we took 1000 repeated samples as a simulation of how sampling is done in real-life! We used these simulations to study:\n\nThe effect of sampling variation on our estimates.\nThe effect of sample size on sampling variation.\n\nThis is not how sampling is done in real life! In a real-life scenario, we wouldn’t take 1000 repeated/replicated samples, but rather a single sample that’s as large as we can afford."
  },
  {
    "objectID": "sampling.html#sec-sampling-conclusion-central-limit-theorem",
    "href": "sampling.html#sec-sampling-conclusion-central-limit-theorem",
    "title": "12  Sampling",
    "section": "12.4 Central Limit Theorem",
    "text": "12.4 Central Limit Theorem\nThis chapter began with (virtual) access to a large bowl of balls (our population) and a desire to figure out the proportion of red balls. Despite having access to this population, in reality, you almost never will have access to the population, either because the population is too large, ever changing, or too expensive to take a census of. Accepting this reality means accepting that we need to use statistical inference.\nIn Section 12.3.1, we stated that “statistical inference is the act of making a guess about a population using a sample.” But how do we do this inference? In the previous section, we defined the sampling framework only to state that in reality we take one large sample, instead of many samples as done in the sampling framework (which we modeled physically by taking many samples from the bowl).\nIn reality, we take only one sample and use that one sample to make statements about the population parameter. This ability of making statements about the population is allowable by a famous theorem, or mathematically proven truth, called the Central Limit Theorem. What you visualized in Figure 12.9 and Figure 12.11 and summarized in Table 12.1 and Table 12.3 was a demonstration of this theorem. It loosely states that when sample means are based on larger and larger sample sizes, the sampling distribution of these sample means becomes both more and more normally shaped and more and more narrow.\nIn other words, as our sample size gets larger (1) the sampling distribution of a point estimate (like a sample proportion) increasingly follows a normal distribution and (2) the variation of these sampling distributions gets smaller, as quantified by their standard errors.\nHere’s what is so surprising about the Central Limit Theorem: regardless of the shape of the underlying population distribution, the sampling distribution of means (such as the sample mean of bunny weights or the sample mean of the length of dragon wings) and proportions (such as the sample proportion red in our shovels) will be normal. Normal distributions are defined by where they are centered and how wide they are, and the Central Limit Theorem gives us both:\n\nThe sampling distribution of the point estimate is centered at the true population parameter\nWe have an estimate for how wide the sampling distribution of the point estimate is, given by the standard error\n\nWhat the Central Limit Theorem creates for us is a ladder between a single sample and the population. By the Central Limit Theorem, we can say that (1) our sample’s point estimate is drawn from a normal distribution centered at the true population parameter and (2)that the width of that normal distribution is governed by the standard error of our point estimate. Relating this to our bowl, if we pull one sample and get the sample proportion of red balls \\(\\widehat{p}\\), this value of \\(\\widehat{p}\\) is drawn from the normal curve centered at the true population proportion of red balls \\(p\\) with the computed standard error."
  },
  {
    "objectID": "sampling.html#sec-sampling-conclusion",
    "href": "sampling.html#sec-sampling-conclusion",
    "title": "12  Sampling",
    "section": "12.5 Conclusion",
    "text": "12.5 Conclusion\n\n12.5.1 Sampling scenarios\nIn this chapter, we performed both tactile and virtual sampling exercises to infer about an unknown proportion. We also presented a case study of sampling in real life with polls. In each case, we used the sample proportion \\(\\widehat{p}\\) to estimate the population proportion \\(p\\). However, we are not just limited to scenarios related to proportions. In other words, we can use sampling to estimate other population parameters using other point estimates as well. We present four more such scenarios in Table 12.4).\n\n\nRows: 6 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): Population parameter, Notation, Point estimate, Symbol(s)\ndbl (1): Scenario\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nTable 12.4: ?(caption)\n\n\n\n\n(a) \\label{tab:summarytable-ch8}Scenarios of sampling for inference\n\n\nScenario\nPopulation parameter\nNotation\nPoint estimate\nSymbol(s)\n\n\n\n\n1\nPopulation proportion\n$p$\nSample proportion\n$\\widehat{p}$\n\n\n2\nPopulation mean\n$\\mu$\nSample mean\n$\\overline{x}$ or $\\widehat{\\mu}$\n\n\n3\nDifference in population proportions\n$p_1 - p_2$\nDifference in sample proportions\n$\\widehat{p}_1 - \\widehat{p}_2$\n\n\n4\nDifference in population means\n$\\mu_1 - \\mu_2$\nDifference in sample means\n$\\overline{x}_1 - \\overline{x}_2$ or $\\widehat{\\mu}_1 - \\widehat{\\mu}_2$\n\n\n5\nPopulation regression slope\n$\\beta_1$\nFitted regression slope\n$b_1$ or $\\widehat{\\beta}_1$\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the rest of this book, we’ll cover all the remaining scenarios as follows:\n\nIn Chapter @ref(confidence-intervals), we’ll cover examples of statistical inference for\n\nScenario 2: The mean age \\(\\mu\\) of all pennies in circulation in the US.\nScenario 3: The difference \\(p_1 - p_2\\) in the proportion of people who yawn when seeing someone else yawn first minus the proportion of people who yawn without seeing someone else yawn first. This is an example of two-sample inference.\n\nIn Chapter @ref(hypothesis-testing), we’ll cover an example of statistical inference for\n\nScenario 4: The difference \\(\\mu_1 - \\mu_2\\) in mean IMDb ratings for action and romance movies. This is another example of two-sample inference.\n\nIn Chapter @ref(inference-for-regression), we’ll cover an example of statistical inference for regression by revisiting the regression models for teaching score as a function of various instructor demographic variables you saw in Chapters @ref(regression) and @ref(multiple-regression).\n\nScenario 5: The slope \\(\\beta_1\\) of the population regression line.\n\n\n\n\n12.5.2 Theory-based standard-errors\nThere exists in many cases a formula that approximates the standard error! In the case of our bowl where we used the sample proportion red \\(\\widehat{p}\\) to estimate the proportion of the bowl’s balls that are red, the formula that approximates the standard error for the sample proportion \\(\\widehat{p}\\) is:\n\\[\\text{SE}_{\\widehat{p}} \\approx \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}}\\]\nFor example, say you sampled \\(n = 50\\) balls and observed 21 red balls. This equals a sample proportion \\(\\widehat{p}\\) of 21/50 = 0.42. So, using the formula, an approximation of the standard error of \\(\\widehat{p}\\) is\n\\[\\text{SE}_{\\widehat{p}} \\approx \\sqrt{\\frac{0.42(1-0.42)}{50}} = \\sqrt{0.004872} = 0.0698 \\approx 0.070\\]\nSay instead you sampled \\(n = 100\\) balls and observed 42 red balls. This once again equals a sample proportion \\(\\widehat{p}\\) of 42/100 = 0.42. However using the formula, an approximation of the standard error of \\(\\widehat{p}\\) is now\n\\[\\text{SE}_{\\widehat{p}} \\approx \\sqrt{\\frac{0.42(1-0.42)}{100}} = \\sqrt{0.002436} = 0.0494\\] Observe that the standard error has gone down from 0.0698 to 0.0494. In other words, the “typical” error of our estimates using \\(n\\) = 100 will go down relative to \\(n\\) = 50 and hence be more precise. Recall that we illustrated the difference between accuracy and precision of estimates in Figure 12.13.\nThe key observation to make in the formula is that there is an \\(n\\) in the denominator. As the sample size \\(n\\) increases, the standard error decreases. We’ve demonstrated this fact using our virtual shovels in Section 12.3.3.\nFurthermore, this is one of the key messages of the Central Limit Theorem we saw in Section 12.4: as the sample size \\(n\\) increases, the distribution of averages gets narrower as quantified by the standard deviation of the sampling distribution of the sample mean. This standard deviation of the sampling distribution of the sample means in turn has a special name: the standard error of the sample mean.\nWhy is this formula true? Unfortunately, we don’t have the tools at this point to prove this; you’ll need to take a more advanced course in probability and statistics. (It is related to the concepts of Bernoulli and Binomial Distributions. You can read more about its derivation here if you like.)"
  },
  {
    "objectID": "missingness.html#what-happened-to-my-data",
    "href": "missingness.html#what-happened-to-my-data",
    "title": "13  Missing Data",
    "section": "13.1 What happened to my data?",
    "text": "13.1 What happened to my data?"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Appendix A — Bibliography",
    "section": "",
    "text": "Anscombe, Francis J. 1973. “Graphs in Statistical\nAnalysis.” The American Statistician 27 (1): 17–21.\n\n\nWilkinson, Leland. 2012. The Grammar of Graphics. Springer."
  }
]