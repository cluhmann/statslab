[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "statslab",
    "section": "",
    "text": "Introduction\nContent covering statistics, data analysis, and computer programming. Sections are listed on the left."
  },
  {
    "objectID": "rlang.html#r-the-language",
    "href": "rlang.html#r-the-language",
    "title": "1  R & RStudio",
    "section": "1.1 R: The Language",
    "text": "1.1 R: The Language\nReleased in 1993, R is an opensource successor to S, a commercially available statistical programming language developed at Bell Labs in the 1970s. The history is not particularly relevant. However, understanding that R was originally developed with the explicit goal of teaching introductory statistics (a bit like Matlab) may help to understand some of the design choices made by the developers of R.\nThere are several attributes that make R useful for teaching statistics:\n\nR is interpreted\nR is weakly typed\nR’s the focus on data analysis is built into its native data types\nR has strong graphical abilities\n\nDon’t worry if all of these benefits are meaningless to you at this point. We’ll get into the details soon enough. The short version is that the first two bullet points are features that make programmers’ lives easier/more convenient and last two mean that R is ready-made to do data analysis.\nThere are also a variety of downsides to R, but we will try to sidestep these as much as we can. One way we will do this is by exclusively using packages from the tidyverse. Once you are familiar with one way of working in R, you can explore the many, many alternatives."
  },
  {
    "objectID": "rlang.html#rstudio",
    "href": "rlang.html#rstudio",
    "title": "1  R & RStudio",
    "section": "1.2 RStudio",
    "text": "1.2 RStudio\nThe interface between a programmer (or data analyst) and a programming language is often an integrate development environment (IDE), which is just a fancy term for a piece of software that collects a bunch of inter-related tools often useful when programming. We will be using RStudio as our IDE. As the name suggests, RStudio was designed as an R-first IDE, making things easy for those new to R and/or programming.\nFirst time users often confuse R and RStudio. At its simplest, R is like a car’s engine while RStudio is like a car’s dashboard as illustrated in Figure Figure 1.1. More precisely, R is the programming language that performs computations and RStudio is how users interact with R.\n\n\n\nFigure 1.1: Analogy of difference between R and RStudio."
  },
  {
    "objectID": "rlang.html#sec-install",
    "href": "rlang.html#sec-install",
    "title": "1  R & RStudio",
    "section": "1.3 Installation",
    "text": "1.3 Installation\nInstallation involves two major steps. The first step is to install R itself. You can get the most recent version from CRAN (the comprehensive R archive network): https://cloud.r-project.org. There are ready-made installers available for Linux, macOS, and Windows.\nThe second step is to install RStudio. You can download it from Posit: https://posit.co/download/rstudio-desktop/. Again, there are versions available for all major platforms."
  },
  {
    "objectID": "rlang.html#using-r-via-rstudio",
    "href": "rlang.html#using-r-via-rstudio",
    "title": "1  R & RStudio",
    "section": "1.4 Using R via RStudio",
    "text": "1.4 Using R via RStudio\nRecall our car analogy from earlier. Much as we don’t drive a car by interacting directly with the engine but rather by interacting with elements on the car’s dashboard, we won’t be using R directly but rather we will use RStudio’s interface. After you install R and RStudio on your computer, you’ll have two new programs or applications you can open. We’ll always work in RStudio and not in the R application. Figure Figure 1.2 shows what icon you should be clicking on your computer.\n\n\n\nFigure 1.2: Icons of R versus RStudio on your computer.\n\n\nAfter you open RStudio, you should see something similar to Figure Figure 1.3. (Note that slight differences might exist if the RStudio interface is updated after 2019 to not be this by default.)\n\n\n\nFigure 1.3: RStudio interface to R.\n\n\nNote the three panes dividing the screen: the console pane, the files pane, and the environment pane. The console pane is where you enter R commands and R returns the corresponding results (and/or error messages if you make a mistake). We’ll dig into the other panes a bit more later."
  },
  {
    "objectID": "rlang.html#sec-packages",
    "href": "rlang.html#sec-packages",
    "title": "1  R & RStudio",
    "section": "1.5 Other Packages",
    "text": "1.5 Other Packages\nOne of the main strengths of R (arguably the most important strength) is the availability of many, many packages (or libraries) that extend the functionality of R in all sort of different ways. These libraries are written by a worldwide community of R users and can be downloaded for free. We will rely on a variety of these packages, so we take a moment now to discuss how packages work in R and RStudio.\nA good analogy for R packages is they are like apps you can download onto your mobile phone. So R is like a new mobile phone: while it has a certain amount of features when you use it for the first time, it doesn’t have everything. R packages are like the apps you can download onto your phone from Apple’s App Store or Android’s Google Play.\nLet’s continue this analogy by considering the Instagram app for editing and sharing pictures. Say you have purchased a new phone and you would like to share a photo you have just taken with friends on Instagram. You need to:\n\nInstall the app: Since your phone is new and does not include the Instagram app, you need to download the app from either the App Store or Google Play. You do this once and you’re set for the time being. You might need to do this again in the future when there is an update to the app.\nOpen the app: After you’ve installed Instagram, you need to open it.\n\nOnce Instagram is open on your phone, you can then proceed to share your photo with your friends and family. The process is very similar for using an R package. You need to:\n\nInstall the package: This is like installing an app on your phone. Most packages are not installed by default when you install R and RStudio. Thus, if you want to use a package, you need to install it first. Once you’ve installed a package, you likely won’t install it again unless you want to update it to a newer version.\n“Load” the package: “Loading” a package is like opening an app on your phone. Packages are not “loaded” by default when you start RStudio on your computer; you need to “load” each package you want to use every time you start RStudio.\n\nLet’s take a look at how these two steps come together to install and load the ggplot2 package for data visualization.\n\n1.5.1 Package Installation\nThere are two ways to install an R package: an easy way and a more advanced way. Let’s install the ggplot2 package the easy way first as shown in Figure Figure 1.4. In the Files pane of RStudio:\n\nClick on the “Packages” tab.\nClick on “Install” next to Update.\nType the name of the package under “Packages (separate multiple with space or comma):” In this case, type ggplot2.\nClick “Install.”\n\n\n\n\nFigure 1.4: Installing packages in R the easy way.\n\n\nAn alternative but slightly less convenient way to install a package is by typing install.packages(\"ggplot2\") in the console pane of RStudio and pressing Return/Enter on your keyboard. Note you must include the quotation marks around the name of the package.\nMuch like an app on your phone, you only have to install a package once. However, if you want to update a previously installed package to a newer version, you need to reinstall it by repeating the earlier steps.\n\n\n1.5.2 Package Loading\nRecall that after you’ve installed a package, you need to “load it.” In other words, you need to “open it.” We do this by using the library() command.\nFor example, to load the ggplot2 package, run the following code in the console pane. What do we mean by “run the following code”? Either type or copy-and-paste the following code into the console pane and then hit the Enter key.\n\nlibrary(ggplot2)\n\nIf after running the earlier code, a blinking cursor returns next to the &gt; “prompt” sign, it means you were successful and the ggplot2 package is now loaded and ready to use. If, however, you get a red “error message” that reads ...\nError in library(ggplot2) : there is no package called ‘ggplot2’\n... it means that you didn’t successfully install it. If you get this error message, go back to Subsection Section 1.5.1 on R package installation and make sure to install the ggplot2 package before proceeding.\n\n\n1.5.3 Package Use\nOne very common mistake new R users make when wanting to use particular packages is they forget to “load” them first by using the library() command we just saw. Remember: you have to load each package you want to use every time you start RStudio. If you don’t first “load” a package, but attempt to use one of its features, you’ll see an error message similar to:\nError: could not find function\nThis is a different error message than the one you just saw on a package not having been installed yet. R is telling you that you are trying to use a function in a package that has not yet been “loaded.” R doesn’t know where to find the function you are using. Almost all new users forget to do this when starting out, and it is a little annoying to get used to doing it. However, you’ll remember with practice and after some time it will become second nature for you."
  },
  {
    "objectID": "rlang.html#quitting-r",
    "href": "rlang.html#quitting-r",
    "title": "1  R & RStudio",
    "section": "1.6 Quitting R",
    "text": "1.6 Quitting R\n\n\n\nFigure 1.5: The dialog box that shows up when you try to close RStudio.\n\n\nThere’s one last thing I should cover in this chapter: how to quit R. You can just shut down the application in the normal way (e.g., File-&gt;Quit Session). However, R also has a function, q(), that you can use to quit, which can be handy. Regardless of what method you use to quit R, when you do so for the first time R will probably ask you if you want to save the “workspace image”. We’ll talk a lot more about loading and saving data in Chapter 3, but I figured we’d better quickly cover this now otherwise you’re going to get annoyed when you close R at the end of the chapter. If you’re using RStudio, you’ll see a dialog box that looks like the one shown in Figure @ref(fig:quitR). If you’re using a text based interface you’ll see this:\n\nq()\n\n## Save workspace image? [y/n/c]: \n\nThe y/n/c part here is short for “yes / no / cancel”. Type y if you want to save, n if you don’t, and c if you’ve changed your mind and you don’t want to quit after all.\nWhat does this actually mean? What’s going on is that R wants to know if you want to save all those variables that you’ve been creating, so that you can use them later. This sounds like a great idea, so it’s really tempting to type y or click the “Save” button. To be honest though, I very rarely do this, and it kind of annoys me a little bit.. what R is really asking is if you want it to store these variables in a “default” data file, which it will automatically reload for you next time you open R. And quite frankly, if I’d wanted to save the variables, then I’d have already saved them before trying to quit. Not only that, I’d have saved them to a location of my choice, so that I can find it again later. So I personally never bother with this.\nIn fact, every time I install R on a new machine one of the first things I do is change the settings so that it never asks me again. You can do this in RStudio really easily: use the menu system to find the RStudio option; the dialog box that comes up will give you an option to tell R never to whine about this again (see Figure 1.5). On a Mac, you can open this window by going to the “RStudio” menu and selecting “Preferences”. On a Windows machine you go to the “Tools” menu and select “Global Options”. Under the “General” tab you’ll see an option that reads “Save workspace to .Rdata on exit”. By default this is set to “ask”. If you want R to stop asking, change it to “never”."
  },
  {
    "objectID": "rlang.html#sec-rlangactivities",
    "href": "rlang.html#sec-rlangactivities",
    "title": "1  R & RStudio",
    "section": "1.7 Activities",
    "text": "1.7 Activities\n\nInstall both R and RStudio using the instructions found in Section 1.3\nOpen RStudio\nSettings\n\nGo to “Tools” menu and select “Global Options…”. On the “General” tab, uncheck the box that says “Restore .RData into workspace at startup”.\n\nSet the “Save workspace to .Rdata on exit” option to “never”.\nInstall the tidyverse package using the instructions found in Section 1.5.1\nVerify installation of the tidyverse package by loading the package using the instructions found in Section 1.5.2\nCreate a directory/folder somewhere convenient on your hard drive and name it “statslab”\nSet the “working directory” for R by going to “Session” menu and selecting “Set Working Directory” and then “Choose Directory…”. Select the “statslab” directory/folder you just created.\nAny time you wish to begin (or return to) work for this course, perform this step so that all of your file (data, scripts, etc.) are saved in a central location. Future activities will provide further instruction assuming that you a) have created this directory/folder and b) have set it to be your working directory in RStudio."
  },
  {
    "objectID": "programming.html#commands-at-the-console",
    "href": "programming.html#commands-at-the-console",
    "title": "2  R as a Calculator",
    "section": "2.1 Commands at the console",
    "text": "2.1 Commands at the console\nOne of the easiest things you can do with R is use it as a simple calculator, so it’s a good place to start. For instance, try typing 10 + 20, and hitting enter. The simple act of typing it rather than “just reading” makes a big difference. It makes the concepts more concrete, and it ties the abstract ideas (programming and statistics) to the actual context in which you need to use them. Statistics is something you do, not just something you read about in a textbook.] When you do this, you’ve entered a command, and R will “execute” that command. What you see on screen now will be this:\n&gt; 10 + 20\n[1] 30\nShould be much surprise here. But there’s a few things worth talking about, even with such a simple example. First, it’s important that you understand how to read the code example. In this example, what was typed into the RStudio console was the 10 + 20 part. The &gt; symbol as not typed. That’s just the command prompt and isn’t part of the actual command. The [1] 30 part was also not typed into the console. That’s what R printed out in response to the 10 + 20 code.\nSecond, it’s important to understand how the output is formatted. Obviously, the correct answer to the sum 10 + 20 is 30, and not surprisingly R has printed that out as part of its output. But it’s also printed out this [1] part, which probably doesn’t make a lot of sense to you right now. You’re going to see that a lot. I’ll talk about what this means in a bit more detail later on, but for now you can think of [1] 30 as if R were saying “the answer to the 1st question you asked is 30”. That’s not quite accurate, but it’s close enough for now. And in any case it’s not really very interesting at the moment: we only asked R to calculate one thing, so obviously there’s only one answer. Later on this will change, and the [1] part will start to make a bit more sense. For now, I just don’t want you to get confused or concerned by it.\n\n2.1.1 An important digression about formatting\nNow that I’ve taught you these rules I’m going to change them pretty much immediately. That is because I want you to be able to copy code from the book directly into R if if you want to test things or conduct your own analyses. However, if you copy this kind of code (that shows the command prompt and the results) directly into R you will get an error:\n\n&gt; 10 + 20\n\nError: &lt;text&gt;:1:1: unexpected '&gt;'\n1: &gt;\n    ^\n\n\nSo instead, I’m going to provide code in a slightly different format so that it looks like this…\n\n10 + 20\n\n[1] 30\n\n\nThere are two main differences.\n\nIn your console, the “&gt;” is the prompt and you type your code after (to the right of) this prompt.\nWe’ll often show the output of a bit of code, but the output will be displayed after the block of code itself.\n\nFor your purposes, this also means that you can easily copy code from any of these code blocks and paste it into your RStudio console in order to execute.\n\n\n2.1.2 Be very careful to avoid typos\nBefore we go on to talk about other types of calculations that we can do with R, there’s a few other things I want to point out. The first thing is that, though R is good software, it’s still software. R, like any programming language, is pretty stupid and because it’s stupid it can’t handle typos. It takes it on faith that you meant to type exactly what you actually typed. For example, suppose that you forgot to hit the shift key when trying to type +, and as a result your command ended up being 10 = 20 rather than 10 + 20. Here’s what happens:\n\n10 = 20\n\nError in 10 = 20: invalid (do_set) left-hand side to assignment\n\n\nWhat’s happened here is that R has attempted to interpret 10 = 20 as a command, and spits out an error message because the command doesn’t make any sense to it. When a human looks at this, and then looks down at his or her keyboard and sees that + and = are on the same key, it’s pretty obvious that the command was a typo. But R doesn’t know this, so it gets upset. And, if you look at it from its perspective, this makes sense. All that R “knows” is that 10 is a legitimate number, 20 is a legitimate number, and = is a legitimate part of the language too. In other words, from its perspective this really does look like the user meant to type 10 = 20, since all the individual parts of that statement are legitimate and it’s too stupid to realize that this is probably a typo. Therefore, R takes it on faith that this is exactly what you meant… it only “discovers” that the command is nonsense when it tries to follow your instructions, typo and all. And then it complains by spitting out an error.\nEven more subtle is the fact that some typos won’t produce errors at all, because they happen to correspond to “well-formed” R commands. For instance, suppose that not only did I forget to hit the shift key when trying to type 10 + 20, I also managed to press the key next to one I meant do. The resulting typo would produce the command 10 - 20. Clearly, R has no way of knowing that you meant to add 20 to 10, not subtract 20 from 10, so what happens this time is this:\n\n10 - 20\n\n[1] -10\n\n\nIn this case, R produces the right answer, but to the the wrong question.\nTo some extent, I’m stating the obvious here, but it’s important. The people who wrote R are smart. You, the user, are smart. But R is a programming language and programming languages are a way to tell computers what to do and computers are dumb. And because they are dumb, they are mindlessly obedient. R does exactly what you tell it to do. R will not try and second-guess what you “actually meant”; there is no “autocorrect”. This is for good reason. When doing advanced stuff – and even the simplest of statistics is pretty advanced in a lot of ways – it’s risky to let a mindless automaton like R try to overrule the human user. So it’s your responsibility to be careful. Always make sure you type exactly what you mean. When dealing with computers, it’s not enough to type “approximately” the right thing. In general, you absolutely must be precise in what you tell R to do … like all machines it is too stupid to be anything other than absurdly literal in its interpretation.\n\n\n2.1.3 R is (a bit) flexible with spacing\nOf course, now that I’ve been so uptight about the importance of always being precise, I should point out that there are some exceptions. Or, more accurately, there are some situations in which R does show a bit more flexibility than my previous description suggests. The first thing R is smart enough to do is ignore redundant spacing. What I mean by this is that, when I typed 10 + 20 before, I could equally have done this\n\n10    + 20\n\n[1] 30\n\n\nor this\n\n10+20\n\n[1] 30"
  },
  {
    "objectID": "programming.html#simple-calculations",
    "href": "programming.html#simple-calculations",
    "title": "2  R as a Calculator",
    "section": "2.2 Simple calculations",
    "text": "2.2 Simple calculations\nOkay, now that we’ve discussed some of the tedious details associated with typing R commands, let’s get back to learning how to use the most powerful piece of statistical software in the world as a $2 calculator. So far, all we know how to do is addition. Clearly, a calculator that only did addition would be a bit stupid, so we’ll discuss other simple calculations you can perform using R. But first, some more terminology. Addition is an example of an “operation” that you can perform (specifically, an arithmetic operation), and the operator that performs it is +. To people with a programming or mathematics background, this terminology probably feels pretty natural, but to other people it might feel like I’m trying to make something very simple (addition) sound more complicated than it is (by calling it an arithmetic operation). To some extent, that’s true: if addition was the only operation that we were interested in, it’d be a bit silly to introduce all this extra terminology. However, as we go along, we’ll start using more and more different kinds of operations, so it’s probably a good idea to get the language straight now, while we’re still talking about very familiar concepts like addition!\n\n2.2.1 Adding, subtracting, multiplying and dividing\nSo, now that we have the terminology, let’s learn how to perform some arithmetic operations in R. To that end, Table 2.1 lists the operators that correspond to the basic arithmetic we learned in primary school: addition, subtraction, multiplication and division.\n\n\nTable 2.1: ?(caption)\n\n\n\n\n(a) Basic arithmetic operations in R. These five operators are used very frequently throughout the text, so it’s important to be familiar with them at the outset.\n\n\noperation\noperator\nexample input\nexample output\n\n\n\n\naddition\n+\n10 + 2\n12\n\n\nsubtraction\n-\n9 - 3\n6\n\n\nmultiplication\n*\n5 * 5\n25\n\n\ndivision\n/\n10 / 3\n3\n\n\npower\n^\n5 ^ 2\n25\n\n\n\n\n\n\nAs you can see, R uses fairly standard symbols to denote each of the different operations you might want to perform: addition is done using the + operator, subtraction is performed by the - operator, and so on. So if I wanted to find out what 57 times 61 is (and who wouldn’t?), I can use R instead of a calculator, like so:\n\n57 * 61\n\n[1] 3477\n\n\nSo that’s handy.\n\n\n2.2.2 Taking powers\nThe first four operations listed in (tab-arithmetic1?) are things we all learned at a young age, but they aren’t the only arithmetic operations built into R. There are three other arithmetic operations that I should probably mention: taking powers, doing integer division, and calculating a modulus. Of the three, the most important is probably taking powers.\nFor those of you who can still remember your high school math, this should be familiar. And if not, it’s not complicated. As I’m sure everyone will probably remember the moment they read this, the act of multiplying a number \\(x\\) by itself \\(n\\) times is called “raising \\(x\\) to the \\(n\\)-th power”. Mathematically, this is written as \\(x^n\\). Some values of \\(n\\) have special names: in particular \\(x^2\\) is called \\(x\\)-squared, and \\(x^3\\) is called \\(x\\)-cubed. So, the 4th power of 5 is calculated like this: \\[\n5^4 = 5 \\times 5 \\times 5 \\times 5\n\\]\nOne way that we could calculate \\(5^4\\) in R would be to type in the complete multiplication as it is shown in the equation above. That is, we could do this\n\n5 * 5 * 5 * 5\n\n[1] 625\n\n\nbut it does seem a bit tedious. It would be very annoying indeed if you wanted to calculate \\(5^{15}\\), since the command would end up being quite long. Therefore, to make our lives easier, we use the power operator instead. When we do that, our command to calculate \\(5^4\\) goes like this:\n\n5 ^ 4\n\n[1] 625\n\n\nMuch easier.\n\n\n2.2.3 Doing calculations in the right order\nOkay. At this point, you know how to take one of the most powerful pieces of statistical software in the world, and use it as a $2 calculator. And as a bonus, you’ve learned a few very basic programming concepts. That’s not nothing (you could argue that you’ve just saved yourself $2) but on the other hand, it’s not very much either. In order to use R more effectively, we need to introduce more programming concepts.\nIn most situations where you would want to use a calculator, you might want to do multiple calculations. R lets you do this, just by typing in longer commands. In fact, we’ve already seen an example of this earlier, when I typed in 5 * 5 * 5 * 5. However, let’s try a slightly different example:\n\n1 + 2 * 4\n\n[1] 9\n\n\nClearly, this isn’t a problem for R either. However, it’s worth stopping for a second, and thinking about what R just did. Clearly, since it gave us an answer of 9 it must have multiplied 2 * 4 (to get an interim answer of 8) and then added 1 to that. But, suppose it had decided to just go from left to right: if R had decided instead to add 1+2 (to get an interim answer of 3) and then multiplied by 4, it would have come up with an answer of 12.\nTo answer this, you need to know the order of operations that R uses. If you remember back to your high school maths classes, it’s actually the same order that you got taught when you were at school: the “BEDMAS” order. That is, first calculate things inside Brackets (), then calculate Exponents ^, then Division / and Multiplication *, then Addition + and Subtraction -. So, to continue the example above, if we want to force R to calculate the 1+2 part before the multiplication, all we would have to do is enclose it in brackets:\n\n(1 + 2) * 4 \n\n[1] 12\n\n\nThis is a fairly useful thing to be able to do. The only other thing I should point out about order of operations is what to expect when you have two operations that have the same priority: that is, how does R resolve ties? For instance, multiplication and division are actually the same priority, but what should we expect when we give R a problem like 4 / 2 * 3 to solve? If it evaluates the multiplication first and then the division, it would calculate a value of two-thirds. But if it evaluates the division first it calculates a value of 6. The answer, in this case, is that R goes from left to right, so in this case the division step would come first:\n\n4 / 2 * 3\n\n[1] 6\n\n\nAll of the above being said, it’s helpful to remember that parentheses always come first. So, if you’re ever unsure about what order R will do things in, an easy solution is to enclose the thing you want it to do first in parentheses In addition, making the order of operations explicit makes your code more readable. By enclosing the division in parentheses (e.g., (4 / 2) * 3) we make it clear which thing happens first."
  },
  {
    "objectID": "programming.html#sec-assign",
    "href": "programming.html#sec-assign",
    "title": "2  R as a Calculator",
    "section": "2.3 Storing a number as a variable",
    "text": "2.3 Storing a number as a variable\nOne of the most important things to be able to do in R (or any programming language, for that matter) is to store information in variables. At a conceptual level you can think of a variable as label for a certain piece of information, or even several different pieces of information. For example, when using R as a calculator, there may be times when you want to store an intermediate result along the way. For example, when calculating an average (the sum divided by the count), you might wish to save the sum before dividing that sum by the count. Let’s look at the very basics for how we create variables and work with them.\n\n2.3.1 Variable assignment using &lt;- and -&gt;\nSince we’ve been working with numbers so far, let’s start by creating variables to store our numbers. And since most people like concrete examples, let’s invent one. Suppose I’m trying to calculate how much money I’m going to make selling this book. There’s several different numbers I might want to store. Firstly, I need to figure out how many copies I’ll sell. This isn’t exactly Harry Potter, so let’s assume I’m only going to sell one copy per student in my class. That’s 30 sales, so let’s create a variable called sales. What I want to do is assign a value to my variable sales, and that value should be 30. We do this by using the assignment operator, which is &lt;-. Here’s how we do it:\n\nsales &lt;- 30\n\nWhen you hit enter, R doesn’t print out any output. It just gives you another command prompt. However, behind the scenes R has created a variable called sales and given it a value of 30. You can check that this has happened by asking R to print the variable on screen. And the simplest way to do that is to type the name of the variable and hit enter.\n\nsales\n\n[1] 30\n\n\nSo that’s nice to know. Anytime you can’t remember what R has got stored in a particular variable, you can just type the name of the variable and hit enter.\nOkay, so now we know how to assign variables. Actually, there’s a bit more you should know. Firstly, one of the curious features of R is that there are several different ways of making assignments. In addition to the &lt;- operator, we can also use -&gt; and =, and it’s pretty important to understand the differences between them. Let’s start by considering -&gt;, since that’s the easy one (we’ll discuss the use of = in Section 2.4.1. As you might expect from just looking at the symbol, it’s almost identical to &lt;-. It’s just that the arrow (i.e., the assignment) goes from left to right. So if I wanted to define my sales variable using -&gt;, I would write it like this:\n\n30 -&gt; sales\n\nThis has the same effect: and it still means that I’m only going to sell 30 copies. Sigh. Apart from this superficial difference, &lt;- and -&gt; are identical. In fact, as far as R is concerned, they’re actually the same operator, just in a “left form” and a “right form”.\n\n\n2.3.2 Doing calculations using variables\nOkay, let’s get back to my original story. In my quest to become rich, I’ve written this textbook. To figure out how good a strategy is, I’ve started creating some variables in R. In addition to defining a sales variable that counts the number of copies I’m going to sell, I can also create a variable called royalty, indicating how much money I get per copy. Let’s say that my royalties are about $7 per book:\n\nsales &lt;- 30\nroyalty &lt;- 7\n\nThe nice thing about variables (in fact, the whole point of having variables) is that we can do anything with a variable that we ought to be able to do with the information that it stores. That is, since R allows me to multiply 30 by 7\n\n30 * 7\n\n[1] 210\n\n\nit also allows me to multiply sales by royalty\n\nsales * royalty\n\n[1] 210\n\n\nAs far as R is concerned, the sales * royalty command is the same as the 30 * 7 command. Not surprisingly, I can assign the output of this calculation to a new variable, which I’ll call revenue. And when we do this, the new variable revenue gets the value 35. So let’s do that, and then get R to print out the value of revenue so that we can verify that it’s done what we asked:\n\nrevenue &lt;- sales * royalty\nrevenue\n\n[1] 210\n\n\nThat’s fairly straightforward. A slightly more subtle thing we can do is reassign the value of my variable, based on its current value. For instance, suppose that one of my students (no doubt under the influence of psychotropic drugs) loves the book so much that he or she donates me an extra $550. The simplest way to capture this is by a command like this:\n\nrevenue &lt;- revenue + 550\nrevenue\n\n[1] 760\n\n\nIn this calculation, R has taken the old value of revenue (i.e., 210) and added 550 to that value, producing a value of 760 This new value is assigned to the revenue variable, overwriting its previous value. In any case, we now know that I’m expecting to make $760 off this. Pretty sweet, I thinks to myself. Or at least, that’s what I thinks until I do a few more calculation and work out what the implied hourly wage I’m making off this looks like.\n\n\n2.3.3 Rules and conventions for naming variables\nIn the examples that we’ve seen so far, my variable names (sales and revenue) have just been English-language words written using lowercase letters. However, R allows a lot more flexibility when it comes to naming your variables, as the following list of rules illustrates:\n\nVariable names can only use the upper case alphabetic characters A-Z as well as the lower case characters a-z. You can also include numeric characters 0-9 in the variable name, as well as the period . or underscore _ character. In other words, you can use SaL.e_s as a variable name (though I can’t think why you would want to), but you can’t use Sales?.\nVariable names cannot include spaces: therefore my sales is not a valid name, but my.sales is.\nVariable names are case sensitive: that is, Sales and sales are different variable names.\nVariable names must start with a letter or a period. You can’t use something like _sales or 1sales as a variable name. You can use .sales as a variable name if you want, but it’s not usually a good idea. By convention, variables starting with a . are used for special purposes, so you should avoid doing so.\nVariable names cannot be one of the reserved keywords. These are special names that R needs to keep “safe” from us mere users, so you can’t use them as the names of variables. The keywords are: if, else, repeat, while, function, for, in, next, break, TRUE, FALSE, NULL, Inf, NaN, NA, NA_integer_, NA_real_, NA_complex_, and finally, NA_character_. Don’t feel especially obliged to memorize these: if you make a mistake and try to use one of the keywords as a variable name, R will complain about it like the whiny little automaton it is.\n\nIn addition to those rules that R enforces, there are some informal conventions that people tend to follow when naming variables. One of them you’ve already seen: i.e., don’t use variables that start with a period. But there are several others. You aren’t obliged to follow these conventions, and there are many situations in which it’s advisable to ignore them, but it’s generally a good idea to follow them when you can:\n\nUse informative variable names. As a general rule, using meaningful names like sales and revenue is preferred over arbitrary ones like variable1 and variable2. Otherwise it’s very hard to remember what the contents of different variables are, and it becomes hard to understand what your commands actually do.\nUse short variable names. Typing is a pain and no-one likes doing it. So we much prefer to use a name like sales over a name like sales.for.this.book.that.you.are.reading. Obviously there’s a bit of a tension between using informative names (which tend to be long) and using short names (which tend to be meaningless), so use a bit of common sense when trading off these two conventions.\nUse one of the conventional naming styles for multi-word variable names. Suppose I want to name a variable that stores “my new salary”. Obviously I can’t include spaces in the variable name, so how should I do this? There are two main conventions that you sometimes see R users employing. First, there is “camel case” in which you use capital letters at the beginning of each constituent word (except the first one), which gives you myNewSalary as the variable name. Second, there is “snake case” in which you separate words using underscores, as in my_new_salary. Finally, you may also see some R users separating words using periods, which would give you my.new.salary. Do not do this because it is syntactically ambiguous (and thus makes your code difficult to read/understand)."
  },
  {
    "objectID": "programming.html#sec-functions",
    "href": "programming.html#sec-functions",
    "title": "2  R as a Calculator",
    "section": "2.4 Functions",
    "text": "2.4 Functions\nThe symbols +, -, *, etc. are examples of operators. As we’ve seen, you can do quite a lot of calculations just by using these operators. However, in order to do more advanced calculations (and later on, to do actual statistics), you’re going to need to start using functions. We’ll see more detail about functions and how they work later, but for now let’s just dive in and use a few. To get started, suppose I wanted to take the square root of 225. The square root, in case your high school math is a bit rusty, is just the opposite of squaring a number. So, for instance, since “5 squared is 25” I can say that “5 is the square root of 25”. This is the usual notation:\n\\[\n\\sqrt{25} = 5\n\\]\nSometimes you’ll also see it written like this:\n\\(25^{0.5} = 5\\)\nThis second way of writing it is kind of useful to “remind” you of the mathematical fact that “square root of \\(x\\)” is actually the same as “raising \\(x\\) to the power of 0.5”. Personally, I’ve never found this to be terribly meaningful psychologically, though I have to admit it’s quite convenient mathematically. Anyway, it’s not important. What is important is that you remember what a square root is, since we’re going to need it later on.\nYou may be able to calculate the square root of 25 in your head. But it gets more difficult when the numbers get bigger, and pretty much impossible if they’re not whole numbers. This is where something like R comes in very handy. Let’s say I wanted to calculate \\(\\sqrt{225}\\), the square root of 225. There’s two ways I could do this using R. First, since the square root of 255 is the same thing as raising 225 to the power of 0.5, we could use the power operator ^, just like we did earlier:\n\n225 ^ 0.5\n\n[1] 15\n\n\nHowever, there’s a second way that we can do this, since R also provides a square root function, sqrt(). To calculate the square root of 255 using this function, what I do is insert the number 225 in the parentheses. That is, the command I type is this:\n\nsqrt( 225 )\n\n[1] 15\n\n\nAs you might expect from our previous discussion, the spaces in between the parentheses are purely cosmetic. We could have typed sqrt(225) or sqrt( 225   ) and gotten the same result. When we use a function to do something, we generally refer to this as calling the function, and the values that we type into the function (there can be more than one) are referred to as the arguments of that function.\nObviously, the sqrt() function doesn’t really give us any new functionality, since we already knew how to do square root calculations by using the power operator ^, though it maybe be more explicit, clearer, and thus easier to read to use sqrt(). However, there are lots of other functions in R: in fact, almost everything of interest that I’ll talk about in this book is an R function of some kind. For example, one function that we will need to use in this book is the absolute value function. Compared to the square root function, it’s extremely simple: it just converts negative numbers to positive numbers, and leaves positive numbers alone. Mathematically, the absolute value of \\(x\\) is written \\(|x|\\) or sometimes \\(\\mbox{abs}(x)\\). Calculating absolute values in R is pretty easy, since R provides the abs() function that you can use for this purpose. When you feed it a positive number…\n\nabs( 21 )\n\n[1] 21\n\n\nthe absolute value function does nothing to it at all. But when you feed it a negative number, it spits out the positive version of the same number, like this:\n\nabs( -13 )\n\n[1] 13\n\n\nIn all honesty, there’s nothing that the absolute value function does that you couldn’t do just by looking at the number and erasing the minus sign if there is one. However, there’s a few places later in the book where we have to use absolute values, so I thought it might be a good idea to explain the meaning of the term early on.\nBefore moving on, it’s worth noting that – in the same way that R allows us to put multiple operations together into a longer command, like 1 + (2*4) for instance – it also lets us put functions together and even combine functions with operators if we so desire. For example, the following is a perfectly legitimate command:\n\nsqrt( 1 + abs(-8) )\n\n[1] 3\n\n\nWhen R executes this command, starts out by calculating the value of abs(-8), which produces an intermediate value of 8. Having done so, the command simplifies to sqrt( 1 + 8 ). To solve the square root it first needs to add 1 + 8 to get 9, at which point it evaluates sqrt(9), and so it finally outputs a value of 3.\n\n2.4.1 Function arguments, their names and their defaults\nThere’s two more fairly important things that you need to understand about how functions work in R, and that’s the use of “named” arguments, and “default values” for arguments. Not surprisingly, that’s not to say that this is the last we’ll hear about how functions work, but they are the last things we desperately need to discuss in order to get you started. To understand what these two concepts are all about, I’ll introduce another function. The round() function can be used to round some value to the nearest whole number. For example, I could type this:\n\nround( 3.1415 )\n\n[1] 3\n\n\nPretty straightforward, really. However, suppose I only wanted to round it to two decimal places: that is, I want to get 3.14 as the output. The round() function supports this, by allowing you to input a second argument to the function that specifies the number of decimal places that you want to round the number to. In other words, I could do this:\n\nround( 3.14165, 2 )\n\n[1] 3.14\n\n\nWhat’s happening here is that I’ve specified two arguments: the first argument is the number that needs to be rounded (i.e., 3.14165), the second argument is the number of decimal places that it should be rounded to (i.e., 2), and the two arguments are separated by a comma. In this simple example, it’s quite easy to remember which one argument comes first and which one comes second, but for more complicated functions this is not easy. Fortunately, most R functions make use of argument names. For the round() function, for example the number that needs to be rounded is specified using the x argument, and the number of decimal points that you want it rounded to is specified using the digits argument. Because we have these names available to us, we can specify the arguments to the function by name. We do so like this:\n\nround( x = 3.1415, digits = 2 )\n\n[1] 3.14\n\n\nNotice that this is kind of similar in spirit to variable assignment, except that = is used here, rather than &lt;-. In both cases we’re specifying specific values to be associated with a label. However, there are some differences between what we were doing earlier on when creating variables, and what we’re doing here when specifying arguments, and so as a consequence it’s important that you use = in this context.\nAs you can see, specifying the arguments by name involves a lot more typing, but it’s also explicit and thus a lot easier to read. Because of this, the commands in this book will usually specify arguments by name, since that makes it clearer to you what I’m doing. However, one important thing to note is that when specifying the arguments using their names, it doesn’t matter what order you type them in. But if you don’t use the argument names, then you have to input the arguments in the correct order. In other words, these three commands all produce the same output…\n\nround( 3.14165, 2 )\n\n[1] 3.14\n\nround( x = 3.1415, digits = 2 )\n\n[1] 3.14\n\nround( digits = 2, x = 3.1415 )\n\n[1] 3.14\n\n\nbut this one does not…\n\nround( 2, 3.14165 )\n\n[1] 2\n\n\nHow do you find out what the correct order is? There’s a few different ways, but the easiest one is to look at the help documentation for the function (e.g., ? round). However, if you’re ever unsure, it’s probably best to actually type in the argument name.\nOkay, so that’s the first thing I said you’d need to know: argument names. The second thing you need to know about is default values. Notice that the first time I called the round() function I didn’t actually specify the digits argument at all, and yet R somehow knew that this meant it should round to the nearest whole number. How did that happen? The answer is that the digits argument has a default value of 0, meaning that if you decide not to specify a value for digits then R will act as if you had typed digits = 0. This is quite handy: the vast majority of the time when you want to round a number you want to round it to the nearest whole number, and it would be pretty annoying to have to specify the digits argument every single time. On the other hand, sometimes you actually do want to round to something other than the nearest whole number, and it would be even more annoying if R didn’t allow this! Thus, by having digits = 0 as the default value, we get the best of both worlds."
  },
  {
    "objectID": "programming.html#sec-vectors",
    "href": "programming.html#sec-vectors",
    "title": "2  R as a Calculator",
    "section": "2.5 Storing many numbers as a vector",
    "text": "2.5 Storing many numbers as a vector\nAt this point we’ve covered functions in enough detail to get us safely through most of the rest of the book, so let’s return to our discussion of variables. When variables were introduced in Section 2.3 we saw how we can use variables to store a single number. In this section, we’ll extend this idea and look at how to store multiple numbers within the one variable. In R, a variable stores multiple values is called a vector. So let’s create one.\n\n2.5.1 Creating a vector\nLet’s stick to my silly “get rich quick by textbook writing” example. Suppose the textbook company (if there actually was one, that is) sends sales data on a monthly basis. Since my class start in late February, we might expect most of the sales to occur towards the start of the year. Let’s suppose that I have 100 sales in February, 200 sales in March and 50 sales in April, and no other sales for the rest of the year. What I would like to do is have a variable – let’s call it sales.by.month – that stores all this sales data. The first number stored should be 0 since I had no sales in January, the second should be 100, and so on. The simplest way to do this in R is to use the combine function, c(). To do so, all we have to do is type all the numbers you want to store in a comma separated list, like this:\n\nsales.by.month &lt;- c(0, 100, 200, 50, 0, 0, 0, 0, 0, 0, 0, 0)\nsales.by.month\n\n [1]   0 100 200  50   0   0   0   0   0   0   0   0\n\n\nTo use the correct terminology here, we have a single variable here called sales.by.month: this variable is a vector that consists of 12 elements.\n\n\n2.5.2 A handy digression\nNow that we’ve learned how to put information into a vector, the next thing to understand is how to pull that information back out again. However, before I do so it’s worth taking a slight detour. If you’ve been following along, typing all the commands into R yourself, it’s possible that the output that you saw when we printed out the sales.by.month vector was slightly different to what I showed above. This would have happened if the window (or the RStudio panel) that contains the R console is really, really narrow. If that were the case, you might have seen output that looks something like this:\n\nsales.by.month\n\n [1]   0 100 200  50\n [5]   0   0   0   0\n [9]   0   0   0   0\n\n\nBecause there wasn’t much room on the screen, R has printed out the results over three lines. But that’s not the important thing to notice. The important point is that the first line has a [1] in front of it, whereas the second line starts with [5] and the third with [9]. It’s pretty clear what’s happening here. For the first row, R has printed out the 1st element through to the 4th element, so it starts that row with a [1]. For the second row, R has printed out the 5th element of the vector through to the 8th one, and so it begins that row with a [5] so that you can tell where it’s up to at a glance. It might seem a bit odd to you that R does this, but in some ways it’s a kindness, especially when dealing with larger data sets!\n\n\n2.5.3 Getting information out of vectors\nTo get back to the main story, let’s consider the problem of how to get information out of a vector. At this point, you might have a sneaking suspicion that the answer has something to do with the [1] and [9] things that R has been printing out. And of course you are correct. Suppose I want to pull out the February sales data only. February is the second month of the year, so let’s try this:\n\nsales.by.month[2]\n\n[1] 100\n\n\nYep, that’s the February sales all right. But there’s a subtle detail to be aware of here: notice that R outputs [1] 100, not [2] 100. This is because R is being extremely literal. When we typed in sales.by.month[2], we asked R to find exactly one thing, and that one thing happens to be the second element of our sales.by.month vector. So, when it outputs [1] 100 what R is saying is that the first number that we just asked for is 100. This behaviour makes more sense when you realise that we can use this trick to create new variables. For example, I could create a february.sales variable like this:\n\nfebruary.sales &lt;- sales.by.month[2]\nfebruary.sales\n\n[1] 100\n\n\nObviously, the new variable february.sales should only have one element and so when I print it out this new variable, the R output begins with a [1] because 100 is the value of the first (and only) element of february.sales. The fact that this also happens to be the value of the second element of sales.by.month is irrelevant. We’ll pick this topic up again shortly ( Section 2.8).\n\n\n2.5.4 Altering the elements of a vector\nSometimes you’ll want to change the values stored in a vector. Imagine my surprise when the publisher rings me up to tell me that the sales data for May are wrong. There were actually an additional 25 books sold in May, but there was an error or something so they hadn’t told me about it. How can I fix my sales.by.month variable? One possibility would be to assign the whole vector again from the beginning, using c(). But that’s a lot of typing. Also, it’s a little wasteful: why should R have to redefine the sales figures for all 12 months, when only the 5th one is wrong? Fortunately, we can tell R to change only the 5th element, using this trick:\n\nsales.by.month[5] &lt;- 25\nsales.by.month\n\n [1]   0 100 200  50  25   0   0   0   0   0   0   0\n\n\nAnother way to edit variables is to use the edit() or fix() functions. I won’t discuss them in detail right now, but you can check them out on your own.\n\n\n2.5.5 Useful things to know about vectors\nBefore moving on, I want to mention a couple of other things about vectors. Firstly, you often find yourself wanting to know how many elements there are in a vector (usually because you’ve forgotten). You can use the length() function to do this. It’s quite straightforward:\n\nlength( x = sales.by.month )\n\n[1] 12\n\n\nSecondly, you often want to alter all of the elements of a vector at once. For instance, suppose I wanted to figure out how much money I made in each month. Since I’m earning an exciting $7 per book (no seriously, that’s actually pretty close to what authors get on the very expensive textbooks that you’re expected to purchase), what I want to do is multiply each element in the sales.by.month vector by 7. R makes this pretty easy, as the following example shows:\n\nsales.by.month * 7\n\n [1]    0  700 1400  350  175    0    0    0    0    0    0    0\n\n\nIn other words, when you multiply a vector by a single number, all elements in the vector get multiplied. The same is true for addition, subtraction, division and taking powers. So that’s neat. On the other hand, suppose I wanted to know how much money I was making per day, rather than per month. Since not every month has the same number of days, I need to do something slightly different. Firstly, I’ll create two new vectors:\n\ndays.per.month &lt;- c(31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31)\nprofit &lt;- sales.by.month * 7\n\nObviously, the profit variable is the same one we created earlier, and the days.per.month variable is pretty straightforward. What I want to do is divide every element of profit by the corresponding element of days.per.month. Again, R makes this pretty easy:\n\nprofit / days.per.month\n\n [1]  0.000000 25.000000 45.161290 11.666667  5.645161  0.000000  0.000000\n [8]  0.000000  0.000000  0.000000  0.000000  0.000000\n\n\nI still don’t like all those zeros, but that’s not what matters here. Notice that the second element of the output is 25, because R has divided the second element of profit (i.e. 700) by the second element of days.per.month (i.e. 28). Similarly, the third element of the output is equal to 1400 divided by 31, and so on. We’ll talk more about calculations involving vectors later on, but that’s enough detail for now."
  },
  {
    "objectID": "programming.html#sec-text",
    "href": "programming.html#sec-text",
    "title": "2  R as a Calculator",
    "section": "2.6 Storing text data",
    "text": "2.6 Storing text data\nA lot of the time your data will be numeric in nature, but not always. Sometimes your data really needs to be described using text, not using numbers. To address this, we need to consider the situation where our variables store text. To create a variable that stores the word “hello”, we can type this:\n\ngreeting &lt;- \"hello\"\ngreeting\n\n[1] \"hello\"\n\n\nWhen interpreting this, it’s important to recognise that the quote marks here aren’t part of the string itself. They’re just something that we use to make sure that R knows to treat the characters that they enclose as a piece of text data, known as a character string. In other words, R treats \"hello\" as a string containing the word “hello”; but if I had typed hello instead, R would go looking for a variable by that name! You can also use 'hello' to specify a character string.\nOkay, so that’s how we store the text. Next, it’s important to recognise that when we do this, R stores the entire word \"hello\" as a single element: our greeting variable is not a vector of five different letters. Rather, it has only the one element, and that element corresponds to the entire character string \"hello\". To illustrate this, if I actually ask R to find the first element of greeting, it prints the whole string:\n\ngreeting[1]\n\n[1] \"hello\"\n\n\nOf course, there’s no reason why I can’t create a vector of character strings. For instance, if we were to continue with the example of my attempts to look at the monthly sales data for my book, one variable I might want would include the names of all 12 months. To do so, I could type in a command like this\n\nmonths &lt;- c(\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n            \"July\", \"August\", \"September\", \"October\", \"November\", \n            \"December\")\n\nThis is a character vector containing 12 elements, each of which is the name of a month. So if I wanted R to tell me the name of the fourth month, all I would do is this:\n\nmonths[4]\n\n[1] \"April\"\n\n\n\n2.6.1 Working with text\nWorking with text data is somewhat more complicated than working with numeric data. There is much to discuss here, but for purposes of the current chapter we only need this bare bones sketch. The only other thing I want to do before moving on is show you an example of a function that can be applied to text data. So far, most of the functions that we have seen (i.e., sqrt(), abs() and round()) only make sense when applied to numeric data (e.g., you can’t calculate the square root of “hello”), and we’ve seen one function that can be applied to pretty much any variable or vector (i.e., length()). So it might be nice to see an example of a function that can be applied to text.\nThe function I’m going to introduce you to is called nchar(), and what it does is count the number of individual characters that make up a string. Recall earlier that when we tried to calculate the length() of our greeting variable it returned a value of 1: the greeting variable contains only the one string, which happens to be \"hello\". But what if I want to know how many letters there are in the word? Sure, I could count them, but that’s boring, and more to the point it’s a terrible strategy if what I wanted to know was the number of letters in War and Peace. That’s where the nchar() function is helpful:\n\nnchar( x = greeting )\n\n[1] 5\n\n\nThat makes sense, since there are in fact 5 letters in the string \"hello\". Better yet, you can apply nchar() to whole vectors. So, for instance, if I want R to tell me how many letters there are in the names of each of the 12 months, I can do this:\n\nnchar( x = months )\n\n [1] 7 8 5 5 3 4 4 6 9 7 8 8\n\n\nSo that’s nice to know. The nchar() function can do a bit more than this, and there’s a lot of other functions that you can do to extract more information from text or do all sorts of fancy things. However, the goal here is not to teach any of that! The goal right now is just to see an example of a function that actually does work when applied to text."
  },
  {
    "objectID": "programming.html#logicals",
    "href": "programming.html#logicals",
    "title": "2  R as a Calculator",
    "section": "2.7 Storing “true or false” data",
    "text": "2.7 Storing “true or false” data\nTime to move onto a third kind of data. A key concept in that a lot of R relies on is the idea of a logical value. A logical value is an assertion about whether something is true or false. This is implemented in R in a pretty straightforward way. There are two logical values, namely TRUE and FALSE. Despite the simplicity, a logical values are very useful things. Let’s see how they work.\n\n2.7.1 Assessing mathematical truths\nIn George Orwell’s classic book 1984, one of the slogans used by the totalitarian Party was “two plus two equals five”, the idea being that the political domination of human freedom becomes complete when it is possible to subvert even the most basic of truths. It’s a terrifying thought, especially when the protagonist Winston Smith finally breaks down under torture and agrees to the proposition. “Man is infinitely malleable”, the book says. I’m pretty sure that this isn’t true of humans but it’s definitely not true of R. R is not infinitely malleable. It has rather firm opinions on the topic of what is and isn’t true, at least as regards basic mathematics. If I ask it to calculate 2 + 2, it always gives the same answer, and it’s not bloody 5:\n\n2 + 2\n\n[1] 4\n\n\nOf course, so far R is just doing the calculations. I haven’t asked it to explicitly assert that \\(2+2 = 4\\) is a true statement. If I want R to make an explicit judgement, I can use a command like this:\n\n2 + 2 == 4\n\n[1] TRUE\n\n\nWhat I’ve done here is use the equality operator, ==, to force R to make a “true or false” judgement. Okay, let’s see what R thinks of the Party slogan:\n\n2+2 == 5\n\n[1] FALSE\n\n\nBooyah! Freedom and ponies for all! Or something like that. Anyway, it’s worth having a look at what happens if I try to force R to believe that two plus two is five by making an assignment statement like 2 + 2 = 5 or 2 + 2 &lt;- 5. When I do this, here’s what happens:\n\n2 + 2 = 5\n\nError in 2 + 2 = 5: target of assignment expands to non-language object\n\n\nR doesn’t like this very much. It recognises that 2 + 2 is not a variable (that’s what the “non-language object” part is saying), and it won’t let you try to “reassign” it. While R is pretty flexible, and actually does let you do some quite remarkable things to redefine parts of R itself, there are just some basic, primitive truths that it refuses to give up. It won’t change the laws of addition, and it won’t change the definition of the number 2.\nThat’s probably for the best.\n\n\n2.7.2 Logical operations\nSo now we’ve seen logical operations at work, but so far we’ve only seen the simplest possible example. You probably won’t be surprised to discover that we can combine logical operations with other operations and functions in a more complicated way, like this:\n\n3*3 + 4*4 == 5*5\n\n[1] TRUE\n\n\nor this\n\nsqrt( 25 ) == 5\n\n[1] TRUE\n\n\nNot only that, but as Table Table 2.2 illustrates, there are several other logical operators that you can use, corresponding to some basic mathematical concepts.\n\n\nTable 2.2: ?(caption)\n\n\n\n\n(a) Some logical operators. Technically I should be calling these “binary relational operators”, but quite frankly I don’t want to. It’s my book so no-one can make me.\n\n\noperation\noperator\nexample input\nanswer\n\n\n\n\nless than\n&lt;\n2 &lt; 3\nTRUE\n\n\nless than or equal to\n&lt;=\n2 &lt;= 2\nTRUE\n\n\ngreater than\n&gt;\n2 &gt; 3\nFALSE\n\n\ngreater than or equal to\n&gt;=\n2 &gt;= 2\nTRUE\n\n\nequal to\n==\n2 == 3\nFALSE\n\n\nnot equal to\n!=\n2 != 3\nTRUE\n\n\n\n\n\n\nHopefully these are all pretty self-explanatory: for example, the less than operator &lt; checks to see if the number on the left is less than the number on the right. If it’s less, then R returns an answer of TRUE:\n\n99 &lt; 100\n\n[1] TRUE\n\n\nbut if the two numbers are equal, or if the one on the right is larger, then R returns an answer of FALSE, as the following two examples illustrate:\n\n100 &lt; 100\n\n[1] FALSE\n\n100 &lt; 99\n\n[1] FALSE\n\n\nIn contrast, the less than or equal to operator &lt;= will do exactly what it says. It returns a value of TRUE if the number of the left hand side is less than or equal to the number on the right hand side. So if we repeat the previous two examples using &lt;=, here’s what we get:\n\n100 &lt;= 100\n\n[1] TRUE\n\n100 &lt;= 99\n\n[1] FALSE\n\n\nAnd at this point I hope it’s pretty obvious what the greater than operator &gt; and the greater than or equal to operator &gt;= do! Next on the list of logical operators is the not equal to operator != which – as with all the others – does what it says it does. It returns a value of TRUE when things on either side are not identical to each other. Therefore, since \\(2+2\\) isn’t equal to \\(5\\), we get:\n\n2 + 2 != 5\n\n[1] TRUE\n\n\nWe’re not quite done yet. There are three more logical operations that are worth knowing about, listed in Table Table 2.3.\n\n\nTable 2.3: ?(caption)\n\n\n\n\n(a) Some more logical operators.\n\n\noperation\noperator\nexample input\nanswer\n\n\n\n\nnot\n!\n!(1==1)\nFALSE\n\n\nor\n|\n(1==1) | (2==3)\nTRUE\n\n\nand\n&\n(1==1) & (2==3)\nFALSE\n\n\n\n\n\n\nThese are the not operator !, the and operator &, and the or operator |. Like the other logical operators, their behaviour is more or less exactly what you’d expect given their names. For instance, if I ask you to assess the claim that “either \\(2+2 = 4\\) or \\(2+2 = 5\\)” you’d say that it’s true. Since it’s an “either-or” statement, all we need is for one of the two parts to be true. That’s what the | operator does:\n\n(2+2 == 4) | (2+2 == 5)\n\n[1] TRUE\n\n\nOn the other hand, if I ask you to assess the claim that “both \\(2+2 = 4\\) and \\(2+2 = 5\\)” you’d say that it’s false. Since this is an and statement we need both parts to be true. And that’s what the & operator does:\n\n(2+2 == 4) & (2+2 == 5)\n\n[1] FALSE\n\n\nFinally, there’s the not operator, which is simple but annoying to describe in English. If I ask you to assess my claim that “it is not true that \\(2+2 = 5\\)” then you would say that my claim is true; because my claim is that “\\(2+2 = 5\\) is false”. And I’m right. If we write this as an R command we get this:\n\n! (2+2 == 5)\n\n[1] TRUE\n\n\nIn other words, since 2+2 == 5 is a FALSE statement, it must be the case that !(2+2 == 5) is a TRUE one. Essentially, what we’ve really done is claim that “not false” is the same thing as “true”. Obviously, this isn’t really quite right in real life. But R lives in a much more black or white world: for R everything is either true or false. No shades of gray are allowed. We can actually see this much more explicitly, like this:\n\n! FALSE\n\n[1] TRUE\n\n\nOf course, in our \\(2+2 = 5\\) example, we didn’t really need to use “not” ! and “equals to” == as two separate operators. We could have just used the “not equals to” operator != like this:\n\n2+2 != 5\n\n[1] TRUE\n\n\nBut there are many situations where you really do need to use the ! operator. We’ll see some later on.\n\n\n2.7.3 Storing and using logical data\nUp to this point, I’ve introduced numeric data (Section 2.3 and Section 2.5) and character data (Section 2.6). So you might not be surprised to discover that these TRUE and FALSE values that R has been producing are actually a third kind of data, called logical data. That is, when I asked R if 2 + 2 == 5 and it said [1] FALSE in reply, it was actually producing information that we can store in variables. For instance, I could create a variable called is.the.Party.correct, which would store R’s opinion:\n\nis.the.Party.correct &lt;- 2 + 2 == 5\nis.the.Party.correct\n\n[1] FALSE\n\n\nAlternatively, you can assign the value directly, by typing TRUE or FALSE in your command. Like this:\n\nis.the.Party.correct &lt;- FALSE\nis.the.Party.correct\n\n[1] FALSE\n\n\nBetter yet, because it’s kind of tedious to type TRUE or FALSE over and over again, R provides you with a shortcut: you can use T and F instead (but it’s case sensitive: t and f won’t work).\n::: {.callout-caution} ## TRUE and FALSE\nTRUE and FALSE are reserved keywords in R, so you can trust that they always mean what they say they do. Unfortunately, the shortcut versions T and F do not have this property. It’s even possible to create variables that set up the reverse meanings, by typing commands like T &lt;- FALSE and F &lt;- TRUE. This is kind of insane, and something that is generally thought to be a design flaw in R. Anyway, the long and short of it is that it’s safer to use TRUE and FALSE.:::\nSo this works:\n\nis.the.Party.correct &lt;- F\nis.the.Party.correct\n\n[1] FALSE\n\n\nbut this doesn’t:\n\nis.the.Party.correct &lt;- f\n\nError in eval(expr, envir, enclos): object 'f' not found\n\n\n\n\n2.7.4 Vectors of logicals\nThe next thing to mention is that you can store vectors of logical values in exactly the same way that you can store vectors of numbers (Section 2.5) and vectors of text data (Section 2.6). Again, we can define them directly via the c() function, like this:\n\nx &lt;- c(TRUE, TRUE, FALSE)\nx\n\n[1]  TRUE  TRUE FALSE\n\n\nor you can produce a vector of logicals by applying a logical operator to a vector. This might not make a lot of sense to you, so let’s unpack it slowly. First, let’s suppose we have a vector of numbers (i.e., a “non-logical vector”). For instance, we could use the sales.by.month vector that we were using in Section 2.5. Suppose I wanted R to tell me, for each month of the year, whether I actually sold a book in that month. I can do that by typing this:\n\nsales.by.month &gt; 0\n\n [1] FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nand again, I can store this in a vector if I want, as the example below illustrates:\n\nany.sales.this.month &lt;- sales.by.month &gt; 0\nany.sales.this.month\n\n [1] FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nIn other words, any.sales.this.month is a logical vector whose elements are TRUE only if the corresponding element of sales.by.month is greater than zero. For instance, since I sold zero books in January, the first element is FALSE.\n\n\n2.7.5 Applying logical operation to text\nIn a moment (Section 2.8) I’ll show you why these logical operations and logical vectors are so handy, but before I do so I want to very briefly point out that you can apply them to text as well as to logical data. It’s just that we need to be a bit more careful in understanding how R interprets the different operations. In this section I’ll talk about how the equal to operator == applies to text, since this is the most important one. Obviously, the not equal to operator != gives the exact opposite answers to == so I’m implicitly talking about that one too, but I won’t give specific commands showing the use of !=. There are a variety of other operators, but those will do for now.\nOkay, let’s see how it works. In one sense, it’s very simple. For instance, I can ask R if the word \"cat\" is the same as the word \"dog\", like this:\n\n\"cat\" == \"dog\"\n\n[1] FALSE\n\n\nThat’s pretty obvious, and it’s good to know that even R can figure that out. Similarly, R does recognise that a \"cat\" is a \"cat\":\n\n\"cat\" == \"cat\"\n\n[1] TRUE\n\n\nAgain, that’s exactly what we’d expect. However, what you need to keep in mind is that R is not at all tolerant when it comes to grammar and spacing. If two strings differ in any way whatsoever, R will say that they’re not equal to each other, as the following examples indicate:\n\n\" cat\" == \"cat\"\n\n[1] FALSE\n\n\"cat\" == \"CAT\"\n\n[1] FALSE\n\n\"cat\" == \"c a t\"\n\n[1] FALSE"
  },
  {
    "objectID": "programming.html#sec-indexing",
    "href": "programming.html#sec-indexing",
    "title": "2  R as a Calculator",
    "section": "2.8 Indexing vectors",
    "text": "2.8 Indexing vectors\nOne last thing to add before finishing up this chapter. So far, whenever I’ve had to get information out of a vector, all I’ve done is typed something like months[4]; and when I do this R prints out the fourth element of the months vector. In this section, I’ll show you two additional tricks for getting information out of the vector.\n\n2.8.1 Extracting multiple elements\nOne very useful thing we can do is pull out more than one element at a time. In the previous example, we only used a single number (i.e., 2) to indicate which element we wanted. Alternatively, we can use a vector. So, suppose I wanted the data for February, March and April. What I could do is use the vector c(2,3,4) to indicate which elements I want R to pull out. That is, I’d type this:\n\nsales.by.month[ c(2,3,4) ]\n\n[1] 100 200  50\n\n\nNotice that the order matters here. If I asked for the data in the reverse order (i.e., April first, then March, then February) by using the vector c(4,3,2), then R outputs the data in the reverse order:\n\nsales.by.month[ c(4,3,2) ]\n\n[1]  50 200 100\n\n\nA second thing to be aware of is that R provides you with handy shortcuts for very common situations. For instance, suppose that I wanted to extract everything from the 2nd month through to the 8th month. One way to do this is to do the same thing I did above, and use the vector c(2,3,4,5,6,7,8) to indicate the elements that I want. That works just fine\n\nsales.by.month[ c(2,3,4,5,6,7,8) ]\n\n[1] 100 200  50  25   0   0   0\n\n\nbut it’s kind of a lot of typing. To help make this easier, R lets you use 2:8 as shorthand for c(2,3,4,5,6,7,8), which makes things a lot simpler. First, let’s just check that this is true:\n\n2:8\n\n[1] 2 3 4 5 6 7 8\n\n\nNext, let’s check that we can use the 2:8 shorthand as a way to pull out the 2nd through 8th elements of sales.by.months:\n\nsales.by.month[2:8]\n\n[1] 100 200  50  25   0   0   0\n\n\nSo that’s kind of neat.\n\n\n2.8.2 Logical indexing\nAt this point, I can introduce an extremely useful tool called logical indexing. In the last section, I created a logical vector any.sales.this.month, whose elements are TRUE for any month in which I sold at least one book, and FALSE for all the others. However, that big long list of TRUEs and FALSEs is a little bit hard to read, so what I’d like to do is to have R select the names of the months for which I sold any books. Earlier on, I created a vector months that contains the names of each of the months. This is where logical indexing is handy. What I need to do is this:\n\nmonths[ sales.by.month &gt; 0 ]\n\n[1] \"February\" \"March\"    \"April\"    \"May\"     \n\n\nTo understand what’s happening here, it’s helpful to notice that sales.by.month &gt; 0 is the same logical expression that we used to create the any.sales.this.month vector in the last section. In fact, I could have just done this:\n\nmonths[ any.sales.this.month ]\n\n[1] \"February\" \"March\"    \"April\"    \"May\"     \n\n\nand gotten exactly the same result. In order to figure out which elements of months to include in the output, what R does is look to see if the corresponding element in any.sales.this.month is TRUE. Thus, since element 1 of any.sales.this.month is FALSE, R does not include \"January\" as part of the output; but since element 2 of any.sales.this.month is TRUE, R does include \"February\" in the output. Note that there’s no reason why I can’t use the same trick to find the actual sales numbers for those months. The command to do that would just be this:\n\nsales.by.month [ sales.by.month &gt; 0 ]\n\n[1] 100 200  50  25\n\n\nIn fact, we can do the same thing with text. Here’s an example. Suppose that – to continue the saga of the textbook sales – I later find out that the bookshop only had sufficient stocks for a few months of the year. They tell me that early in the year they had \"high\" stocks, which then dropped to \"low\" levels, and in fact for one month they were \"out\" of copies of the book for a while before they were able to replenish them. Thus I might have a variable called stock.levels which looks like this:\n\nstock.levels&lt;-c(\"high\", \"high\", \"low\", \"out\", \"out\", \"high\",\n                \"high\", \"high\", \"high\", \"high\", \"high\", \"high\")\n\nstock.levels\n\n [1] \"high\" \"high\" \"low\"  \"out\"  \"out\"  \"high\" \"high\" \"high\" \"high\" \"high\"\n[11] \"high\" \"high\"\n\n\nThus, if I want to know the months for which the bookshop was out of my book, I could apply the logical indexing trick, but with the character vector stock.levels, like this:\n\nmonths[stock.levels == \"out\"]\n\n[1] \"April\" \"May\"  \n\n\nAlternatively, if I want to know when the bookshop was either low on copies or out of copies, I could do this:\n\nmonths[stock.levels == \"out\" | stock.levels == \"low\"]\n\n[1] \"March\" \"April\" \"May\"  \n\n\nor this\n\nmonths[stock.levels != \"high\" ]\n\n[1] \"March\" \"April\" \"May\"  \n\n\nEither way, I get the answer I want.\nAt this point, I hope you can see why logical indexing is such a useful thing. It’s a very basic, yet very powerful way to manipulate data. Subsequent chapters will talk a lot more about how to manipulate data, since it’s a critical skill for real world research that is often overlooked in introductory statistics courses It does take a bit of practice to become completely comfortable using logical indexing, so it’s a good idea to play around with these sorts of commands. Try creating a few different variables of your own, and then ask yourself questions like “how do I get R to spit out all the elements that are [blah]”. Practice makes perfect, and it’s only by practicing logical indexing that you’ll perfect the art of yelling frustrated insults at your computer."
  },
  {
    "objectID": "programming.html#activities",
    "href": "programming.html#activities",
    "title": "2  R as a Calculator",
    "section": "2.9 Activities",
    "text": "2.9 Activities\n\nOpen RStudio and"
  },
  {
    "objectID": "data.html#sec-load",
    "href": "data.html#sec-load",
    "title": "3  Working with Data",
    "section": "3.1 Loading and saving data",
    "text": "3.1 Loading and saving data\nThere are several different types of files that are likely to be relevant to us when doing data analysis. There are three in particular that are especially important from the perspective of this book:\n\nComma separated value (CSV) files are those with a .csv file extension. These are just regular old text files, and they can be opened with almost any software. This means that storing data in CSV files does not tie users to any particular software and keeps things simple.\nWorkspace files are those with a .Rdata file extension. This is the standard kind of file that R uses to store data and variables. They’re called “workspace files” because you can use them to save your whole workspace.\n\n\n3.1.1 Importing data from CSV files using read_csv\nOne quite commonly used data format is the humble “comma separated value” file, also called a CSV file, and usually bearing the file extension .csv. CSV files are just plain old-fashioned text files, and what they store is basically just a table of data. This is illustrated in Figure Figure 3.1, which shows a file called booksales.csv that I’ve created. As you can see, each row corresponds to a variable, and each row represents the book sales data for one month. The first row doesn’t contain actual data though: it has the names of the variables.\n\n\n\nFigure 3.1: The booksales.csv data file. On the left, I’ve opened the file in using a spreadsheet program (OpenOffice), which shows that the file is basically a table. On the right, the same file is open in a standard text editor (the TextEdit program on a Mac), which shows how the file is formatted. The entries in the table are wrapped in quote marks and separated by commas.\n\n\nIf RStudio were not available to you, the easiest way to open this file would be to use the read.csv() function. This function is pretty flexible, and I’ll talk a lot more about it’s capabilities in Section 3.1.2 for more details, but for now there’s only two arguments to the function that I’ll mention:\n\nfile. This should be a character string that specifies a path to the file that needs to be loaded. You can use an absolute path or a relative path to do so.\nheader. This is a logical value indicating whether or not the first row of the file contains variable names. The default value is TRUE.\n\nTherefore, to import the CSV file, the command I need is:\n\nbooks &lt;- read_csv(\"./data/booksales.csv\")\n\nThere are two very important points to notice here. Firstly, notice that I didn’t try to use the load() function, because that function is only meant to be used for .Rdata files. If you try to use load() on other types of data, you get an error. Secondly, notice that when I imported the CSV file I assigned the result to a variable, which I imaginatively called books file. There’s a reason for this. The idea behind an .Rdata file is that it stores a whole workspace. So, if you had the ability to look inside the file yourself you’d see that the data file keeps track of all the variables and their names. So when you load() the file, R restores all those original names. CSV files are treated differently: as far as R is concerned, the CSV only stores one variable, but that variable is big table. So when you import that table into the workspace, R expects you to give it a name.] Let’s have a look at what we’ve got:\n\nprint(books)\n\n# A tibble: 12 × 4\n   Month      Days Sales Stock.Levels\n   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n 1 January      31     0 high        \n 2 February     28   100 high        \n 3 March        31   200 low         \n 4 April        30    50 out         \n 5 May          31     0 out         \n 6 June         30     0 high        \n 7 July         31     0 high        \n 8 August       31     0 high        \n 9 September    30     0 high        \n10 October      31     0 high        \n11 November     30     0 high        \n12 December     31     0 high        \n\n\nYou can instead open the data in RStudio’s spreadsheet viewer:\n\nView(books)\n\nThe books data set is quite small, so just calling print() is fine. But for larger data sets, the View() and the spreadsheet viewer allows for a more thorough inspection.\nClearly, it’s worked, but the format of this output is a bit unfamiliar. We haven’t seen anything like this before. What you’re looking at is a data frame, which is a very important kind of variable in R, and one I’ll discuss at length in Chapter 5. For now, let’s just be happy that we imported the data and that it looks about right.\n\n\n\n\n\n\nNote\n\n\n\nIn a lot of books you’ll see the read.table() or read.csv() functions used for this purpose instead of read_csv(). They’re similar functions, but read_csv() is a function from the readr package, one of the many packages that make up “the tidyverse”. The tidyverse is a set of interoperable packages we will be using throughout this course. Further information about the tidyverse can be found in Chapter 4.\n\n\n\n\n3.1.2 Importing data from CSV files using RStudio\n\n\n\n\n\n\nWarning\n\n\n\nAs detailed above, read_csv() is how we will import CSV data files into R. This section details an alternative way to import CSV data files, but it will not produce the same kind of variable. Using RStudio’s built-in “Import Dataset” functionality will produce a dataframe, whereas read_csv produces a “tibble”. We will Further information about the difference between plain dataframes and tibbles can be found in Chapter 5.\n\n\nIn the environment panel in RStudio you should see a button called “Import Dataset”. Click on that, and it will give you a couple of options: select the “From Text File…” option, and it will open up a very familiar dialog box asking you to select a file: if you’re on a Mac, it’ll look like the usual Finder/Explorer window that you use to choose a file. I’m assuming that you’re familiar with your own computer, so you should have no problem finding the CSV file that you want to import! Find the one you want, then click on the “Open” button. When you do this, you’ll see a window that looks like the one in Figure 3.2.\n\n\n\nFigure 3.2: A dialog box on a Mac asking you to select the CSV file R should try to import. Mac users will recognise this immediately: it’s the usual way in which a Mac asks you to find a file. Windows users won’t see this: they’ll see the usual explorer window that Windows always gives you when it wants you to select a file.\n\n\nThe import data set window is relatively straightforward to understand.\n\n\n\nThe RStudio window for importing a CSV file into R.\n\n\nIn the top left corner, you need to type the name of the variable you R to create. By default, that will be the same as the file name: our file is called booksales.csv, so RStudio suggests the name booksales. If you’re happy with that, leave it alone. If not, type something else. Immediately below this are a few things that you can tweak to make sure that the data gets imported correctly:\n\nHeading. Does the first row of the file contain raw data, or does it contain headings for each variable? The booksales.csv file has a header at the top, so I selected “yes”.\nSeparator. What character is used to separate different entries? In most CSV files this will be a comma (it is “comma separated” after all). But you can change this if your file is different.\nDecimal. What character is used to specify the decimal point? In English speaking countries, this is almost always a period (i.e., .). That’s not universally true: many European countries use a comma. So you can change that if you need to.\nQuote. What character is used to denote a block of text? That’s usually going to be a double quote mark. It is for the booksales.csv file, so that’s what I selected.\n\nOne nice thing about the RStudio window is that it shows you the raw data file at the top of the window, and it shows you a preview of the data at the bottom. If the data at the bottom doesn’t look right, try changing some of the settings on the left hand side. Once you’re happy, click “Import”. When you do, two commands appear in the R console:\n\nbooksales &lt;- read.csv(\"~/Rbook/data/booksales.csv\")\nView(booksales)\n\nThe first of these commands is the one that loads the data. The second one will display a pretty table showing the data in RStudio.\nNote, however, that this variable looks a bit different from the sales variable we created using read_csv. This is because RStudio has used read.csv() rather than read_csv() as we did in the previous section. Again, the differences will be discussed more in Chapter 5.\n\n\n3.1.3 Loading workspace files using R\nWhen I used the list.files() command to list the contents of the /Users/dan/Rbook/data directory, the output referred to a file called booksales.Rdata. Let’s say I want to load the data from this file into my workspace. The way I do this is with the load() function. There are two arguments to this function, but the only one we’re interested in is\n\nfile. This should be a character string that specifies a path to the file that needs to be loaded. You can use an absolute path or a relative path to do so.\n\nUsing the absolute file path, the command would look like this:\n\nload( file = \"/home/christian/Documents/teaching/statslab/data/booksales.Rdata\" )\n\nbut this is long and ugly. Given that the working directory is /home/christian/Documents/teaching/statslab, I could use a relative file path, like so:\n\nload( file = \"./data/booksales.Rdata\" )\n\nAnother strategy would be to first change the working directory to whatever directory contains the desired file (setwd() or Session-&gt;Set Working Directory) and then load the file using only the file name (e.g., load(file = \"booksales.Rdata\")). This may seem tempting, but we will avoid doing this because changing the working directory can be confusing if you’re not paying close attention. Instead, we will assume that a) our working directory is the “statslab” directory we created previously (Section 4.3) and b) all of our data files (CSVs and workspaces) are located in the “data” subdirectory. Outside of this class, however, you may not be able to move data files and/or R scripts. In such cases, it may be easier to modify the working directory. Just be aware that you may need to change the working directory both before and after loading the desired file(s). Don’t get lost (you can always figure out what the current working directory is with getwd())!\n\n\n3.1.4 Loading workspace files using RStudio\nOkay, so how do we open an .Rdata file using the RStudio file panel? It’s terribly simple. First, use the file panel to find the folder that contains the file you want to load. If you look at Figure Figure 3.3, you can see that there are several .Rdata files listed. Let’s say I want to load the booksales.Rdata file. All I have to do is click on the file name. RStudio brings up a little dialog box asking me to confirm that I do want to load this file. I click yes. The following command then turns up in the console,\n\nload(\"./data/booksales.Rdata\")\n\nand the new variables will appear in the workspace (you’ll see them in the Environment panel in RStudio, or if you type who()). So easy it barely warrants having its own section.\n\n\n\nFigure 3.3: The file panel is the area shown in the lower right hand corner. It provides a very easy way to browse and navigate your computer using R. See main text for details.\n\n\n\n\n3.1.5 Saving a workspace file using save\nNot surprisingly, saving data is very similar to loading data. Although RStudio provides a simple way to save files (see below), it’s worth understanding the actual commands involved. There are two commands you can use to do this, save() and save.image(). If you’re happy to save all of the variables in your workspace into the data file, then you should use save.image(). And if you’re happy for R to save the file into the current working directory, all you have to do is this:\n\nsave.image( file = \"myfile.Rdata\" )\n\nSince file is the first argument, you can shorten this to save.image(\"myfile.Rdata\"); and if you want to save to a different directory, then (as always) you need to be more explicit about specifying the path to the file. Suppose, however, I have several variables in my workspace, and I only want to save some of them. For instance, I might have this as my workspace:\n\nwho()\n##   -- Name --   -- Class --   -- Size --\n##   data         data.frame    3 x 2     \n##   handy        character     1         \n##   junk         numeric       1        \n\nI want to save data and handy, but not junk. But I don’t want to delete junk right now, because I want to use it for something else later on. This is where the save() function is useful, since it lets me indicate exactly which variables I want to save. Here is one way I can use the save function to solve my problem:\n\nsave(data, handy, file = \"myfile.Rdata\")\n\nImportantly, you must specify the name of the file argument. The reason is that if you don’t do so, R will think that \"myfile.Rdata\" is actually a variable that you want to save, and you’ll get an error message. Finally, I should mention a second way to specify which variables the save() function should save, which is to use the list argument. You do so like this:\n\nvars_to_save &lt;- c(\"data\", \"handy\")   # the variables to be saved\nsave( file = \"booksales2.Rdata\", list = vars_to_save )   # the command to save them\n\n\n\n3.1.6 Saving a workspace file using RStudio\nRStudio allows you to save the workspace pretty easily. In the environment panel ((fig:workspace?)) you can see the “save” button. There’s no text, but it’s the same icon that gets used on every computer everywhere: it’s the one that looks like a floppy disk. You know, those things that haven’t been used in about 20 years.\n\n\n\nFigure 3.4: The RStudio Environment panel shows you the contents of the workspace. The view shown above is the list view. To switch to the grid view, click on the menu item on the top right that currently reads list. Select grid from the dropdown menu, and then it will switch to a view like the one shown in the other workspace figure\n\n\nAlternatively, go to the “Session” menu and click on the “Save Workspace As…” option. This will bring up the standard “save” dialog box for your operating system. Type in the name of the file that you want to save it to, and all the variables in your workspace will be saved to disk. You’ll see an R command like this:\n\nsave.image(\"~/Desktop/Untitled.RData\")\n\nPretty straightforward, really."
  },
  {
    "objectID": "data.html#sec-dataactivities",
    "href": "data.html#sec-dataactivities",
    "title": "3  Working with Data",
    "section": "3.2 Activities",
    "text": "3.2 Activities\n\nDownload data\nDownload the following file: TBD\nInside the “statslab” directory/folder you created at the end of the previous chapter (Section 4.3), create a sub-directory/folder and name it “data”\nUnzip the data files and place them in the “data” directory/folder you just created\nOpen the 2015 data file (nba_all_seasons.csv) in your favorite spreadsheet software (e.g., Microsoft Excel)\nBriefly review the content of this file."
  },
  {
    "objectID": "tidyverse.html#tidy-data-ex",
    "href": "tidyverse.html#tidy-data-ex",
    "title": "4  The Tidyverse",
    "section": "4.1 “Tidy” data",
    "text": "4.1 “Tidy” data\nLet’s now learn about the concept of “tidy” data format.\n\n4.1.1 Definition of “tidy” data\nYou have surely heard the word “tidy” in your life:\n\n“Tidy up your room!”\n“Write your homework in a tidy way so it is easier to provide feedback.”\nMarie Kondo’s best-selling book, The Life-Changing Magic of Tidying Up: The Japanese Art of Decluttering and Organizing, and Netflix TV series Tidying Up with Marie Kondo.\n“I am not by any stretch of the imagination a tidy person, and the piles of unread books on the coffee table and by my bed have a plaintive, pleading quality to me - ‘Read me, please!’” - Linda Grant\n\nWhat does it mean for your data to be “tidy”? While “tidy” has a clear English meaning of “organized,” the word “tidy” in data science using R means that your data follows a standardized format. We will follow Hadley Wickham’s definition of “tidy” data:\n\nA dataset is a collection of values, usually either numbers (if quantitative) or strings AKA text data (if qualitative/categorical). Values are organised in two ways. Every value belongs to a variable and an observation. A variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units. An observation contains all values measured on the same unit (like a person, or a day, or a city) across attributes.\n“Tidy” data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\n\n\n\n\nTidy data\n\n\nFor example, say you have the following table of stock prices:\n\n\n\nStock prices (non-tidy format)\n\n\nDate\nBoeing stock price\nAmazon stock price\nGoogle stock price\n\n\n\n\n2009-01-01\n$173.55\n$174.90\n$174.34\n\n\n2009-01-02\n$172.61\n$171.42\n$170.04\n\n\n\n\n\n\n\nAlthough the data are neatly organized in a rectangular spreadsheet-type format, they do not follow the definition of data in “tidy” format. While there are three variables corresponding to three unique pieces of information (date, stock name, and stock price), there are not three columns. In “tidy” data format, each variable should be its own column. Notice that both tables present the same information, but in different formats.\n\n\n\nStock prices (tidy format)\n\n\nDate\nStock Name\nStock Price\n\n\n\n\n2009-01-01\nBoeing\n$173.55\n\n\n2009-01-01\nAmazon\n$174.90\n\n\n2009-01-01\nGoogle\n$174.34\n\n\n2009-01-02\nBoeing\n$172.61\n\n\n2009-01-02\nAmazon\n$171.42\n\n\n2009-01-02\nGoogle\n$170.04\n\n\n\n\n\n\n\nNow we have the requisite three columns Date, Stock Name, and Stock Price. On the other hand, consider this data:\n\n\n\nExample of tidy data\n\n\nDate\nBoeing Price\nWeather\n\n\n\n\n2009-01-01\n$173.55\nSunny\n\n\n2009-01-02\n$172.61\nOvercast\n\n\n\n\n\n\n\nIn this case, even though the variable “Boeing Price” occurs just like in our non-“tidy” data, the data is “tidy” since there are three variables corresponding to three unique pieces of information: Date, Boeing price, and the Weather that particular day."
  },
  {
    "objectID": "tidyverse.html#tidyverse-package",
    "href": "tidyverse.html#tidyverse-package",
    "title": "4  The Tidyverse",
    "section": "4.2 tidyverse package",
    "text": "4.2 tidyverse package\nThe following four packages, which are among four of the most frequently used R packages for data science, will be heavily used throughout the book:\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(tidyr)\n\nThe ggplot2 package is for data visualization, dplyr is for data wrangling, readr is for importing CSV files into R (we used it in Section 3.1.1), and tidyr is for converting data to “tidy” format. There is a much quicker way to load these packages than by individually loading them: by installing and loading the tidyverse package. The tidyverse package acts as an “umbrella” package whereby installing/loading it will install/load multiple packages at once for you.\nAfter installing the tidyverse package as you would a normal package (see Section 1.5), running:\n\nlibrary(tidyverse)\n\naccomplishes the the same things as running:\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(stringr)\nlibrary(forcats)\n\nThe purrr, tibble, stringr, and forcats are left for a more advanced book; check out R for Data Science to learn about these packages.\nFor the remainder of this book, we’ll start every chapter by running library(tidyverse) instead of loading the various component packages individually. The tidyverse “umbrella” package gets its name from the fact that all the functions in all its packages are designed to have common inputs and outputs: data frames that are in “tidy” format. This standardization of input and output makes transitions between different functions in the different packages as seamless as possible. For more information, check out the tidyverse.org webpage for the package."
  },
  {
    "objectID": "tidyverse.html#sec-rlangactivities",
    "href": "tidyverse.html#sec-rlangactivities",
    "title": "4  The Tidyverse",
    "section": "4.3 Activities",
    "text": "4.3 Activities\n\nOpen RStudio\nIf you have not done so already, install the tidyverse package using the instructions found in Section 1.5.1\nVerify installation of the tidyverse package by loading the package using the instructions found in Section 1.5.2\nCheck out all the cheatsheets available for tidyverse package by going to the Help menu and selecting “Cheat Sheets”. You can look over the readr cheat sheet for now, but remember this resource as we will be diving into the other tidyverse packages (e.g., dplyr, ggplot2) in future chapters."
  },
  {
    "objectID": "tibbles.html#what-is-a-tibble",
    "href": "tibbles.html#what-is-a-tibble",
    "title": "5  Dataframes",
    "section": "5.1 What is a tibble?",
    "text": "5.1 What is a tibble?\nA tibble is a type of dataframe commonly used in the tidyverse. Tibbles contain the same basic information as a corresponding dataframe, but tibbles are slightly different in a variety of minor ways. Let’s take a look at the iris data represented as a tibble. We’ll again confine our inspection to the first five rows:\n\n\n# A tibble: 5 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1          5.1         3.5          1.4         0.2 setosa \n2          4.9         3            1.4         0.2 setosa \n3          4.7         3.2          1.3         0.2 setosa \n4          4.6         3.1          1.5         0.2 setosa \n5          5           3.6          1.4         0.2 setosa \n\n\nNote that the data is identical to that seen above. However, the tibble provides some additional useful information. It tells us that this particular tibble has five columns and, because we only asked for the first five rows, also has five rows. In addition, it tells us about the data type of each column. The first four columns are of type dbl, which is double-precision floating point number (a decimal). The last column is of type fct, or factor. In R, factors are used to represent categorical variables, variables that have a fixed set of possible values (in this case, the set of species)."
  },
  {
    "objectID": "tibbles.html#tidying-your-data",
    "href": "tibbles.html#tidying-your-data",
    "title": "5  Dataframes",
    "section": "5.2 Tidying your data",
    "text": "5.2 Tidying your data\nFor the rest of this book, we will primarily deal with data that is already in “tidy” format as explained Chapter 4. However, many data sets you encounter in the world are in so-called “wide” format. If you wish to use the tidyverse packages, you will first have to convert these date sets to “tidy” format. To do so, we recommend using the pivot_longer() function in the tidyr package.\nTo illustrated, let’s load some data from the fivethirtyeight package. The fivethirtyeight package provides access to the data sets used in many articles published by the data journalism website, FiveThirtyEight.com. For a complete list of all data sets included in the fivethirtyeight package, check out the package webpage.\nLet’s focus our attention on the drinks dataframe:\n\n\n# A tibble: 5 × 5\n  country     beer_servings spirit_servings wine_servings total_litres_of_pure…¹\n  &lt;chr&gt;               &lt;int&gt;           &lt;int&gt;         &lt;int&gt;                  &lt;dbl&gt;\n1 Afghanistan             0               0             0                    0  \n2 Albania                89             132            54                    4.9\n3 Algeria                25               0            14                    0.7\n4 Andorra               245             138           312                   12.4\n5 Angola                217              57            45                    5.9\n# ℹ abbreviated name: ¹​total_litres_of_pure_alcohol\n\n\nAfter reading the help file (i.e., by running ?drinks), you’ll see that drinks is a dataframe containing results from a survey of beer, spirits, and wine consumption originally reported on FiveThirtyEight.com in Mona Chalabi’s article: “Dear Mona Followup: Where Do People Drink The Most Beer, Wine And Spirits?”.\nLet’s narrow down the data a bit. We’ll only consider 4 countries (the United States, China, Italy, and Saudi Arabia), omit the total_litres_of_pure_alcohol variable, and rename the other variables to something a bit more convenient. Don’t worry about the code here. We’ll get into all of these operations more in Chapter 6.\n\ndrinks_smaller &lt;- drinks %&gt;% \n  filter(country %in% c(\"USA\", \"China\", \"Italy\", \"Saudi Arabia\")) %&gt;% \n  select(-total_litres_of_pure_alcohol) %&gt;% \n  rename(beer = beer_servings, spirit = spirit_servings, wine = wine_servings)\ndrinks_smaller\n\n# A tibble: 4 × 4\n  country       beer spirit  wine\n  &lt;chr&gt;        &lt;int&gt;  &lt;int&gt; &lt;int&gt;\n1 China           79    192     8\n2 Italy           85     42   237\n3 Saudi Arabia     0      5     0\n4 USA            249    158    84\n\n\nNote that this data is not in “tidy” format. However, we can convert it to tidy format by using the pivot_longer() function from the tidyr package as follows:\n\ndrinks_smaller_tidy &lt;- drinks_smaller %&gt;% \n  pivot_longer(names_to = \"type\", \n               values_to = \"servings\", \n               cols = -country)\ndrinks_smaller_tidy\n\n# A tibble: 12 × 3\n   country      type   servings\n   &lt;chr&gt;        &lt;chr&gt;     &lt;int&gt;\n 1 China        beer         79\n 2 China        spirit      192\n 3 China        wine          8\n 4 Italy        beer         85\n 5 Italy        spirit       42\n 6 Italy        wine        237\n 7 Saudi Arabia beer          0\n 8 Saudi Arabia spirit        5\n 9 Saudi Arabia wine          0\n10 USA          beer        249\n11 USA          spirit      158\n12 USA          wine         84\n\n\nWe set the arguments to pivot_longer() as follows:\n\nnames_to here corresponds to the name of the variable in the new “tidy”/long dataframe that will contain the column names of the original data. Observe how we set names_to = \"type\". In the resulting drinks_smaller_tidy, the column type contains the three types of alcohol beer, spirit, and wine. Since type is a variable name that doesn’t appear in drinks_smaller, we use quotation marks around it. You’ll receive an error if you just use names_to = type here.\nvalues_to here is the name of the variable in the new “tidy” dataframe that will contain the values of the original data. Observe how we set values_to = \"servings\" since each of the numeric values in each of the beer, wine, and spirit columns of the drinks_smaller data corresponds to a value of servings. In the resulting drinks_smaller_tidy, the column servings contains the 4 \\(\\times\\) 3 = 12 numerical values. Note again that servings doesn’t appear as a variable in drinks_smaller so it again needs quotation marks around it for the values_to argument.\nThe third argument cols is the columns in the drinks_smaller dataframe you either want to or don’t want to “tidy.” Observe how we set this to -country indicating that we don’t want to “tidy” the country variable in drinks_smaller and rather only beer, spirit, and wine. Since country is a column that appears in drinks_smaller we don’t put quotation marks around it.\n\nThe third argument here of cols is a little nuanced, so let’s consider code that’s written slightly differently but that produces the same output:\n\n#|eval: false\ndrinks_smaller %&gt;% \n  pivot_longer(names_to = \"type\", \n               values_to = \"servings\", \n               cols = c(beer, spirit, wine))\n\n# A tibble: 12 × 3\n   country      type   servings\n   &lt;chr&gt;        &lt;chr&gt;     &lt;int&gt;\n 1 China        beer         79\n 2 China        spirit      192\n 3 China        wine          8\n 4 Italy        beer         85\n 5 Italy        spirit       42\n 6 Italy        wine        237\n 7 Saudi Arabia beer          0\n 8 Saudi Arabia spirit        5\n 9 Saudi Arabia wine          0\n10 USA          beer        249\n11 USA          spirit      158\n12 USA          wine         84\n\n\nNote that the third argument now specifies which columns we want to “tidy” with c(beer, spirit, wine), instead of the columns we don’t want to “tidy” using -country. We use the c() function to create a vector of the columns in drinks_smaller that we’d like to “tidy.” Note that since these three columns appear one after another in the drinks_smaller dataframe, we could also do the following for the cols argument:\n\ndrinks_smaller %&gt;% \n  pivot_longer(names_to = \"type\", \n               values_to = \"servings\", \n               cols = beer:wine)\n\n# A tibble: 12 × 3\n   country      type   servings\n   &lt;chr&gt;        &lt;chr&gt;     &lt;int&gt;\n 1 China        beer         79\n 2 China        spirit      192\n 3 China        wine          8\n 4 Italy        beer         85\n 5 Italy        spirit       42\n 6 Italy        wine        237\n 7 Saudi Arabia beer          0\n 8 Saudi Arabia spirit        5\n 9 Saudi Arabia wine          0\n10 USA          beer        249\n11 USA          spirit      158\n12 USA          wine         84\n\ndrinks_smaller_tidy &lt;- drinks_smaller %&gt;%\n  gather(type, servings, -country)\n\nLet’s see what our “tidy” formatted dataframe, drinks_smaller_tidy, looks like.\n\n\n# A tibble: 12 × 3\n   country      type   servings\n   &lt;chr&gt;        &lt;chr&gt;     &lt;int&gt;\n 1 China        beer         79\n 2 Italy        beer         85\n 3 Saudi Arabia beer          0\n 4 USA          beer        249\n 5 China        spirit      192\n 6 Italy        spirit       42\n 7 Saudi Arabia spirit        5\n 8 USA          spirit      158\n 9 China        wine          8\n10 Italy        wine        237\n11 Saudi Arabia wine          0\n12 USA          wine         84\n\n\nConverting “wide” format data to “tidy” format often confuses new R users. The only way to learn to get comfortable with the pivot_longer() function is with practice, practice, and more practice using different data sets. For example, run ?pivot_longer and look at the examples in the bottom of the help file.\nIf however you want to convert a “tidy” dataframe to “wide” format, you will need to use the pivot_wider() function instead. Run ?pivot_wider and look at the examples in the bottom of the help file for examples.\nYou can also view examples of both pivot_longer() and pivot_wider() on the tidyverse.org webpage. There’s a nice example to check out the different functions available for data tidying and a case study using data from the World Health Organization on that webpage. Furthermore, each week the R4DS Online Learning Community posts a data set in the weekly #TidyTuesday event that might serve as a nice place for you to find other data to explore and transform."
  },
  {
    "objectID": "tibbles.html#sec-tibblesactivities",
    "href": "tibbles.html#sec-tibblesactivities",
    "title": "5  Dataframes",
    "section": "5.3 Activities",
    "text": "5.3 Activities\n\nLet’s load data we downloaded back in Section 3.2. Run the following:\n\n\nnba &lt;- read_csv(\"./data/nba_all_seasons.csv\")\n\n\nNow let’s take a quick peek at what’s in there by running the following:\n\n\nhead(nba)\n\n\nConsider the following questions:\n\nHow many columns are there in nba?\nHow many rows are there in nba?\nWhat is the data type of the player_height column?\nWhat is the data type of the player_weight column?\nWhat is the data type of the pts column?\nWhat is the data type of the draft_year column?\nWhat is the data type of the draft_round column?\nWhat is the data type of the draft_number column?\n\nLet’s take a moment to talk about what this data set contains. This data reflects statistics for players in the National Basketball Association (NBA). The data covers seasons from 1996-1997 to 2021-2022 (NBA seasons run from approximately October to June of the following year). Each row represents a player . Columns include each player’s name (player_name), the team the player played for (team_abbreviation), where the player went to college (college), the player’s height (player_height) and weight (player_weight), the year, round, and order in which the player was drafted (draft_year, draft_round, and draft_number) as well as a variety of statistics about the player’s performance (e.g., points scored, pts).\n\n\nDo any of these data types seem incorrect? Which ones?\nLet’s take a closer look. Open the data in RStudio’s spreadsheet viewer by running the following:\n\n\nView(nba)\n\n\nInspect the college column. Do you see any unusual values?\nGiven what you see in the tibble and the description above, do you think this data is in tidy format?"
  },
  {
    "objectID": "dplyr.html#what-is-dplyr",
    "href": "dplyr.html#what-is-dplyr",
    "title": "6  Manipulating Data",
    "section": "6.1 What is dplyr",
    "text": "6.1 What is dplyr\nIn the tidyverse, the package responsible for such activities is called dplyr. This package contains a set of functions, each of which manipulates data in a particular way. Each of these functions is very useful on its own. However, the real power of dplyr comes from the fact that you can repeatedly applying dplyr functions, each operating on the result from the previous. These “chains” of functions allow you to compose complex data manipulation operations that ultimately transform your data into whatever you need. For this reason, dplyr is sometimes said to instantiate a “grammar” of data manipulation, a grammar which is made up of several “verbs” (functions). These function include:\n\nfilter(): select a subset of rows from a data frame\narrange(): sort a data frame’s rows\nmutate() create new columns/variables based on existing columns/variables\nsummarize(): aggregate one or more columns/variables with a summary statistic (e.g., the mean)\ngroup_by(): assign rows to groups, such that each group shares some values in common\n\nBecause dplyr is part of the tidyverse, these all work similarly. Specifically, each dplyr function:\n\nTakes a dataframe as its first argument\nTakes additional arguments that often indicate which columns are to be operated on\nReturns a modified dataframe\n\nLet’s see some of these characteristics in action. To do so, we’ll first load the data we downloaded back in Section 3.2. Run the following:\n\nnba &lt;- read_csv(\"./data/nba_all_seasons.csv\", na = c(\"Undrafted\"))\n\nNote that we have utilized the na argument here. This tells read_csv that we would like any values found in the file that match \"Undrafted\" to be treated as “missing”. R uses NA to represent missing values and so any \"Undrafted\" values will be converted in to NA."
  },
  {
    "objectID": "dplyr.html#filter",
    "href": "dplyr.html#filter",
    "title": "6  Manipulating Data",
    "section": "6.2 filter",
    "text": "6.2 filter\nThe filter() function allows you to specify criteria about the values of a variable in your data set and then filters out only the rows that match that criteria.\nThe team_abbreviation for the New York Knicks is \"NYK\". Run the following and look at the results in RStudio’s spreadsheet viewer to ensure that only players on the Knicks heading to Portland are chosen here:\n\nfilter(nba, team_abbreviation==\"NYK\")\n\n# A tibble: 410 × 22\n    ...1 player_name team_abbreviation   age player_height player_weight college\n   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;             &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;  \n 1    81 Herb Willi… NYK                  39          211.         118.  Ohio S…\n 2   146 Allan Hous… NYK                  26          198.          90.7 Tennes…\n 3   195 Buck Willi… NYK                  37          203.         102.  Maryla…\n 4   204 Charles Oa… NYK                  33          206.         111.  Virgin…\n 5   209 Chris Chil… NYK                  29          190.          88.5 Boise …\n 6   212 Chris Jent  NYK                  27          201.          99.8 Ohio S…\n 7   218 Charlie Wa… NYK                  26          188.          86.2 Florid…\n 8   230 Scott Broo… NYK                  31          180.          74.8 Califo…\n 9   293 Walter McC… NYK                  23          208.         104.  Kentuc…\n10   347 Larry John… NYK                  28          201.         119.  Nevada…\n# ℹ 400 more rows\n# ℹ 15 more variables: country &lt;chr&gt;, draft_year &lt;dbl&gt;, draft_round &lt;dbl&gt;,\n#   draft_number &lt;dbl&gt;, gp &lt;dbl&gt;, pts &lt;dbl&gt;, reb &lt;dbl&gt;, ast &lt;dbl&gt;,\n#   net_rating &lt;dbl&gt;, oreb_pct &lt;dbl&gt;, dreb_pct &lt;dbl&gt;, usg_pct &lt;dbl&gt;,\n#   ts_pct &lt;dbl&gt;, ast_pct &lt;dbl&gt;, season &lt;chr&gt;\n\n\nIf you prefer a more thorough inspection, you can open the data in RStudio’s spreadsheet viewer:\n\nView(filter(nba, team_abbreviation==\"NYK\"))\n\nIn either case, we are asking for a test of equality, keeping any rows where team_abbreviation==\"NYK\" is true and removing (filtering) any rows where team_abbreviation==\"NYK\" is false. To do so, we use the double equal sign ==, not a single equal sign =. In other words filter(nba, team_abbreviation = \"NYK\") will yield an error. This is a convention across many programming languages. If you are new to coding, you’ll probably forget to use the double equal sign == a few times before you get the hang of it.\nThe equality operator is not the only operator available to us. Others include:\n\n&gt; corresponds to “greater than”\n&lt; corresponds to “less than”\n&gt;= corresponds to “greater than or equal to”\n&lt;= corresponds to “less than or equal to”\n!= corresponds to “not equal to.” The ! is used in many programming languages to indicate “not.”\n\nFurthermore, you can combine multiple criteria using operators that make comparisons:\n\n| corresponds to “or”\n& corresponds to “and”\n\nLet’s look at an example that uses the ! “not” operator to pick rows that don’t match a criteria. As mentioned earlier, the ! can be read as “not.” Here we are filtering rows corresponding to players thare are not from the United States:\n\nfilter(nba, country!=\"USA\")\n\nLet’s combine two different requirements using the |:\n\nfilter(nba, country==\"Jamaica\" | college==\"Michigan\")\n\nThis will select players that are either from Jamica or went to Michigan (or both). We can also request the inverse of this.\n\nfilter(nba, !(country==\"Jamaica\" | college==\"Michigan\"))\n\nThis will select players that are not from Jamica and those players that did not go to Michigan. Note the use of parentheses around `(country==\"Jamaica\" | college==\"Michigan\"). If we used the !, but did not use the parentheses, we would only applying the “not” to the first test (country==“Jamaica”), not to the combination of (country==\"Jamaica\" | college==\"Michigan\"). You can try it and compare the results:\n\nfilter(nba, !country==\"Jamaica\" | college==\"Michigan\")\n\nThis request, in contrast to the one above, is for players that are not from Jamaica or went to Michigan (or both). So be very careful about the order of operations and use parentheses liberally. It helps to minimize errors and makes your code more explicit and therefore more readable.\nLet’s see a slightly more complicated request that combines several different operators:\n\nfilter(nba, team_abbreviation==\"NYK\" & (country!=\"USA\" | college==\"Michigan\") & age &gt;= 25)\n\nHere we have filtered the data to retain all rows corresponding to players from the NY Knicks, who are age 25 or older, and either went to Michigan or are not from the United States. You can this this yourself and verify that the output matches these requirements (remember that you can use View() if you wish to inspect the entire result).\nLet’s request players that went to either Michigan, Duke, or Georgetown.\n\nfilter(nba, college==\"Michigan\" | college==\"Duke\" | college==\"Georgetown\")\n\nThis works, but as we progressively include more collegs, this will get unwieldy to write. A slightly shorter approach uses the %in% operator along with the c() function. Recall from Subsection Section 2.5 that the c() function “combines” or “concatenates” values into a single vector of values.\n\nfilter(nba, college %in% c(\"Michigan\", \"Duke\", \"Georgetown\", \"UCLA\", \"Kentucky\"))\n\nWhat this code is doing is filtering our for all flights where college is in the vector of airports c(\"Michigan\", \"Duke\", \"Georgetown\", \"UCLA\", \"Kentucky\"). This approach produces results that are similar to a sequence of | operators, but takes much less energy to write (and read). The %in% operator is useful for looking for matches commonly in one vector/variable compared to another.\nAs a final note, we recommend that filter() should often be among the first tidyverse “verbs” you consider applying to your data. This cleans your dataset to only those rows you care about, or to put it another way, it narrows down the scope of your data to just the observations you care about."
  },
  {
    "objectID": "dplyr.html#pipe",
    "href": "dplyr.html#pipe",
    "title": "6  Manipulating Data",
    "section": "6.3 pipe",
    "text": "6.3 pipe\nBefore we go any further, let’s first introduce a nifty tool that gets loaded with the dplyr package: the pipe operator %&gt;%. The pipe operator allows us to combine multiple tidyverse operations into a single sequential chain of actions.\nLet’s start with a hypothetical example. Say you would like to perform a hypothetical sequence of operations on a hypothetical data frame x using hypothetical functions f(), g(), and h():\n\nTake x then\nUse x as an input to a function f() then\nUse the output of f(x) as an input to a function g() then\nUse the output of g(f(x)) as an input to a function h()\n\nOne way to achieve this sequence of operations is by using nesting parentheses as follows:\n\nh(g(f(x)))\n\nThis code isn’t so hard to read since we are applying only three functions: f(), then g(), then h() and each of the functions has a short name. Further, each of these functions also only has one argument. However, you can imagine that this will get progressively more difficult to read as the number of functions applied in your sequence increases and the arguments in each function grow more numerous. This is where the pipe operator %&gt;% comes in handy. The pipe takes the output of one function and then “pipes” it to be the input of the next function. Furthermore, a helpful trick is to read %&gt;% as “then” or “and then”. For example, you can obtain the same output as the hypothetical sequence of functions as follows:\n\nx %&gt;% \n  f() %&gt;% \n  g() %&gt;% \n  h()\n\nYou would read this sequence as:\n\nTake x then\nUse this output as the input to the next function f() then\nUse this output as the input to the next function g() then\nUse this output as the input to the next function h()\n\nThough both approaches achieve the same goal, the latter is much more human-readable because you can clearly read the sequence of operations line-by-line. But what are the hypothetical x, f(), g(), and h()? Throughout this chapter on data wrangling:\n\nThe starting value, x, will be a data frame. For example, the nba data frame we have been exploring so far.\nThe sequence of functions, here f(), g(), and h(), will mostly be a sequence of any number of the data wrangling verb-named functions we listed above. For example, the filter(college == \"Michigan\") function and argument specified we previewed earlier.\nThe result will be the transformed/modified data frame that you want.\n\nSo instead of\n\nfilter(nba, team_abbreviation==\"NYK\")\n\nwe can instead write:\n\nnba %&gt;%\n    filter(team_abbreviation==\"NYK\")\n\nThe benefits of this may not be immediately obvious. But the ability to form a chain of data wrangling operations by combining tidyverse functions (verbs) into a single sequence will be utilized extensively and is made possible by the pipe operator %&gt;%."
  },
  {
    "objectID": "dplyr.html#summarize",
    "href": "dplyr.html#summarize",
    "title": "6  Manipulating Data",
    "section": "6.4 summarize",
    "text": "6.4 summarize\nThe next common task when working with data frames is to compute summary statistics. Summary statistics are single numerical values that summarize a large number of values. Commonly known examples of summary statistics include the mean (i.e., the “average”), the median, and the mode. Other examples of summary statistics that might not immediately come to mind include the sum, the smallest value also called the minimum, the largest value also called the maximum, and the standard deviation.\nLet’s calculate two summary statistics of the draft_round variable in the nba data frame: the mean and standard deviation. To compute these summary statistics, we need the mean() and sd() summary functions in R. Summary functions in R take in many values and return a single value. More precisely, we’ll use the mean() and sd() summary functions within the summarize() function from the dplyr package. The summarize() function takes in a data frame and returns a data frame with only one row corresponding to the value of the summary statistic(s).\nWe’ll save the results in a new data frame called summary_round that will have two columns/variables: the mean and the std_dev:\n\nsummary_round &lt;- nba %&gt;% \n  summarize(mean = mean(draft_round), std_dev = sd(draft_round))\nsummary_round\n\n# A tibble: 1 × 2\n   mean std_dev\n  &lt;dbl&gt;   &lt;dbl&gt;\n1    NA      NA\n\n\nWhy are the values returned NA? NA is how R encodes missing values where NA indicates “not available” or “not applicable.” If a value for a particular row and a particular column does not exist, NA is stored instead. Values can be missing for many reasons. Perhaps the data was collected but someone forgot to enter it. Perhaps the data was not collected at all because it was too difficult to do so. Perhaps there was an erroneous value that someone entered that has been corrected to read as missing. You’ll often encounter issues with missing values when working with real data.\nGoing back to our summary_round output, by default any time you try to calculate a summary statistic of a variable that has one or more NA missing values in R, NA is returned. If what you wish to calculate is the summary of all the valid, non-missing values, you can set the na.rm argument to TRUE, where rm is short for “remove”. The code that follows computes the mean and standard deviation of all non-missing values of age:\n\nsummary_round &lt;- nba %&gt;% \n  summarize(mean = mean(draft_round, na.rm = TRUE), \n            std_dev = sd(draft_round, na.rm = TRUE))\nsummary_round\n\n# A tibble: 1 × 2\n   mean std_dev\n  &lt;dbl&gt;   &lt;dbl&gt;\n1  1.30   0.507\n\n\nNotice how the na.rm = TRUE are used as arguments to the mean() and sd() summary functions individually, and not to the summarize() function.\nHowever, one needs to be cautious whenever ignoring missing values as we’ve just done. We will consider the possible ramifications of blindly sweeping rows with missing values “under the rug”. This is in fact why the na.rm argument to any summary statistic function in R is set to FALSE by default. In other words, R does not ignore rows with missing values by default. R is alerting you to the presence of missing data and you should be mindful of this missingness and any potential causes of this missingness throughout your analysis.\nWhat are other summary functions we can use inside the summarize() verb to compute summary statistics? You can use any function in R that takes many values and returns just one. Here are just a few:\n\nmean(): the average\nsd(): the standard deviation, which is a measure of spread\nmin() and max(): the minimum and maximum values, respectively\nIQR(): interquartile range\nsum(): the total amount when adding multiple numbers\nn(): a count of the number of rows in each group. This particular summary function will make more sense when group_by() is covered below.\n\n\n\n\n\n\n\nSometimes missingness is data\n\n\n\nImagine a public health researcher is studying the effect of smoking on lung cancer for a large number of patients who are observed every five years. She notices that a large number of patients have missing data points, particularly in later observations. The researcher takes the approach outlined above, choosing to ignore these patients in her analysis. How might this be misleading?\n\n\nLet’s see just a couple more examples of summaries. Here we ask for the number of unique college that appear in the data set:\n\nnba %&gt;%\n    summarise(n_colleges = n_distinct(college))\n\n# A tibble: 1 × 1\n  n_colleges\n       &lt;int&gt;\n1        345\n\n\nHere we combine a filter operation with a summarize operation to calculate the average number of points scored by players in the 2000-2001 NBA season:\n\nnba %&gt;%\n    filter(season == \"2000-01\") %&gt;%\n    summarize(total_points = mean(pts))\n\n# A tibble: 1 × 1\n  total_points\n         &lt;dbl&gt;\n1         7.81"
  },
  {
    "objectID": "dplyr.html#group_by",
    "href": "dplyr.html#group_by",
    "title": "6  Manipulating Data",
    "section": "6.5 group_by",
    "text": "6.5 group_by\nSay instead of a single mean number of points for the entire NBA, you would like a mean number of points separately for each college players attended. In other words, we would like to compute the mean number of points split by college. We can do this by “grouping” the pts measurements by the values of another variable, in this case by the values of the variable college . Run the following code:\n\nnba %&gt;%\n    group_by(college) %&gt;% \n    summarize(mean = mean(pts),\n            std_dev = sd(pts))\n\n# A tibble: 345 × 3\n   college                   mean std_dev\n   &lt;chr&gt;                    &lt;dbl&gt;   &lt;dbl&gt;\n 1 \"\"                        9.9    3.79 \n 2 \"Alabama\"                 9.21   5.83 \n 3 \"Alabama A&M\"             2.2   NA    \n 4 \"Alabama Huntsville\"      2.05   0.778\n 5 \"Alabama-Birmingham\"      2.4    1.42 \n 6 \"Albany State (GA)\"       0.45   0.212\n 7 \"American International\"  8.34   2.71 \n 8 \"American University\"     6      8.49 \n 9 \"Arizona\"                 8.93   5.83 \n10 \"Arizona St.\"             0     NA    \n# ℹ 335 more rows\n\n\nThis code is similar to the previous code that created summary_round, but with an extra group_by(month) added before the summarize(). Grouping the nba dataset by college and then applying the summarize() functions yields a data frame that displays the mean and standard deviation temperature split by the different colleges that appear in the data set.\nIt is important to note that the group_by() function doesn’t change data frames by itself. Rather it changes the meta-data, or data about the data, specifically the grouping structure. It is only after we apply the summarize() function that the data frame changes.\nFor example, when we run this code:\n\nnba\n\n# A tibble: 12,305 × 22\n    ...1 player_name team_abbreviation   age player_height player_weight college\n   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;             &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;  \n 1     0 Dennis Rod… CHI                  36          198.          99.8 Southe…\n 2     1 Dwayne Sch… LAC                  28          216.         118.  Florida\n 3     2 Earl Curet… TOR                  39          206.          95.3 Detroi…\n 4     3 Ed O'Bannon DAL                  24          203.         101.  UCLA   \n 5     4 Ed Pinckney MIA                  34          206.         109.  Villan…\n 6     5 Eddie John… HOU                  38          201.          97.5 Illino…\n 7     6 Eddie Jones LAL                  25          198.          86.2 Temple \n 8     7 Elden Camp… LAL                  28          213.         113.  Clemson\n 9     8 Eldridge R… ATL                  29          193.          86.2 Washin…\n10     9 Elliot Per… MIL                  28          183.          72.6 Memphis\n# ℹ 12,295 more rows\n# ℹ 15 more variables: country &lt;chr&gt;, draft_year &lt;dbl&gt;, draft_round &lt;dbl&gt;,\n#   draft_number &lt;dbl&gt;, gp &lt;dbl&gt;, pts &lt;dbl&gt;, reb &lt;dbl&gt;, ast &lt;dbl&gt;,\n#   net_rating &lt;dbl&gt;, oreb_pct &lt;dbl&gt;, dreb_pct &lt;dbl&gt;, usg_pct &lt;dbl&gt;,\n#   ts_pct &lt;dbl&gt;, ast_pct &lt;dbl&gt;, season &lt;chr&gt;\n\n\nObserve that the first line of the output reads # A tibble: 12305 x 22. This is an example of meta-data, in this case the number of observations/rows and variables/columns in nba. The actual data itself are the subsequent table of values. Now let’s pipe the nba data frame into group_by(college):\n\nnba %&gt;% \n  group_by(college)\n\n# A tibble: 12,305 × 22\n# Groups:   college [345]\n    ...1 player_name team_abbreviation   age player_height player_weight college\n   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;             &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;  \n 1     0 Dennis Rod… CHI                  36          198.          99.8 Southe…\n 2     1 Dwayne Sch… LAC                  28          216.         118.  Florida\n 3     2 Earl Curet… TOR                  39          206.          95.3 Detroi…\n 4     3 Ed O'Bannon DAL                  24          203.         101.  UCLA   \n 5     4 Ed Pinckney MIA                  34          206.         109.  Villan…\n 6     5 Eddie John… HOU                  38          201.          97.5 Illino…\n 7     6 Eddie Jones LAL                  25          198.          86.2 Temple \n 8     7 Elden Camp… LAL                  28          213.         113.  Clemson\n 9     8 Eldridge R… ATL                  29          193.          86.2 Washin…\n10     9 Elliot Per… MIL                  28          183.          72.6 Memphis\n# ℹ 12,295 more rows\n# ℹ 15 more variables: country &lt;chr&gt;, draft_year &lt;dbl&gt;, draft_round &lt;dbl&gt;,\n#   draft_number &lt;dbl&gt;, gp &lt;dbl&gt;, pts &lt;dbl&gt;, reb &lt;dbl&gt;, ast &lt;dbl&gt;,\n#   net_rating &lt;dbl&gt;, oreb_pct &lt;dbl&gt;, dreb_pct &lt;dbl&gt;, usg_pct &lt;dbl&gt;,\n#   ts_pct &lt;dbl&gt;, ast_pct &lt;dbl&gt;, season &lt;chr&gt;\n\n\nObserve that now there is additional meta-data: # Groups:   college [345] indicating that the grouping structure meta-data has been set based on the unique values of the variable college. On the other hand, observe that the data has not changed: it is still a table of 12305 \\(\\times\\) 22 values.\nOnly by combining a group_by() with another data wrangling operation, in this case summarize(), will the data actually be transformed.\n\nnba %&gt;% \n  group_by(college) %&gt;% \n  summarize(mean = mean(pts))\n\n# A tibble: 345 × 2\n   college                   mean\n   &lt;chr&gt;                    &lt;dbl&gt;\n 1 \"\"                        9.9 \n 2 \"Alabama\"                 9.21\n 3 \"Alabama A&M\"             2.2 \n 4 \"Alabama Huntsville\"      2.05\n 5 \"Alabama-Birmingham\"      2.4 \n 6 \"Albany State (GA)\"       0.45\n 7 \"American International\"  8.34\n 8 \"American University\"     6   \n 9 \"Arizona\"                 8.93\n10 \"Arizona St.\"             0   \n# ℹ 335 more rows\n\n\nIf you would like to remove this grouping structure meta-data, we can pipe the resulting data frame into the ungroup() function:\n\nnba %&gt;% \n  group_by(college) %&gt;% \n  ungroup()\n\n# A tibble: 12,305 × 22\n    ...1 player_name team_abbreviation   age player_height player_weight college\n   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;             &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;  \n 1     0 Dennis Rod… CHI                  36          198.          99.8 Southe…\n 2     1 Dwayne Sch… LAC                  28          216.         118.  Florida\n 3     2 Earl Curet… TOR                  39          206.          95.3 Detroi…\n 4     3 Ed O'Bannon DAL                  24          203.         101.  UCLA   \n 5     4 Ed Pinckney MIA                  34          206.         109.  Villan…\n 6     5 Eddie John… HOU                  38          201.          97.5 Illino…\n 7     6 Eddie Jones LAL                  25          198.          86.2 Temple \n 8     7 Elden Camp… LAL                  28          213.         113.  Clemson\n 9     8 Eldridge R… ATL                  29          193.          86.2 Washin…\n10     9 Elliot Per… MIL                  28          183.          72.6 Memphis\n# ℹ 12,295 more rows\n# ℹ 15 more variables: country &lt;chr&gt;, draft_year &lt;dbl&gt;, draft_round &lt;dbl&gt;,\n#   draft_number &lt;dbl&gt;, gp &lt;dbl&gt;, pts &lt;dbl&gt;, reb &lt;dbl&gt;, ast &lt;dbl&gt;,\n#   net_rating &lt;dbl&gt;, oreb_pct &lt;dbl&gt;, dreb_pct &lt;dbl&gt;, usg_pct &lt;dbl&gt;,\n#   ts_pct &lt;dbl&gt;, ast_pct &lt;dbl&gt;, season &lt;chr&gt;\n\n\nObserve how the # Groups:   college [345] meta-data is no longer present.\nLet’s now revisit the n() counting summary function we briefly introduced previously. Recall that the n() function counts rows. This is opposed to the sum() summary function that returns the sum of a numerical variable. For example, suppose we’d like to count how many NBA players went to each of the different colleges, we can run this:\n\nby_college &lt;- nba %&gt;% \n  group_by(college) %&gt;% \n  summarize(count = n())\nby_college\n\n# A tibble: 345 × 2\n   college                  count\n   &lt;chr&gt;                    &lt;int&gt;\n 1 \"\"                           5\n 2 \"Alabama\"                  119\n 3 \"Alabama A&M\"                1\n 4 \"Alabama Huntsville\"         2\n 5 \"Alabama-Birmingham\"         7\n 6 \"Albany State (GA)\"          2\n 7 \"American International\"     5\n 8 \"American University\"        2\n 9 \"Arizona\"                  279\n10 \"Arizona St.\"                1\n# ℹ 335 more rows\n\n\nWe see that there are 119 rows in which college==\"Alabama\" and 279 rows in which college==\"Arizona\". Note there is a subtle but important difference between sum() and n(); while sum() returns the sum of a numerical variable (adding), n() returns a count of the number of rows/observations (counting).\n\n6.5.1 Grouping by more than one variable\nYou are not limited to grouping by one variable. Say you want to know the number of players coming from each college for each NBA season. We can also group by a second variable season using group_by(college, season):\n\nby_college_annually &lt;- nba %&gt;% \n  group_by(college, season) %&gt;% \n  summarize(count = n())\n\n`summarise()` has grouped output by 'college'. You can override using the\n`.groups` argument.\n\nby_college_annually\n\n# A tibble: 3,637 × 3\n# Groups:   college [345]\n   college   season  count\n   &lt;chr&gt;     &lt;chr&gt;   &lt;int&gt;\n 1 \"\"        2017-18     2\n 2 \"\"        2018-19     2\n 3 \"\"        2020-21     1\n 4 \"Alabama\" 1996-97     9\n 5 \"Alabama\" 1997-98    10\n 6 \"Alabama\" 1998-99     8\n 7 \"Alabama\" 1999-00     6\n 8 \"Alabama\" 2000-01     7\n 9 \"Alabama\" 2001-02     6\n10 \"Alabama\" 2002-03     4\n# ℹ 3,627 more rows\n\n\nObserve that there are now 3637 rows to by_college_annually because there are 12305 unique colleges.\nWhy do we group_by(college, season) and not group_by(college) and then group_by(season)? Let’s investigate:\n\nby_college_annually_incorrect &lt;- nba %&gt;% \n  group_by(college) %&gt;% \n  group_by(season) %&gt;% \n  summarize(count = n())\nby_college_annually_incorrect\n\n# A tibble: 26 × 2\n   season  count\n   &lt;chr&gt;   &lt;int&gt;\n 1 1996-97   441\n 2 1997-98   439\n 3 1998-99   439\n 4 1999-00   438\n 5 2000-01   441\n 6 2001-02   440\n 7 2002-03   428\n 8 2003-04   442\n 9 2004-05   464\n10 2005-06   458\n# ℹ 16 more rows\n\n\nWhat happened here is that the second group_by(season) overwrote the grouping structure meta-data of the earlier group_by(college), so that in the end we are only grouping by season. The lesson here is if you want to group_by() two or more variables, you should include all the variables at the same time in the same group_by() adding a comma between the variable names."
  },
  {
    "objectID": "dplyr.html#mutate",
    "href": "dplyr.html#mutate",
    "title": "6  Manipulating Data",
    "section": "6.6 mutate",
    "text": "6.6 mutate\nAnother common transformation of data is to create/compute new variables based on existing ones. For example, the heights in our nba data set are measured in units of centimeters. But say you are more comfortable thinking of inches instead of centimeters. The formula to convert centimeters to inches is:\n\\[\n\\text{height in inches} = \\frac{\\text{height in centimeters.}}{2.54}\n\\]\nWe can apply this formula to the player_height variable using the mutate() function from the dplyr package, which takes existing variables and mutates them to create new ones.\n\nnba &lt;- nba %&gt;% \n  mutate(player_height_in_inch = player_height / 2.54)\n\nIn this code, we mutate() the nba data frame by creating a new variable player_height_in_inch = height / 2.54 and then overwrite the original nba data frame. Why did we overwrite the data frame nba, instead of assigning the result to a new data frame like nba_new? As a rough rule of thumb, as long as you are not losing original information that you might need later, it’s acceptable practice to overwrite existing data frames with updated ones, as we did here. On the other hand, why did we not overwrite the variable player_height, but instead created a new variable called player_height_in_inch? Because if we did this, we would have erased the original information contained in player_height of temperatures in centimeters that may still be valuable to us.\nLet’s now compute average heights in both inches and centimeters using the group_by() and summarize():\n\nsummary_college_height &lt;- nba %&gt;%\n    group_by(college) %&gt;% \n    summarize(mean_height_in_inch_in_cm = mean(player_height),\n              mean_height_in_inch_in_in = mean(player_height_in_inch))\nsummary_college_height\n\n# A tibble: 345 × 3\n   college                  mean_height_in_inch_in_cm mean_height_in_inch_in_in\n   &lt;chr&gt;                                        &lt;dbl&gt;                     &lt;dbl&gt;\n 1 \"\"                                            204.                      80.2\n 2 \"Alabama\"                                     200.                      78.6\n 3 \"Alabama A&M\"                                 211.                      83  \n 4 \"Alabama Huntsville\"                          185.                      73  \n 5 \"Alabama-Birmingham\"                          196.                      77.1\n 6 \"Albany State (GA)\"                           206.                      81  \n 7 \"American International\"                      196.                      77  \n 8 \"American University\"                         190.                      75  \n 9 \"Arizona\"                                     198.                      77.8\n10 \"Arizona St.\"                                 196.                      77  \n# ℹ 335 more rows\n\n\nLet’s consider another example. We can imagine placing players on a continuum, with one end representing the “star” players and the other end representing “support” players. We can quantify this dimension by by comparing a player’s average points per game (pts) to his average assists per game (ast). Let’s calculate this new variable using the mutate() function:\n\nnba &lt;- nba %&gt;% \n  mutate(star_support = pts - ast)\n\nLet’s take a look at only the pts, ast, and the resulting star_support variables for a few rows in our updated nba data frame.\n\n\n# A tibble: 5 × 3\n    pts   ast star_support\n  &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;\n1   5.7   3.1          2.6\n2   2.3   0.3          2  \n3  17.2   3.4         13.8\n4   1.5   2.9         -1.4\n5   3.7   0.6          3.1\n\n\nThe player in the third row scored an average of 17.2 points per game and made an average of 3.4 assists per game, so his star_support value is \\(17.2-3.5=13.8\\). On the other hand, the player in the fourth row averaged 1.5 points and 2.9 assists, so its star_support value is \\(1.5 - 2.9 = -1.4\\).\nLet’s look at some summary statistics of the star_support variable by considering multiple summary functions at once in the same summarize() code:\n\nstar_support_summary &lt;- nba %&gt;% \n  summarize(\n    min = min(star_support, na.rm = TRUE),\n    q1 = quantile(star_support, 0.25, na.rm = TRUE),\n    median = quantile(star_support, 0.5, na.rm = TRUE),\n    q3 = quantile(star_support, 0.75, na.rm = TRUE),\n    max = max(star_support, na.rm = TRUE),\n    mean = mean(star_support, na.rm = TRUE),\n    sd = sd(star_support, na.rm = TRUE),\n    missing = sum(is.na(star_support))\n  )\nstar_support_summary\n\n# A tibble: 1 × 8\n    min    q1 median    q3   max  mean    sd missing\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n1  -2.5   2.6    5.1     9  30.9  6.36  4.97       0\n\n\nWe see for example that the average value is 6.36 minutes, whereas the largest is 30.9! This summary contains quite a bit of information. However, it is often easier to visualize data to evaluate how values are distributed. We’ll take a look at that in Chapter 7.\nTo close out our discussion on the mutate() function to create new variables, note that we can create multiple new variables at once in the same mutate() code. Furthermore, within the same mutate() code we can refer to new variables we just created. Consider following:\n\nnba &lt;- nba %&gt;% \n  mutate(\n    player_height_in_inch = player_height / 2.54,\n    player_weight_in_lbs = player_weight / 2.2,\n    bmi = 703 * (player_weight_in_lbs / (player_height_in_inch^2)),\n  )"
  },
  {
    "objectID": "dplyr.html#arrange",
    "href": "dplyr.html#arrange",
    "title": "6  Manipulating Data",
    "section": "6.7 arrange",
    "text": "6.7 arrange\nOne of the most commonly performed data wrangling tasks is to sort a data frame’s rows in the alphanumeric order of one of the variables. The dplyr package’s arrange() function allows us to sort/reorder a data frame’s rows according to the values of the specified variable.\nSuppose we are interested in determining the average number of points per game scored by NBA players from different colleges:\n\nmean_pts &lt;- nba %&gt;% \n  group_by(college) %&gt;% \n  summarize(mean = mean(pts), \n            std_dev = sd(pts))\nmean_pts\n\n# A tibble: 345 × 3\n   college                   mean std_dev\n   &lt;chr&gt;                    &lt;dbl&gt;   &lt;dbl&gt;\n 1 \"\"                        9.9    3.79 \n 2 \"Alabama\"                 9.21   5.83 \n 3 \"Alabama A&M\"             2.2   NA    \n 4 \"Alabama Huntsville\"      2.05   0.778\n 5 \"Alabama-Birmingham\"      2.4    1.42 \n 6 \"Albany State (GA)\"       0.45   0.212\n 7 \"American International\"  8.34   2.71 \n 8 \"American University\"     6      8.49 \n 9 \"Arizona\"                 8.93   5.83 \n10 \"Arizona St.\"             0     NA    \n# ℹ 335 more rows\n\n\nObserve that by default the rows of the resulting mean_pts data frame are sorted in alphabetical order of college. Say instead we would like to see the same data, but sorted from the highest to the lowest average points (mean) instead:\n\nmean_pts %&gt;% \n  arrange(mean)\n\n# A tibble: 345 × 3\n   college                         mean std_dev\n   &lt;chr&gt;                          &lt;dbl&gt;   &lt;dbl&gt;\n 1 Arizona St.                     0     NA    \n 2 Chicago St.                     0     NA    \n 3 Denver                          0     NA    \n 4 Fairfield                       0     NA    \n 5 George Mason                    0     NA    \n 6 Lebanon Valley                  0     NA    \n 7 Lincoln Memorial                0     NA    \n 8 University of Colorado Boulder  0     NA    \n 9 Toledo                          0.2   NA    \n10 Albany State (GA)               0.45   0.212\n# ℹ 335 more rows\n\n\nThis is, however, the opposite of what we want. The rows are sorted with the least frequent destination airports displayed first. This is because arrange() always returns rows sorted in ascending order by default. To switch the ordering to be in “descending” order instead, we use the desc() function as so:\n\nmean_pts %&gt;% \n  arrange(desc(mean))\n\n# A tibble: 345 × 3\n   college                           mean std_dev\n   &lt;chr&gt;                            &lt;dbl&gt;   &lt;dbl&gt;\n 1 Davidson                          19.6    9.95\n 2 Lehigh                            18.4    7.06\n 3 Western Carolina                  16.1    7.50\n 4 Navy                              15.4    4.25\n 5 Marist                            15.4    1.92\n 6 Butler Community College          13.6    6.57\n 7 Louisiana Tech                    13.5    6.78\n 8 Trinity Valley Community College  13.5    6.63\n 9 Weber State                       13.3   11.6 \n10 Central Arkansas                  13.1    4.73\n# ℹ 335 more rows"
  },
  {
    "objectID": "dplyr.html#sec-tibblesactivities",
    "href": "dplyr.html#sec-tibblesactivities",
    "title": "6  Manipulating Data",
    "section": "6.8 Activities",
    "text": "6.8 Activities\nIf you haven’t already, load load nba data set we downloaded back in Section 3.2. And let’s pass c(\"Undrafted\") as the na argument:\n\nnba &lt;- read_csv(\"./data/nba_all_seasons.csv\", na = c(\"Undrafted\"))\n\n\nProduce a data frame consisting exclusively of players under the age of 25.\nProduce a data frame consisting exclusively of players who attended “Marquette”.\nProduce a data frame consisting exclusively of players who are either under the age of 25 or attended “Marquette”.\nProduce a data frame consisting exclusively of players under the age of 25 and attended “Marquette”.\nDetermine the average height of all players.\nDetermine the median height of all players who attended “Connecticut”.\nFor each college appearing in the data set, what is the average pts and ast of players who attended that college?\nTry calculating these school-wise averages separately for each season.\nCreate a new column in the data set called reb_diff and calculate it as the difference between the offensive rebound percentage (oreb_pct) and the defensive rebound percentage (dreb_pct).\nWhat is the standard deviation of reb_diff?\nFor each college appearing in the data set, what is the average ptsof players who attended that college. Which college has produced the highest average? Which college has produced the lowest average?"
  },
  {
    "objectID": "ggplot2.html#grammarofgraphics",
    "href": "ggplot2.html#grammarofgraphics",
    "title": "7  Visualizing Data",
    "section": "7.1 The grammar of graphics",
    "text": "7.1 The grammar of graphics\nWe start with a discussion of a theoretical framework for data visualization known as “the grammar of graphics.” This framework serves as the foundation for the ggplot2 package which we’ll use extensively in this chapter. In Chapter 6, we saw how dplyr provides a “grammar” of data manipulation, a grammar which is made up of several “verbs” (functions like filter and mutate). Similar to dplyr’s grammar of data manipulation, ggplto2 provides a a grammar of graphics that defines a set of rules for constructing statistical graphics by combining different types of layers. This grammar was created by Leland Wilkinson (Wilkinson 2012) and has been implemented in a variety of data visualization software platforms like R, but also Plotly and Tableau.\n\n7.1.1 Components of the grammar\nIn short, the grammar tells us that:\n\nA statistical graphic is a mapping of data variables to aesthetic attributes of geometric objects.\n\nSpecifically, we can break a graphic into the following three essential components:\n\ndata: the dataset containing the variables of interest.\ngeom: the geometric object in question. This refers to the type of object we can observe in a plot. For example: points, lines, and bars.\naes: aesthetic attributes of the geometric object. For example, x/y position, color, shape, and size. Aesthetic attributes are mapped to variables in the data set.\n\nYou might be wondering why we wrote the terms data, geom, and aes in a computer code type font. We’ll see very shortly that we’ll specify the elements of the grammar in R using these terms. However, let’s first break down the grammar with an example.\n\n\n7.1.2 An initial example\nLet’s take another look at our nba data set, this time via the grammar of graphics. Let’s specifically take a look at how things have changed over the years. As always, we need to load the data we downloaded back in Section 3.2. Run the following:\n\nnba &lt;- read_csv(\"./data/nba_all_seasons.csv\", na = c(\"Undrafted\"))\n\nWe need to do a bit of work before we can use season as a measure of time. This is because the season column is currently stored as a character vector, with values such as “2000-01” and “2011-12”. So we need two things: these character vectors need to be trimmed so that we retain only the first 4 characters in each vector and we then need to convert these character vectors to their corresponding numeric values. We’ll do that using the stringr package, yet another package that is part of the tidyverse.\n\nnba &lt;- nba %&gt;%\n    # select first 4 characters of `season`\n    mutate(season_int = substr(nba$season, start=1, stop=4))  %&gt;%\n    # convert to integer\n    mutate(season_int = as.integer(season_int))\n\nNow that we have time represented as a numeric column in our data set, we can use it to plot some data to visualize.\n\n\n`summarise()` has grouped output by 'team_abbreviation'. You can override using\nthe `.groups` argument.\n\n\n\n\n\nLet’s view this plot through the grammar of graphics. First, we have actually used two type of geometric object here: a line object and a point object. The point object provides the small circular data points. The line object provides the line segments connecting the points.\n\nThe data variable season_int gets mapped to the x-position aesthetic of the lines and the points.\nThe data variable pts gets mapped to the y-position aesthetic of the lines and the points.\nThe data variable team gets mapped to the color aesthetic of the lines and the points.\nThe data variable ast gets mapped to the size aesthetic of the points.\n\nThat being said, this is just an example. Plots can specify points, lines, bars, and a variety of other geometric objects.\nLet’s summarize the three essential components of the grammar in ?tbl-grammar-summary.\n\n\n?(caption)\n\n\n\n# A tibble: 7 × 3\n  geom  aes   `data variable`\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;          \n1 line  x     season_int     \n2 line  y     pts            \n3 line  color team           \n4 point x     season_int     \n5 point y     pts            \n6 point color team           \n7 point size  ast            \n\n\n\n\n\n7.1.3 Other components\nThere are other components of the grammar of graphics we can control as well. As you start to delve deeper into the grammar of graphics, you’ll start to encounter these topics more frequently. In this book, we’ll keep things simple and only work with these two additional components:\n\nfaceting breaks up a plot into several plots split by the values of another variable\nposition adjustments for barplots\n\nOther more complex components like scales and coordinate systems are left for a more advanced text such as R for Data Science. Generally speaking, the grammar of graphics allows for a high degree of customization of plots and also a consistent framework for easily updating and modifying them.\n\n\n7.1.4 ggplot2 package\nIn this book, we will use the ggplot2 package for data visualization, which is an implementation of the grammar of graphics for R. As we noted earlier, a lot of the previous section was written in a computer code type font. This is because the various components of the grammar of graphics are specified in the ggplot() function included in the ggplot2 package. For the purposes of this book, we’ll always provide the ggplot() function with the following arguments (i.e., inputs) at a minimum:\n\nThe data frame where the variables exist: the data argument.\nThe mapping of the variables to aesthetic attributes: the mapping argument which specifies the aesthetic attributes involved.\n\nAfter we’ve specified these components, we then add layers to the plot using the + sign. The most essential layer to add to a plot is the layer that specifies which type of geometric object we want the plot to involve: points, lines, bars, and others. Other layers we can add to a plot include the plot title, axes labels, visual themes for the plots, and facets (which we’ll see in Section 7.6).\nLet’s now put the theory of the grammar of graphics into practice."
  },
  {
    "objectID": "ggplot2.html#sec-FiveNG",
    "href": "ggplot2.html#sec-FiveNG",
    "title": "7  Visualizing Data",
    "section": "7.2 Five named graphs - the 5NG",
    "text": "7.2 Five named graphs - the 5NG\nIn order to keep things simple in this book, we will only focus on five different types of graphics, each with a commonly given name. We term these “five named graphs” or in abbreviated form, the 5NG:\n\nscatterplots\nlinegraphs\nhistograms\nboxplots\nbarplots\n\nWe’ll also present some variations of these plots, but with this basic repertoire of five graphics in your toolbox, you can visualize a wide array of different variable types. Note that certain plots are only appropriate for categorical variables, while others are only appropriate for numerical variables."
  },
  {
    "objectID": "ggplot2.html#sec-scatterplots",
    "href": "ggplot2.html#sec-scatterplots",
    "title": "7  Visualizing Data",
    "section": "7.3 5NG#1: Scatterplots",
    "text": "7.3 5NG#1: Scatterplots\nThe simplest of the 5NG are scatterplots. They allow you to visualize the relationship between two numerical variables. While you may already be familiar with scatterplots, let’s view them through the lens of the grammar of graphics. Specifically, we will visualize the relationship between the following two numerical variables in the nba data frame:\n\npts: average points per game each player scored\nast: average number of assists per game each player made\n\n\n7.3.1 Scatterplots via geom_point\nLet’s now go over the code that will create the desired scatterplot, while keeping in mind the grammar of graphics framework we introduced above. Let’s take a look at the code and break it down piece-by-piece.\n\nggplot(data = nba, mapping = aes(x = pts, y = ast)) + \n  geom_point()\n\nWithin the ggplot() function, we specify two of the components of the grammar of graphics as arguments (i.e., inputs):\n\nThe data as the nba data frame via data = nba.\nThe aesthetic mapping by setting mapping = aes(x = pts, y = ast). Specifically, the variable pts maps to the x position aesthetic, whereas the variable ast maps to the y position.\n\nWe then add a layer to the ggplot() function call using the + sign. The added layer in question specifies the third component of the grammar: the geometric object. In this case, the geometric object is set to be points by specifying geom_point(). After running these two lines of code in your console, you’ll notice two outputs: a warning message and this graphic.\n\n\n\n\n\nFigure 7.1: Assists versus points\n\n\n\n\nLet’s first unpack the graphic in Figure 7.1. Observe that a positive relationship exists between pts and ast: as the number of points increases, the number of assists also increases. Observe also the large mass of points clustered near (0, 0), the point indicating players have no points and no assists (e.g., what would be expected from a player that doesn’t play very much).\nBefore we continue, let’s make a few more observations about this code that created the scatterplot. Note that the + sign comes at the end of lines, and not at the beginning. You’ll get an error in R if you put it at the beginning of a line. When adding layers to a plot, you are encouraged to start a new line after the + (by pressing the Return/Enter button on your keyboard) so that the code for each layer is on a new line. As we add more and more layers to plots, you’ll see this will greatly improve the legibility of your code.\nTo stress the importance of adding the layer specifying the geometric object, consider Figure 7.2 where no layers are added. Because the geometric object was not specified, we have a blank plot which is not very useful!\n\nggplot(data = nba, mapping = aes(x = pts, y = ast))\n\n\n\n\nFigure 7.2: Assists versus points\n\n\n\n\n\n\n7.3.2 Overplotting\nThe large mass of points near (0, 0) in Figure can cause some confusion since it is hard to tell the true number of points that are actually in this lower corner. This is the result of a phenomenon called overplotting. As one may guess, this corresponds to points being plotted on top of each other over and over again. When overplotting occurs, it is difficult to know the number of points being plotted. There are two methods to address the issue of overplotting. Either by\n\nAdjusting the transparency of the points or\nAdding a little random “jitter”, or random “nudges”, to each of the points.\n\nMethod 1: Changing the transparency\nThe first way of addressing overplotting is to change the transparency/opacity of the points by setting the alpha argument in geom_point(). We can change the alpha argument to be any value between 0 and 1, where 0 sets the points to be 100% transparent and 1 sets the points to be 100% opaque. By default, alpha is set to 1. In other words, if we don’t explicitly set an alpha value, R will use alpha = 1.\nNote how the following code is identical to the code in Section @ref(scatterplots) that created the scatterplot with overplotting, but with alpha = 0.05 added to the geom_point() function:\n\nggplot(data = nba, mapping = aes(x = pts, y = ast)) + \n  geom_point(alpha = 0.05)\n\n\n\n\nFigure 7.3: Assists versus points\n\n\n\n\nThe key feature to note in Figure 7.3 is that the transparency of the points is cumulative: areas with a high-degree of overplotting are darker, whereas areas with a lower degree are less dark. Note furthermore that there is no aes() surrounding alpha = 0.05. This is because we are not mapping a variable to an aesthetic attribute, but rather merely changing the default setting of alpha. In fact, you’ll receive an error if you try to change the second line to read geom_point(aes(alpha = 0.05)).\nMethod 2: Jittering the points\nThe second way of addressing overplotting is by jittering all the points. This means giving each point a small “nudge” in a random direction. You can think of “jittering” as shaking the points around a bit on the plot. Let’s illustrate using a simple example first. Say we have a data frame with 4 identical rows of x and y values: (0,0), (0,0), (0,0), and (0,0). In Figure 7.4, we present both the regular scatterplot of these 4 points (on the left) and its jittered counterpart (on the right).\n\n\n\n\n\nFigure 7.4: Regular and jittered scatterplots\n\n\n\n\nIn the left scatterplot, observe that the 4 points are superimposed on top of each other. While we know there are 4 values being plotted, this fact might not be apparent to others. In the right scatterplot, the points are jittered and it is now plainly evident that this plot involves four points since each point is given a random “nudge.”\nKeep in mind, however, that jittering is strictly a visualization tool; even after creating a jittered scatterplot, the original values saved in the data frame remain unchanged.\nTo create a jittered scatterplot, instead of using geom_point(), we use geom_jitter(). Observe how the following code is very similar to the code that created Figure 7.1, but with geom_point() replaced with geom_jitter().\n\n\n\n\n\nFigure 7.5: Assists versus points jittered scatterplot\n\n\n\n\nIn order to specify how much jitter to add, we adjusted the width and height arguments to geom_jitter(). This corresponds to how hard you’d like to shake the plot in horizontal x-axis units and vertical y-axis units, respectively. In this case, both axes are in counts (number of points, number of assists). How much jitter should we add using the width and height arguments? On the one hand, it is important to add just enough jitter to break any overlap in points, but on the other hand, not so much that we completely alter the original pattern in points.\nAs can be seen in the resulting Figure 7.5, in this case jittering doesn’t really provide much new insight. In this particular case, it can be argued that changing the transparency of the points by setting alpha proved more effective. When would it be better to use a jittered scatterplot? When would it be better to alter the points’ transparency? There is no single right answer that applies to all situations. You need to make a subjective choice and own that choice. At the very least when confronted with overplotting, however, we suggest you make both types of plots and see which one better emphasizes the point you are trying to make.\n\n\n7.3.3 Summary\nScatterplots display the relationship between two numerical variables. They are among the most commonly used plots because they can provide an immediate way to see the trend in one numerical variable versus another. However, if you try to create a scatterplot where either one of the two variables is not numerical, you might get strange results. Be careful!\nWith medium to large datasets, you may need to play around with the different modifications to scatterplots we saw such as changing the transparency/opacity of the points or by jittering the points. This tweaking is often a fun part of data visualization, since you’ll have the chance to see different relationships emerge as you tinker with your plots."
  },
  {
    "objectID": "ggplot2.html#sec-linegraphs",
    "href": "ggplot2.html#sec-linegraphs",
    "title": "7  Visualizing Data",
    "section": "7.4 5NG#2: Linegraphs",
    "text": "7.4 5NG#2: Linegraphs\nThe next of the five named graphs are linegraphs. Linegraphs show the relationship between two numerical variables when the variable on the x-axis is ordinal; there is an inherent ordering to the variable.\nThe most common examples of linegraphs have some notion of time on the x-axis: hours, days, weeks, years, etc. Since time is naturally ordinal, we connect consecutive observations of the variable on the y-axis with a line. Linegraphs that have some notion of time on the x-axis are also called time series plots.\n\n7.4.1 Linegraphs via geom_line\nLet’s a linegraph to visualize a single NBA player’s number of points scored across seasons. To do so, we’ll use geom_line(), instead of using geom_point() as we did for the scatterplots above:\n\nggplot(\n    data = nba %&gt;%\n        filter(player_name == \"Stephen Curry\"),\n    mapping = aes(x = season_int, y = pts)\n) +\n    geom_line()\n\n\n\n\nStephen Curry points over time\n\n\n\n\nLet’s break down this code piece-by-piece in terms of the grammar of graphics. Within the ggplot() function call, we specify two of the components of the grammar of graphics as arguments:\n\nThe data. Here we have provided a filtered version of our nba data set, selecting only those row where player_name==\"Stephen Curry\".\nThe aesthetic mapping by setting mapping = aes(x = season_int, y = pts). Specifically, the variable season_int maps to the x position aesthetic, whereas the variable pts maps to the y position aesthetic.\n\nWe add a layer to the ggplot() function call using the + sign. The layer in question specifies the third component of the grammar: the geometric object in question. In this case, the geometric object is a line set by specifying geom_line().\n\n\n7.4.2 Summary\nLinegraphs, just like scatterplots, display the relationship between two numerical variables. However, it is preferred to use linegraphs over scatterplots when the variable on the x-axis (i.e., the explanatory variable) has an inherent ordering, such as some notion of time."
  },
  {
    "objectID": "ggplot2.html#sec-histograms",
    "href": "ggplot2.html#sec-histograms",
    "title": "7  Visualizing Data",
    "section": "7.5 5NG#3: Histograms",
    "text": "7.5 5NG#3: Histograms\nLet’s consider the pts variable in the nba data frame once again, but unlike with the linegraphs in Section 7.4, let’s say we don’t care about its relationship with time, but rather we only care about how the values of pts distribute. In other words:\n\nWhat are the smallest and largest values?\nWhat is the “center” or “most typical” value?\nHow do the values spread out?\nWhat are frequent and infrequent values?\n\nOne way to visualize this distribution of this single variable pts is to plot them on a horizontal line as we do in Figure 7.6:\n\n\n\n\n\nFigure 7.6: Plot of players’ points per-game point averages.\n\n\n\n\nThis gives us a bit of an idea of how the values of pts are distributed: note that values range from zero to approximately 20. In addition, there appear to be more values falling between approximately 3 and 10 than there are values falling above this range. However, because of the high degree of overplotting in the points, it’s hard to get a sense of exactly how many values are between 5 and 10.\nWhat is commonly produced instead of Figure 7.6 is known as a histogram. A histogram is a plot that visualizes the distribution of a numerical value as follows:\n\nWe first cut up the x-axis into a series of bins, where each bin represents a range of values.\nFor each bin, we count the number of observations that fall in the range corresponding to that bin.\nThen for each bin, we draw a bar whose height marks the corresponding count.\n\nLet’s drill-down on an example of a histogram, shown in Figure 7.7.\n\n\n\n\n\nFigure 7.7: Example histogram.\n\n\n\n\nLet’s focus only on values between 10 points and 20 points for now. Observe that there are five bins of equal width between 10 points and 20 points. Thus we have five bins of width 2 points each: one bin for the 10-12 range, another bin for the 13-14 range, etc.\n\nThe bin for the 10-12 range has a height of around 150. In other words, around 150 players scored a season average of between 10 and 20 points.\nThe bin for the 13-14 range has a height of around 100. In other words, around 100. players scored a season average of between 13 and 14 points.\nThe bin for the 15-16 range has a height of around 50. In other words, around 50 players scored a season average of between 15 and 16 points.\nAnd so on…\n\n\n7.5.1 Histograms via geom_histogram\nLet’s now present the ggplot() code to plot your first histogram! Unlike with scatterplots and linegraphs, there is now only one variable being mapped in aes(): the single numerical variable pts. The y-aesthetic of a histogram, the count of the observations in each bin, gets computed for you automatically. Furthermore, the geometric object layer is now a geom_histogram(). After running the following code, you’ll see the histogram in Figure 7.8 as well as a warning message. We’ll discuss the warning message first.\n\nggplot(data = nba %&gt;%\n        group_by(player_name) %&gt;%\n        summarize(pts = mean(pts)),\n       mapping = aes(x = pts)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 7.8: Histogram of average pts per game.\n\n\n\n\nThe warning is telling us that the histogram was constructed using bins = 30 for 30 equally spaced bins. This is known the default value for this argument (Section 2.4.1). We’ll see in the next section how to change the number of bins to another value than the default.\nNow let’s unpack the resulting histogram in Figure 7.9. Observe that values greater than 20 are rather rare. However, because of the large number of bins, it’s hard to get a sense for which range of temperatures is spanned by each bin; everything is one giant amorphous blob. So let’s add white vertical borders demarcating the bins by adding a color = \"white\" argument to geom_histogram() and ignore the warning about setting the number of bins to a better value:\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 7.9: Histogram of average pts per game.\n\n\n\n\nWe now have an easier time associating ranges of temperatures to each of the bins in Figure 7.10. We can also vary the color of the bars by setting the fill argument. For example, you can set the bin colors to be “blue steel” by setting fill = \"steelblue\":\n\nggplot(data = nba %&gt;%\n        group_by(player_name) %&gt;%\n        summarize(pts = mean(pts)),\n       mapping = aes(x = pts)) +\n  geom_histogram(color = \"white\", fill = \"steelblue\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 7.10: Histogram of average pts per game.\n\n\n\n\nIf you’re curious, run colors() to see all 657 possible choice of colors in R!\n\n\n7.5.2 Adjusting the bins\nObserve in Figure 7.10 that in the 10-20 range there appear to be roughly 111 bins. Thus each bin has width 20-10 divided by 11, or 0.91 points, which is not a very easily interpretable range to work with. Let’s improve this by adjusting the number of bins in our histogram in one of two ways:\n\nBy adjusting the number of bins via the bins argument to geom_histogram().\nBy adjusting the width of the bins via the binwidth argument to geom_histogram().\n\nUsing the first method, we have the power to specify how many bins we would like to cut the x-axis up in. As mentioned in the previous section, the default number of bins is 30. We can override this default, to say 25 bins, as follows:\n\nggplot(data = nba %&gt;%\n        group_by(player_name) %&gt;%\n        summarize(pts = mean(pts)),\n       mapping = aes(x = pts)) +\n  geom_histogram(bins = 25, color = \"white\")\n\nUsing the second method, instead of specifying the number of bins, we specify the width of the bins by using the binwidth argument in the geom_histogram() layer. For example, let’s set the width of each bin to be 5 points.\n\nggplot(data = nba %&gt;%\n        group_by(player_name) %&gt;%\n        summarize(pts = mean(pts)),\n       mapping = aes(x = pts)) +\n  geom_histogram(binwidth = 5, color = \"white\")\n\nWe compare both resulting histograms side-by-side in Figure 7.11.\n\n\n\n\n\nFigure 7.11: Setting histogram bins in two ways.\n\n\n\n\n\n\n7.5.3 Summary\nHistograms, unlike scatterplots and linegraphs, present information on only a single numerical variable. Specifically, they are visualizations of the distribution of the numerical variable in question."
  },
  {
    "objectID": "ggplot2.html#sec-facets",
    "href": "ggplot2.html#sec-facets",
    "title": "7  Visualizing Data",
    "section": "7.6 Facets",
    "text": "7.6 Facets\nBefore continuing with the next of the 5NG, let’s briefly introduce a new concept called faceting. Faceting is used when we’d like to split a particular visualization by the values of another variable. This will create multiple copies of the same type of plot with matching x and y axes, but whose content will differ.\nFor example, suppose we were interested in looking at how the histogram of players’ average points per game changed across seasons. We could “split” this histogram so that we had a separate histogram of pts for each of several values of season_int. We do this by adding facet_wrap(~ season_int) layer. Note the ~ is a “tilde” and can generally be found on the key next to the “1” key on US keyboards. The tilde is required and you’ll receive the error Error in as.quoted(facets) : object 'season_int' not found if you don’t include it here.\n\nggplot(\n    data = nba %&gt;%\n        group_by(player_name, season_int) %&gt;%\n        summarize(pts = mean(pts), season_int=first(season_int)),\n    mapping = aes(x = pts)\n) +\n    geom_histogram(binwidth = 5, color = \"white\") +\n    facet_wrap( ~ season_int)\n\n`summarise()` has grouped output by 'player_name'. You can override using the\n`.groups` argument.\n\n\n\n\n\nFigure 7.12: Faceted histogram of points per game.\n\n\n\n\nWe can also specify the number of rows and columns in the grid by using the nrow and ncol arguments inside of facet_wrap(). For example, say we would like our faceted histogram to have 4 rows instead of 3. We simply add an nrow = 4 argument to facet_wrap(~ season_int).\n\nggplot(\n    data = nba %&gt;%\n        group_by(player_name, season_int) %&gt;%\n        summarize(pts = mean(pts), season_int=first(season_int)),\n    mapping = aes(x = pts)\n) +\n    geom_histogram(binwidth = 5, color = \"white\") +\n    facet_wrap( ~ season_int, nrow = 4)\n\n`summarise()` has grouped output by 'player_name'. You can override using the\n`.groups` argument.\n\n\n\n\n\nFigure 7.13: Faceted histogram of points per game."
  },
  {
    "objectID": "ggplot2.html#sec-boxplots",
    "href": "ggplot2.html#sec-boxplots",
    "title": "7  Visualizing Data",
    "section": "7.7 5NG#4: Boxplots",
    "text": "7.7 5NG#4: Boxplots\nThough faceted histograms are one type of visualization used to compare the distribution of a numerical variable split by the values of another variable, another type of visualization that achieves a similar goal is a side-by-side boxplot. A boxplot is constructed from the information provided in the five-number summary of a numerical variable.\nLet’s again consider the distribution of points. For now, let’s confine ourselves to the 1996 season to keep things simple.\n\nbase_plot &lt;- nba %&gt;%\n        filter(season_int %in% c(1996)) %&gt;%\n    ggplot(mapping = aes(x = factor(season_int), y=pts))\nbase_plot + geom_jitter(width = 0.1, height = 0, alpha = 0.3)\n\n\n\n\nFigure 7.14: Points from 1996 represented as jittered points.\n\n\n\n\nThese observations have the following five-number summary:\n\nMinimum: 0\nFirst quartile (25th percentile): 3 points\nMedian (second quartile, 50th percentile): 6 points\nThird quartile (75th percentile): 12 points\nMaximum: 29.6\n\nIn the leftmost plot of Figure 7.15, let’s mark these 5 values with dashed horizontal lines on top of the actual data points. In the middle plot of Figure 7.15 let’s add the boxplot. In the rightmost plot of Figure 7.15, let’s remove the points and the dashed horizontal lines for clarity’s sake.\n\n\n\n\n\nFigure 7.15: Building up a boxplot of points\n\n\n\n\nWhat the boxplot does is visually summarize the points by cutting them into quartiles at the dashed lines, where each quartile contains four equally-size groups of observations. Thus\n\n25% of points fall below the bottom edge of the box, which is the first quartile of 3 points. In other words, 25% of observations were below 3 points.\n25% of points fall between the bottom edge of the box and the solid middle line, which is the median of 6 points. Thus, 25% of observations were between 3 points and 6 points and 50% of observations were below 6 points.\n25% of points fall between the solid middle line and the top edge of the box, which is the third quartile of 12 points. It follows that 25% of observations were between 6 points and 12 points and 75% of observations were below 12 points.\n25% of points fall above the top edge of the box. In other words, 25% of observations were above 12 points.\nThe middle 50% of points lie within the interquartile range (IQR) between the first and third quartile. Thus, the IQR for this example is 3 - 12 = -9 point. The interquartile range is one measure of a numerical variable’s spread.\n\nFurthermore, in the rightmost plot of Figure 7.15, we see the whiskers of the boxplot. The whiskers stick out from either end of the box all the way to the minimum and maximum observations of 0 and 29.6 points, respectively. However, the whiskers don’t always extend to the smallest and largest observed values as they do here. They in fact extend no more than 1.5 \\(\\times\\) the interquartile range from either end of the box. In this case, we see a small number of observations that lie more than 1.5 \\(\\times\\) -9 points = -13.5 points the top of the box. These observations are are called outliers.\n\n7.7.1 Boxplots via geom_boxplot\nLet’s now create a side-by-side boxplot of players’ average points per game split by the different seasons as we did previously with the faceted histograms. We do this by mapping the season_int variable to the x-position aesthetic, the pts variable to the y-position aesthetic, and by adding a geom_boxplot() layer:\n\nggplot(data = nba, mapping = aes(x = season_int, y = pts)) +\n  geom_boxplot()\n\nWarning: Continuous x aesthetic\nℹ did you forget `aes(group = ...)`?\n\n\n\n\n\nFigure 7.16: Invalid boxplot specification.\n\n\n\n\nObserve in Figure 7.16 that this plot does not provide information about points separated by season. The warning message clues us in as to why. It is telling us that we have a “continuous”, or numerical variable, on the x-position aesthetic. Boxplots, however, require a categorical variable to be mapped to the x-position aesthetic.\nWe can convert the numerical variable season_int into a factor categorical variable by using the factor() function. So after applying factor(season_int), season_int goes from having numerical values just the 1995, 1996, etc. to having an associated ordering. With this ordering, ggplot() now knows how to work with this variable to produce the needed plot.\n\nggplot(data = nba, mapping = aes(x = factor(season_int), y = pts)) +\n  geom_boxplot()\n\n\n\n\nFigure 7.17: Side-by-side boxplot of average points per game split by season.\n\n\n\n\nThe resulting Figure 7.17 shows 26 separate “box and whiskers” plots similar to the rightmost plot of Figure 7.15 of only data from 1996. Thus the different boxplots are shown “side-by-side.”\n\nThe “box” portions of the visualization represent the 1st quartile, the median (the 2nd quartile), and the 3rd quartile.\nThe height of each box (the value of the 3rd quartile minus the value of the 1st quartile) is the interquartile range (IQR). It is a measure of the spread of the middle 50% of values, with longer boxes indicating more variability.\nThe “whisker” portions of these plots extend out from the bottoms and tops of the boxes and represent points less than the 25th percentile and greater than the 75th percentiles, respectively. They’re set to extend out no more than \\(1.5 \\times IQR\\) units away from either end of the boxes. We say “no more than” because the ends of the whiskers have to correspond to observed points per game averages The length of these whiskers show how the data outside the middle 50% of values vary, with longer whiskers indicating more variability.\nThe dots representing values falling outside the whiskers are called outliers. These can be thought of as potentially anomalous (“out-of-the-ordinary”) values.\n\nIt is important to keep in mind that the definition of an outlier is somewhat arbitrary and not absolute. In this case, they are defined by the length of the whiskers, which are no more than \\(1.5 \\times IQR\\) units long for each boxplot. Looking at this side-by-side plot we can easily compare scorring distributions across seasons by drawing imaginary horizontal lines across the plot. Furthermore, the heights of the boxes as quantified by the interquartile ranges are informative too; they tell us about variability, or spread, of the different players’ averages recorded in a given season.\n\n\n7.7.2 Summary\nSide-by-side boxplots provide us with a way to compare the distribution of a numerical variable across multiple values of another variable. One can see where the median falls across the different groups by comparing the solid lines in the center of the boxes.\nTo study the spread of a numerical variable within one of the boxes, look at both the length of the box and also how far the whiskers extend from either end of the box. Outliers are even more easily identified when looking at a boxplot than when looking at a histogram as they are marked with distinct points."
  },
  {
    "objectID": "ggplot2.html#sec-geombar",
    "href": "ggplot2.html#sec-geombar",
    "title": "7  Visualizing Data",
    "section": "7.8 5NG#5: Barplots",
    "text": "7.8 5NG#5: Barplots\nBoth histograms and boxplots are tools to visualize the distribution of numerical variables. Another commonly desired task is to visualize the distribution of a categorical variable. This is a simpler task, as we are simply counting different categories within a categorical variable, also known as the levels of the categorical variable. Often the best way to visualize these different counts, also known as frequencies, is with barplots (also called barcharts).\nOne complication, however, is how your data is represented. Is the categorical variable of interest “pre-counted” or not? For example, run the following code that manually creates two data frames representing a collection of fruit: 3 apples and 2 oranges.\n\nfruits &lt;- tibble(\n  fruit = c(\"apple\", \"apple\", \"orange\", \"apple\", \"orange\")\n)\nfruits_counted &lt;- tibble(\n  fruit = c(\"apple\", \"orange\"),\n  number = c(3, 2)\n)\n\nWe see both the fruits and fruits_counted data frames represent the same collection of fruit. Whereas fruits just lists the fruit individually…\n\n\n# A tibble: 5 × 1\n  fruit \n  &lt;chr&gt; \n1 apple \n2 apple \n3 orange\n4 apple \n5 orange\n\n\n… fruits_counted has a variable count which represent the “pre-counted” values of each fruit.\n\n\n# A tibble: 2 × 2\n  fruit  number\n  &lt;chr&gt;   &lt;dbl&gt;\n1 apple       3\n2 orange      2\n\n\nDepending on how your categorical data is represented, you’ll need to add a different geometric layer type to your ggplot() to create a barplot, as we now explore.\n\n7.8.1 Barplots via geom_bar or geom_col\nLet’s generate barplots using these two different representations of the same basket of fruit: 3 apples and 2 oranges. Using the fruits data frame where all 5 fruits are listed individually in 5 rows, we map the fruit variable to the x-position aesthetic and add a geom_bar() layer:\n\nggplot(data = fruits, mapping = aes(x = fruit)) +\n  geom_bar()\n\n\n\n\nFigure 7.18: Barplot when counts are not pre-counted.\n\n\n\n\nHowever, using the fruits_counted data frame where the fruits have been “pre-counted”, we once again map the fruit variable to the x-position aesthetic, but here we also map the count variable to the y-position aesthetic, and add a geom_col() layer instead.\n\nggplot(data = fruits_counted, mapping = aes(x = fruit, y = number)) +\n  geom_col()\n\n\n\n\nFigure 7.19: Barplot when counts are pre-counted.\n\n\n\n\nCompare the barplots in Figures Figure 7.18 and Figure 7.19. They are identical because they reflect counts of the same five fruits. However, depending on how our categorical data is represented, either “pre-counted” or not, we must add a different geom layer. When the categorical variable whose distribution you want to visualize\n\nIs not pre-counted in your data frame, we use geom_bar().\nIs pre-counted in your data frame, we use geom_col() with the y-position aesthetic mapped to the variable that has the counts.\n\nLet’s now go back to the nba data frame and visualize the distribution of the categorical variable college. Specifically, we’ll visualize the number of players who graduated from different colleges. We’ll focus on the New York Knicks (team_abbreviation==NYK) and data from the 2006-2009 seasons.\nRecall from Chapter 6, you saw that each row in the nba data set corresponds to a player in a given year In other words, the nba data frame is more like the fruits data frame than the fruits_counted data frame because the numbers of players from each college have not been pre-counted. Thus we should use geom_bar() instead of geom_col() to create a barplot. Much like a geom_histogram(), there is only one variable in the aes() aesthetic mapping: the variable college gets mapped to the x-position. As a difference though, histograms typically have bars that touch whereas bar graphs typically have space between the bars.\n\nnba %&gt;%\n    group_by(player_name, college) %&gt;%\n    filter(team_abbreviation %in% c(\"NYK\"),\n           season_int &lt; 2010,\n       season_int &gt; 2005) %&gt;%\n    ggplot(mapping = aes(x = college)) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90))\n\n\n\n\nFigure 7.20: Number of players by colege using geom_bar().\n\n\n\n\nObserve in Figure 7.20 that there are many Knicks players who either did not attend any college (“None”), attended Arizona State, or attended Florida. If you don’t know which airlines correspond to which carrier codes, then run View(airlines) to see a directory of airlines. For example, B6 is JetBlue Airways. Alternatively, say you had a data frame where the number of flights for each carrier was pre-counted as in Table @ref(tab:flights-counted).\n\n\n\n\n\nNumber of players, pre-counted for each college\n\n\ncollege\nnumber\n\n\n\n\nArizona\n1\n\n\nArizona State\n1\n\n\nDePaul\n6\n\n\nDrexel\n2\n\n\nDuke\n2\n\n\nFlorida\n4\n\n\nFlorida A&M\n3\n\n\nFlorida State\n1\n\n\nGeorgia Tech\n2\n\n\nIndiana\n3\n\n\nIowa State\n1\n\n\nKansas State\n1\n\n\nKentucky\n3\n\n\nMaryland\n2\n\n\nMemphis\n1\n\n\nMichigan\n3\n\n\nMichigan State\n1\n\n\nNew Mexico\n1\n\n\nNone\n13\n\n\nOregon\n1\n\n\nSaint Louis\n1\n\n\nSouth Carolina\n2\n\n\nSyracuse\n1\n\n\nTemple\n2\n\n\nWashington\n3\n\n\n\n\n\n\nFigure 7.21: ?(caption)\n\n\n\nIn order to create a barplot visualizing the distribution of the categorical variable college in this case, we would now use geom_col() instead of geom_bar(), with an additional y = number in the aesthetic mapping on top of the x = carrier. The resulting barplot would be identical to Figure 7.20.\n\n\n7.8.2 Must avoid pie charts!\nOne of the most common plots used to visualize the distribution of categorical data is the pie chart. Though they may seem harmless, pie charts actually present a problem in that humans are unable to judge angles well. As Naomi Robbins describes in her book, Creating More Effective Graphs (robbins2013?), people tend to overestimate angles greater than 90 degrees and underestimate angles less than 90 degrees. In other words, it is difficult to determine the relative size of one piece of the pie compared to another. So stay away!\n\n\n7.8.3 Two categorical variables\nBarplots are a very common way to visualize the frequency of different categories, or levels, of a single categorical variable. Another use of barplots is to visualize the joint distribution of two categorical variables at the same time. Let’s examine the joint distribution of players by college as well as season. In other words, the number of players for each combination of college and season.\nFor example, the number of WestJet flights from JFK, the number of WestJet flights from LGA, the number of WestJet flights from EWR, the number of American Airlines flights from JFK, and so on. Recall the ggplot() code that created the barplot of carrier frequency in Figure @ref(fig:flightsbar):\nWe can now map the additional variable origin by adding a fill = origin inside the aes() aesthetic mapping.\n\nnba %&gt;%\n    group_by(player_name, college) %&gt;%\n    filter(team_abbreviation %in% c(\"NYK\"),\n           season_int &lt; 2010,\n           season_int &gt; 2005) %&gt;%\n    ggplot(mapping = aes(x = college, fill = factor(season_int))) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90))\n\n\n\n\nFigure 7.22: ?(caption)\n\n\n\n\nFigure 7.22 is an example of a stacked barplot. Though simple to make, in certain aspects it is not ideal. For example, it is not particularly easy to compare the heights of the different colors between the bars, corresponding to comparing the number of players from each season_int between the different teams.\nBefore we continue, let’s address some common points of confusion among new R users. First, the fill aesthetic corresponds to the color used to fill the bars, whereas the color aesthetic corresponds to the color of the outline of the bars. This is identical to how we added color to our histogram in Section 7.5.1: we set the outline of the bars to white by setting color = \"white\" and the colors of the bars to blue steel by setting fill = \"steelblue\". Observe in Figure 7.23 that mapping season_int to color and not fill yields grey bars with different colored outlines.\n\nnba %&gt;%\n    group_by(player_name, college) %&gt;%\n    filter(team_abbreviation %in% c(\"NYK\"),\n           season_int &lt; 2010,\n           season_int &gt; 2005) %&gt;%\n    ggplot(mapping = aes(x = college, color = factor(season_int))) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90))\n\n\n\n\nFigure 7.23: ?(caption)\n\n\n\n\nSecond, note that fill is another aesthetic mapping much like x-position; thus we were careful to include it within the parentheses of the aes() mapping. The following code, where the fill aesthetic is specified outside the aes() mapping will yield an error. This is a fairly common error that new ggplot users make:\n\n...\n    ggplot(mapping = aes(x = college), color = factor(season_int)) +\n    geom_bar() +\n\nAn alternative to stacked barplots are side-by-side barplots, also known as dodged barplots, as seen in Figure 7.24. The code to create a side-by-side barplot is identical to the code to create a stacked barplot, but with a position = \"dodge\" argument added to geom_bar(). In other words, we are overriding the default barplot type, which is a stacked barplot, and specifying it to be a side-by-side barplot instead.\n\nnba %&gt;%\n    group_by(player_name, college) %&gt;%\n    filter(team_abbreviation %in% c(\"NYK\"),\n           season_int &lt; 2010,\n           season_int &gt; 2005) %&gt;%\n    ggplot(mapping = aes(x = college, fill = factor(season_int))) +\n    geom_bar(position = \"dodge\") +\n    theme(axis.text.x = element_text(angle = 90))\n\n\n\n\nFigure 7.24: ?(caption)\n\n\n\n\nHere, the width of the bars for DuPaul, Florida, and None is different than the width of the bars for Arizona and Iowa State. We can make one tweak to the position argument to get them to be the same size in terms of width as the other bars by using the more robust position_dodge() function.\n\nnba %&gt;%\n    group_by(player_name, college) %&gt;%\n    filter(team_abbreviation %in% c(\"NYK\"),\n           season_int &lt; 2010,\n           season_int &gt; 2005) %&gt;%\n    ggplot(mapping = aes(x = college, fill = factor(season_int))) +\n    geom_bar(position = position_dodge(preserve = \"single\")) +\n    theme(axis.text.x = element_text(angle = 90))\n\n\n\n\nLastly, another type of barplot is a faceted barplot. Recall in Section 7.6 we visualized the distribution of players’ points split by season using facets. We apply the same principle to our barplot visualizing the frequency of college split by season_int: instead of mapping college to fill we include it as the variable to create small multiples of the plot across the levels of college.\n\nggplot(data = flights, mapping = aes(x = carrier)) +\n  geom_bar() +\n  facet_wrap(~ origin, ncol = 1)\n\n\nnba %&gt;%\n    group_by(player_name, college) %&gt;%\n    filter(team_abbreviation %in% c(\"NYK\"),\n           season_int &lt; 2010,\n           season_int &gt; 2005) %&gt;%\n    ggplot(mapping = aes(x = college)) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90)) +\n    facet_wrap(~season_int, ncol = 1)\n\n\n\n\nFigure 7.25: Faceted barplot comparing the number of flights by carrier and origin.\n\n\n\n\n\n\n7.8.4 Summary\nBarplots are a common way of displaying the distribution of a categorical variable, or in other words the frequency with which the different categories (also called levels) occur. They are easy to understand and make it easy to make comparisons across levels. Furthermore, when trying to visualize the relationship of two categorical variables, you have many options: stacked barplots, side-by-side barplots, and faceted barplots. Depending on what aspect of the relationship you are trying to emphasize, you will need to make a choice between these three types of barplots and own that choice."
  },
  {
    "objectID": "ggplot2.html#sec-data-vis-conclusion",
    "href": "ggplot2.html#sec-data-vis-conclusion",
    "title": "7  Visualizing Data",
    "section": "7.9 Conclusion",
    "text": "7.9 Conclusion\n\n7.9.1 Summary table\nLet’s recap all five of the five named graphs (5NG) in Table 7.1 summarizing their differences. Using these 5NG, you’ll be able to visualize the distributions and relationships of variables contained in a wide array of datasets. This will be even more the case as we start to map more variables to more of each geometric object’s aesthetic attribute options, further unlocking the awesome power of the ggplot2 package.\n\n\nTable 7.1: My Caption\n\n\n\n\n\n\n\n\nNamed graph\nShows\nGeometric object\nNotes\n\n\n\n\nScatterplot\nRelationship between 2 numerical variables\ngeom_point()\n\n\n\nLinegraph\nRelationship between 2 numerical variables\ngeom_line()\nUsed when there is a sequential order to x-variable, e.g., time\n\n\nHistogram\nDistribution of 1 numerical variable\ngeom_histogram()\nFacetted histograms show the distribution of 1 numerical variable split by the values of another variable\n\n\nBoxplot\nDistribution of 1 numerical variable split by the values of another variable\ngeom_boxplot()\nC\n\n\nBarplot\nDistribution of 1 categorical variable\ngeom_bar() when counts are not pre-counted, geom_col() when counts are pre-counted `\nStacked, side-by-side, and faceted barplots show the joint distribution of 2 categorical variables\n\n\n\n\n\n\n7.9.2 Function argument specification\nLet’s go over some important points about specifying the arguments (i.e., inputs) to functions. Run the following two segments of code:\n\n# Segment 1:\nggplot(data = nba, mapping = aes(x = team_abbreviation)) +\n  geom_bar()\n\n# Segment 2:\nggplot(flights, aes(x = team_abbreviation)) +\n  geom_bar()\n\nYou’ll notice that both code segments create the same barplot, even though in the second segment we omitted the data = and mapping = code argument names. This is because the ggplot() function by default assumes that the data argument comes first and the mapping argument comes second. As long as you specify the data frame in question first and the aes() mapping second, you can omit the explicit statement of the argument names data = and mapping =. That being said, explicit is better than implicit. Given the uniformity of the tidyverse packages, you will often see the data= argument name omitted (because the first argument of all tidyverse function is a tibble), but it is good practice to include the names of other arguments for readability and clarity purposes.\n\n\n\n\nWilkinson, Leland. 2012. The Grammar of Graphics. Springer."
  },
  {
    "objectID": "single.html",
    "href": "single.html",
    "title": "9  Simple",
    "section": "",
    "text": "What is regression"
  },
  {
    "objectID": "multiple.html",
    "href": "multiple.html",
    "title": "10  Multiple",
    "section": "",
    "text": "What is multiple regression"
  },
  {
    "objectID": "anova.html",
    "href": "anova.html",
    "title": "11  ANOVA",
    "section": "",
    "text": "What is ANOVA\nWhat is ANCOVA"
  },
  {
    "objectID": "simulation.html#simulate",
    "href": "simulation.html#simulate",
    "title": "12  Simulating Data",
    "section": "12.1 Simulate?",
    "text": "12.1 Simulate?"
  },
  {
    "objectID": "missingness.html#what-happened-to-my-data",
    "href": "missingness.html#what-happened-to-my-data",
    "title": "13  Missing Data",
    "section": "13.1 What happened to my data?",
    "text": "13.1 What happened to my data?"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Wilkinson, Leland. 2012. The Grammar of Graphics. Springer."
  }
]