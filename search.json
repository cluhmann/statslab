[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "statslab",
    "section": "",
    "text": "Introduction\nContent covering statistics, data analysis, and computer programming. Sections are listed on the left.\nInstructor\nChristian Luhmann\nTeaching Assistant\nEmily Bibby"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "statslab",
    "section": "Schedule",
    "text": "Schedule\nSubject to change, particularly topics scheduled for later in the semester.\n\n\n\n\n\n\n\n\n\nWeek\nDate\nLecture\nActivity\n\n\n\n\n1\n09-01\nR & Rstudio, Basic Programming\n\n\n\n2\n09-08\nAdv. Programming and Handling Data in R\n\n\n\n3\n09-15\nSampling\n\n\n\n4\n09-22\nChristian Out of Town\nSampling: p-values ala Lakens\n\n\n5\n09-29\nThe Tidyverse, Tibbles, and dplyr\n\n\n\n6\n10-06\nPlotting\n\n\n\n7\n10-13\nDescriptive Statistics\n\n\n\n8\n10-20\n\nSimulation: PRE sampling distribution\n\n\n9\n10-27\n\nSimulation: Power\n\n\n10\n11-03\nRegression\n\n\n\n11\n11-10\n\nVisualizing Residuals\n\n\n12\n11-17\nMultiple Regression\n\n\n\n13\n11-24\nNo Class – Thanksgiving Break\n\n\n\n14\n12-01\nRegression w/ Categorical Predictors\n\n\n\n15\n12-08\nRegression w/ Interactions"
  },
  {
    "objectID": "rlang.html#r-the-language",
    "href": "rlang.html#r-the-language",
    "title": "1  R & RStudio",
    "section": "1.1 R: The Language",
    "text": "1.1 R: The Language\nReleased in 1993, R is an opensource successor to S, a commercially available statistical programming language developed at Bell Labs in the 1970s. The history is not particularly relevant. However, understanding that R was originally developed with the explicit goal of teaching introductory statistics (a bit like Matlab) may help to understand some of the design choices made by the developers of R.\nThere are several attributes that make R useful for teaching statistics:\n\nR is interpreted\nR is weakly typed\nR’s the focus on data analysis is built into its native data types\nR has strong graphical abilities\n\nDon’t worry if all of these benefits are meaningless to you at this point. We’ll get into the details soon enough. The short version is that the first two bullet points are features that make programmers’ lives easier/more convenient and last two mean that R is ready-made to do data analysis.\nThere are also a variety of downsides to R, but we will try to sidestep these as much as we can. One way we will do this is by exclusively using packages from the tidyverse. Once you are familiar with one way of working in R, you can explore the many, many alternatives."
  },
  {
    "objectID": "rlang.html#rstudio",
    "href": "rlang.html#rstudio",
    "title": "1  R & RStudio",
    "section": "1.2 RStudio",
    "text": "1.2 RStudio\nThe interface between a programmer (or data analyst) and a programming language is often an integrate development environment (IDE), which is just a fancy term for a piece of software that collects a bunch of inter-related tools often useful when programming. We will be using RStudio as our IDE. As the name suggests, RStudio was designed as an R-first IDE, making things easy for those new to R and/or programming.\nFirst time users often confuse R and RStudio. At its simplest, R is like a car’s engine while RStudio is like a car’s dashboard as illustrated in Figure Figure 1.1. More precisely, R is the programming language that performs computations and RStudio is how users interact with R.\n\n\n\nFigure 1.1: Analogy of difference between R and RStudio."
  },
  {
    "objectID": "rlang.html#sec-install",
    "href": "rlang.html#sec-install",
    "title": "1  R & RStudio",
    "section": "1.3 Installation",
    "text": "1.3 Installation\nInstallation involves two major steps. The first step is to install R itself. You can get the most recent version from CRAN (the comprehensive R archive network): https://cloud.r-project.org. There are ready-made installers available for Linux, macOS, and Windows.\nThe second step is to install RStudio. You can download it from Posit: https://posit.co/download/rstudio-desktop/. Again, there are versions available for all major platforms."
  },
  {
    "objectID": "rlang.html#using-r-via-rstudio",
    "href": "rlang.html#using-r-via-rstudio",
    "title": "1  R & RStudio",
    "section": "1.4 Using R via RStudio",
    "text": "1.4 Using R via RStudio\nRecall our car analogy from earlier. Much as we don’t drive a car by interacting directly with the engine but rather by interacting with elements on the car’s dashboard, we won’t be using R directly but rather we will use RStudio’s interface. After you install R and RStudio on your computer, you’ll have two new programs or applications you can open. We’ll always work in RStudio and not in the R application. Figure Figure 1.2 shows what icon you should be clicking on your computer.\n\n\n\nFigure 1.2: Icons of R versus RStudio on your computer.\n\n\nAfter you open RStudio, you should see something similar to Figure Figure 1.3. (Note that slight differences might exist if the RStudio interface is updated after 2019 to not be this by default.)\n\n\n\nFigure 1.3: RStudio interface to R.\n\n\nNote the three panes dividing the screen: the console pane, the files pane, and the environment pane. The console pane is where you enter R commands and R returns the corresponding results (and/or error messages if you make a mistake). We’ll dig into the other panes a bit more later."
  },
  {
    "objectID": "rlang.html#sec-packages",
    "href": "rlang.html#sec-packages",
    "title": "1  R & RStudio",
    "section": "1.5 Other Packages",
    "text": "1.5 Other Packages\nOne of the main strengths of R (arguably the most important strength) is the availability of many, many packages (or libraries) that extend the functionality of R in all sort of different ways. These libraries are written by a worldwide community of R users and can be downloaded for free. We will rely on a variety of these packages, so we take a moment now to discuss how packages work in R and RStudio.\nA good analogy for R packages is they are like apps you can download onto your mobile phone. So R is like a new mobile phone: while it has a certain amount of features when you use it for the first time, it doesn’t have everything. R packages are like the apps you can download onto your phone from Apple’s App Store or Android’s Google Play.\nLet’s continue this analogy by considering the Instagram app for editing and sharing pictures. Say you have purchased a new phone and you would like to share a photo you have just taken with friends on Instagram. You need to:\n\nInstall the app: Since your phone is new and does not include the Instagram app, you need to download the app from either the App Store or Google Play. You do this once and you’re set for the time being. You might need to do this again in the future when there is an update to the app.\nOpen the app: After you’ve installed Instagram, you need to open it.\n\nOnce Instagram is open on your phone, you can then proceed to share your photo with your friends and family. The process is very similar for using an R package. You need to:\n\nInstall the package: This is like installing an app on your phone. Most packages are not installed by default when you install R and RStudio. Thus, if you want to use a package, you need to install it first. Once you’ve installed a package, you likely won’t install it again unless you want to update it to a newer version.\n“Load” the package: “Loading” a package is like opening an app on your phone. Packages are not “loaded” by default when you start RStudio on your computer; you need to “load” each package you want to use every time you start RStudio.\n\nLet’s take a look at how these two steps come together to install and load the ggplot2 package for data visualization.\n\n1.5.1 Package Installation\nThere are two ways to install an R package: an easy way and a more advanced way. Let’s install the ggplot2 package the easy way first as shown in Figure Figure 1.4. In the Files pane of RStudio:\n\nClick on the “Packages” tab.\nClick on “Install” next to Update.\nType the name of the package under “Packages (separate multiple with space or comma):” In this case, type ggplot2.\nClick “Install.”\n\n\n\n\nFigure 1.4: Installing packages in R the easy way.\n\n\nAn alternative but slightly less convenient way to install a package is by typing install.packages(\"ggplot2\") in the console pane of RStudio and pressing Return/Enter on your keyboard. Note you must include the quotation marks around the name of the package.\nMuch like an app on your phone, you only have to install a package once. However, if you want to update a previously installed package to a newer version, you need to reinstall it by repeating the earlier steps.\n\n\n1.5.2 Package Loading\nRecall that after you’ve installed a package, you need to “load it.” In other words, you need to “open it.” We do this by using the library() command.\nFor example, to load the ggplot2 package, run the following code in the console pane. What do we mean by “run the following code”? Either type or copy-and-paste the following code into the console pane and then hit the Enter key.\n\nlibrary(ggplot2)\n\nIf after running the earlier code, a blinking cursor returns next to the &gt; “prompt” sign, it means you were successful and the ggplot2 package is now loaded and ready to use. If, however, you get a red “error message” like this:\nError in library(ggplot2) : there is no package called ‘ggplot2’\nit means that you didn’t successfully install it. If you get this error message, go back to Subsection Section 1.5.1 on R package installation and make sure to install the ggplot2 package before proceeding.\n\n\n1.5.3 Package Use\nOne very common mistake new R users make when wanting to use particular packages is they forget to “load” them first by using the library() command we just saw. Remember: you have to load each package you want to use every time you start RStudio. If you don’t first “load” a package, but attempt to use one of its features, you’ll see an error message similar to:\nError: could not find function\nThis is a different error message than the one you just saw on a package not having been installed yet. R is telling you that you are trying to use a function in a package that has not yet been “loaded.” R doesn’t know where to find the function you are using. Almost all new users forget to do this when starting out, and it is a little annoying to get used to doing it. However, you’ll remember with practice and after some time it will become second nature for you."
  },
  {
    "objectID": "rlang.html#quitting-r",
    "href": "rlang.html#quitting-r",
    "title": "1  R & RStudio",
    "section": "1.6 Quitting R",
    "text": "1.6 Quitting R\n\n\n\nFigure 1.5: The dialog box that shows up when you try to close RStudio.\n\n\nThere’s one last thing I should cover in this chapter: how to quit R. You can just shut down the application in the normal way (e.g., File-&gt;Quit Session). However, R also has a function, q(), that you can use to quit, which can be handy. Regardless of what method you use to quit R, when you do so for the first time R will probably ask you if you want to save the “workspace image”. We’ll talk a lot more about loading and saving data in Chapter 4, but I figured we’d better quickly cover this now otherwise you’re going to get annoyed when you close R at the end of the chapter. If you’re using RStudio, you’ll see a dialog box that looks like the one shown in Figure @ref(fig:quitR). If you’re using a text based interface you’ll see this:\n\nq()\n\n## Save workspace image? [y/n/c]: \n\nThe y/n/c part here is short for “yes / no / cancel”. Type y if you want to save, n if you don’t, and c if you’ve changed your mind and you don’t want to quit after all.\nWhat does this actually mean? What’s going on is that R wants to know if you want to save all those variables that you’ve been creating, so that you can use them later. This sounds like a great idea, so it’s really tempting to type y or click the “Save” button. To be honest though, I very rarely do this, and it kind of annoys me a little bit.. what R is really asking is if you want it to store these variables in a “default” data file, which it will automatically reload for you next time you open R. And quite frankly, if I’d wanted to save the variables, then I’d have already saved them before trying to quit. Not only that, I’d have saved them to a location of my choice, so that I can find it again later. So I personally never bother with this.\nIn fact, every time I install R on a new machine one of the first things I do is change the settings so that it never asks me again. You can do this in RStudio really easily: use the menu system to find the RStudio option; the dialog box that comes up will give you an option to tell R never to whine about this again (see Figure 1.5). On a Mac, you can open this window by going to the “RStudio” menu and selecting “Preferences”. On a Windows machine you go to the “Tools” menu and select “Global Options”. Under the “General” tab you’ll see an option that reads “Save workspace to .Rdata on exit”. By default this is set to “ask”. If you want R to stop asking, change it to “never”."
  },
  {
    "objectID": "rlang.html#sec-rlangactivities",
    "href": "rlang.html#sec-rlangactivities",
    "title": "1  R & RStudio",
    "section": "1.7 Activities",
    "text": "1.7 Activities\n\nInstall both R and RStudio using the instructions found in Section 1.3\nOpen RStudio\nSettings\n\nGo to “Tools” menu and select “Global Options…”. On the “General” tab, uncheck the box that says “Restore .RData into workspace at startup”.\n\nSet the “Save workspace to .Rdata on exit” option to “never”.\nInstall the tidyverse package using the instructions found in Section 1.5.1\nVerify installation of the tidyverse package by loading the package using the instructions found in Section 1.5.2\nCreate a directory/folder somewhere convenient on your hard drive and name it “statslab”\nSet the “working directory” for R by going to “Session” menu and selecting “Set Working Directory” and then “Choose Directory…”. Select the “statslab” directory/folder you just created.\nAny time you wish to begin (or return to) work for this course, perform this step so that all of your file (data, scripts, etc.) are saved in a central location. Future activities will provide further instruction assuming that you a) have created this directory/folder and b) have set it to be your working directory in RStudio."
  },
  {
    "objectID": "programming.html#commands-at-the-console",
    "href": "programming.html#commands-at-the-console",
    "title": "2  R as a Calculator",
    "section": "2.1 Commands at the console",
    "text": "2.1 Commands at the console\nOne of the easiest things you can do with R is use it as a simple calculator, so it’s a good place to start. For instance, try typing 10 + 20, and hitting enter. The simple act of typing it rather than “just reading” makes a big difference. It makes the concepts more concrete, and it ties the abstract ideas (programming and statistics) to the actual context in which you need to use them. Statistics is something you do, not just something you read about in a textbook.] When you do this, you’ve entered a command, and R will “execute” that command. What you see on screen now will be this:\n&gt; 10 + 20\n[1] 30\nShould be much surprise here. But there’s a few things worth talking about, even with such a simple example. First, it’s important that you understand how to read the code example. In this example, what was typed into the RStudio console was the 10 + 20 part. The &gt; symbol as not typed. That’s just the command prompt and isn’t part of the actual command. The [1] 30 part was also not typed into the console. That’s what R printed out in response to the 10 + 20 code.\nSecond, it’s important to understand how the output is formatted. Obviously, the correct answer to the sum 10 + 20 is 30, and not surprisingly R has printed that out as part of its output. But it’s also printed out this [1] part, which probably doesn’t make a lot of sense to you right now. You’re going to see that a lot. I’ll talk about what this means in a bit more detail later on, but for now you can think of [1] 30 as if R were saying “the answer to the 1st question you asked is 30”. That’s not quite accurate, but it’s close enough for now. And in any case it’s not really very interesting at the moment: we only asked R to calculate one thing, so obviously there’s only one answer. Later on this will change, and the [1] part will start to make a bit more sense. For now, I just don’t want you to get confused or concerned by it.\n\n2.1.1 An important digression about formatting\nNow that I’ve taught you these rules I’m going to change them pretty much immediately. That is because I want you to be able to copy code from the book directly into R if if you want to test things or conduct your own analyses. However, if you copy this kind of code (that shows the command prompt and the results) directly into R you will get an error:\n\n&gt; 10 + 20\n\nError: &lt;text&gt;:1:1: unexpected '&gt;'\n1: &gt;\n    ^\n\n\nSo instead, I’m going to provide code in a slightly different format so that it looks like this…\n\n10 + 20\n\n[1] 30\n\n\nThere are two main differences.\n\nIn your console, the “&gt;” is the prompt and you type your code after (to the right of) this prompt.\nWe’ll often show the output of a bit of code, but the output will be displayed after the block of code itself.\n\nFor your purposes, this also means that you can easily copy code from any of these code blocks and paste it into your RStudio console in order to execute.\n\n\n2.1.2 Be very careful to avoid typos\nBefore we go on to talk about other types of calculations that we can do with R, there’s a few other things I want to point out. The first thing is that, though R is good software, it’s still software. R, like any programming language, is pretty stupid and because it’s stupid it can’t handle typos. It takes it on faith that you meant to type exactly what you actually typed. For example, suppose that you forgot to hit the shift key when trying to type +, and as a result your command ended up being 10 = 20 rather than 10 + 20. Here’s what happens:\n\n10 = 20\n\nError in 10 = 20: invalid (do_set) left-hand side to assignment\n\n\nWhat’s happened here is that R has attempted to interpret 10 = 20 as a command, and spits out an error message because the command doesn’t make any sense to it. When a human looks at this, and then looks down at his or her keyboard and sees that + and = are on the same key, it’s pretty obvious that the command was a typo. But R doesn’t know this, so it gets upset. And, if you look at it from its perspective, this makes sense. All that R “knows” is that 10 is a legitimate number, 20 is a legitimate number, and = is a legitimate part of the language too. In other words, from its perspective this really does look like the user meant to type 10 = 20, since all the individual parts of that statement are legitimate and it’s too stupid to realize that this is probably a typo. Therefore, R takes it on faith that this is exactly what you meant… it only “discovers” that the command is nonsense when it tries to follow your instructions, typo and all. And then it complains by spitting out an error.\nEven more subtle is the fact that some typos won’t produce errors at all, because they happen to correspond to “well-formed” R commands. For instance, suppose that not only did I forget to hit the shift key when trying to type 10 + 20, I also managed to press the key next to one I meant do. The resulting typo would produce the command 10 - 20. Clearly, R has no way of knowing that you meant to add 20 to 10, not subtract 20 from 10, so what happens this time is this:\n\n10 - 20\n\n[1] -10\n\n\nIn this case, R produces the right answer, but to the the wrong question.\nTo some extent, I’m stating the obvious here, but it’s important. The people who wrote R are smart. You, the user, are smart. But R is a programming language and programming languages are a way to tell computers what to do and computers are dumb. And because they are dumb, they are mindlessly obedient. R does exactly what you tell it to do. R will not try and second-guess what you “actually meant”; there is no “autocorrect”. This is for good reason. When doing advanced stuff – and even the simplest of statistics is pretty advanced in a lot of ways – it’s risky to let a mindless automaton like R try to overrule the human user. So it’s your responsibility to be careful. Always make sure you type exactly what you mean. When dealing with computers, it’s not enough to type “approximately” the right thing. In general, you absolutely must be precise in what you tell R to do … like all machines it is too stupid to be anything other than absurdly literal in its interpretation.\n\n\n2.1.3 R is (a bit) flexible with spacing\nOf course, now that I’ve been so uptight about the importance of always being precise, I should point out that there are some exceptions. Or, more accurately, there are some situations in which R does show a bit more flexibility than my previous description suggests. The first thing R is smart enough to do is ignore redundant spacing. What I mean by this is that, when I typed 10 + 20 before, I could equally have done this\n\n10    + 20\n\n[1] 30\n\n\nor this\n\n10+20\n\n[1] 30"
  },
  {
    "objectID": "programming.html#simple-calculations",
    "href": "programming.html#simple-calculations",
    "title": "2  R as a Calculator",
    "section": "2.2 Simple calculations",
    "text": "2.2 Simple calculations\nOkay, now that we’ve discussed some of the tedious details associated with typing R commands, let’s get back to learning how to use the most powerful piece of statistical software in the world as a $2 calculator. So far, all we know how to do is addition. Clearly, a calculator that only did addition would be a bit stupid, so we’ll discuss other simple calculations you can perform using R. But first, some more terminology. Addition is an example of an “operation” that you can perform (specifically, an arithmetic operation), and the operator that performs it is +. To people with a programming or mathematics background, this terminology probably feels pretty natural, but to other people it might feel like I’m trying to make something very simple (addition) sound more complicated than it is (by calling it an arithmetic operation). To some extent, that’s true: if addition was the only operation that we were interested in, it’d be a bit silly to introduce all this extra terminology. However, as we go along, we’ll start using more and more different kinds of operations, so it’s probably a good idea to get the language straight now, while we’re still talking about very familiar concepts like addition!\n\n2.2.1 Adding, subtracting, multiplying and dividing\nSo, now that we have the terminology, let’s learn how to perform some arithmetic operations in R. To that end, Figure 2.1 lists the operators that correspond to the basic arithmetic we learned in primary school: addition, subtraction, multiplication and division.\n\n\n\n\n\n\noperation\noperator\nexample input\nexample output\n\n\n\n\naddition\n+\n10 + 2\n12\n\n\nsubtraction\n-\n9 - 3\n6\n\n\nmultiplication\n*\n5 * 5\n25\n\n\ndivision\n/\n10 / 3\n3.333333\n\n\npower\n^\n5 ^ 2\n25\n\n\n\nFigure 2.1: Basic arithmetic operations in R. These five operators are used very frequently throughout the text, so it’s important to be familiar with them at the outset.\n\n\n\nAs you can see, R uses fairly standard symbols to denote each of the different operations you might want to perform: addition is done using the + operator, subtraction is performed by the - operator, and so on. So if I wanted to find out what 57 times 61 is (and who wouldn’t?), I can use R instead of a calculator, like so:\n\n57 * 61\n\n[1] 3477\n\n\nSo that’s handy.\n\n\n2.2.2 Taking powers\nThe first four operations listed in Figure 2.1 are things we all learned at a young age, but they aren’t the only arithmetic operations built into R. There are three other arithmetic operations that I should probably mention: taking powers, doing integer division, and calculating a modulus. Of the three, the most important is probably taking powers.\nFor those of you who can still remember your high school math, this should be familiar. And if not, it’s not complicated. As I’m sure everyone will probably remember the moment they read this, the act of multiplying a number \\(x\\) by itself \\(n\\) times is called “raising \\(x\\) to the \\(n\\)-th power”. Mathematically, this is written as \\(x^n\\). Some values of \\(n\\) have special names: in particular \\(x^2\\) is called \\(x\\)-squared, and \\(x^3\\) is called \\(x\\)-cubed. So, the 4th power of 5 is calculated like this:\n\\[\n5^4 = 5 \\times 5 \\times 5 \\times 5\n\\] One way that we could calculate \\(5^4\\) in R would be to type in the complete multiplication as it is shown in the equation above. That is, we could do this\n\n5 * 5 * 5 * 5\n\n[1] 625\n\n\nbut it does seem a bit tedious. It would be very annoying indeed if you wanted to calculate \\(5^{15}\\), since the command would end up being quite long. Therefore, to make our lives easier, we use the power operator instead. When we do that, our command to calculate \\(5^4\\) goes like this:\n\n5 ^ 4\n\n[1] 625\n\n\nMuch easier.\n\n\n2.2.3 Doing calculations in the right order\nOkay. At this point, you know how to take one of the most powerful pieces of statistical software in the world, and use it as a $2 calculator. And as a bonus, you’ve learned a few very basic programming concepts. That’s not nothing (you could argue that you’ve just saved yourself $2) but on the other hand, it’s not very much either. In order to use R more effectively, we need to introduce more programming concepts.\nIn most situations where you would want to use a calculator, you might want to do multiple calculations. R lets you do this, just by typing in longer commands. In fact, we’ve already seen an example of this earlier, when I typed in 5 * 5 * 5 * 5. However, let’s try a slightly different example:\n\n1 + 2 * 4\n\n[1] 9\n\n\nClearly, this isn’t a problem for R either. However, it’s worth stopping for a second, and thinking about what R just did. Clearly, since it gave us an answer of 9 it must have multiplied 2 * 4 (to get an interim answer of 8) and then added 1 to that. But, suppose it had decided to just go from left to right: if R had decided instead to add 1+2 (to get an interim answer of 3) and then multiplied by 4, it would have come up with an answer of 12.\nTo answer this, you need to know the order of operations that R uses. If you remember back to your high school maths classes, it’s actually the same order that you got taught when you were at school: the “BEDMAS” order. That is, first calculate things inside Brackets (), then calculate Exponents ^, then Division / and Multiplication *, then Addition + and Subtraction -. So, to continue the example above, if we want to force R to calculate the 1+2 part before the multiplication, all we would have to do is enclose it in brackets:\n\n(1 + 2) * 4 \n\n[1] 12\n\n\nThis is a fairly useful thing to be able to do. The only other thing I should point out about order of operations is what to expect when you have two operations that have the same priority: that is, how does R resolve ties? For instance, multiplication and division are actually the same priority, but what should we expect when we give R a problem like 4 / 2 * 3 to solve? If it evaluates the multiplication first and then the division, it would calculate a value of two-thirds. But if it evaluates the division first it calculates a value of 6. The answer, in this case, is that R goes from left to right, so in this case the division step would come first:\n\n4 / 2 * 3\n\n[1] 6\n\n\nAll of the above being said, it’s helpful to remember that parentheses always come first. So, if you’re ever unsure about what order R will do things in, an easy solution is to enclose the thing you want it to do first in parentheses In addition, making the order of operations explicit makes your code more readable. By enclosing the division in parentheses (e.g., (4 / 2) * 3) we make it clear which thing happens first."
  },
  {
    "objectID": "programming.html#sec-assign",
    "href": "programming.html#sec-assign",
    "title": "2  R as a Calculator",
    "section": "2.3 Storing a number as a variable",
    "text": "2.3 Storing a number as a variable\nOne of the most important things to be able to do in R (or any programming language, for that matter) is to store information in variables. At a conceptual level you can think of a variable as label for a certain piece of information, or even several different pieces of information. For example, when using R as a calculator, there may be times when you want to store an intermediate result along the way. For example, when calculating an average (the sum divided by the count), you might wish to save the sum before dividing that sum by the count. Let’s look at the very basics for how we create variables and work with them.\n\n2.3.1 Variable assignment using &lt;-\nSince we’ve been working with numbers so far, let’s start by creating variables to store our numbers. And since most people like concrete examples, let’s invent one. Suppose I’m trying to calculate how much money I’m going to make selling this book. There’s several different numbers I might want to store. Firstly, I need to figure out how many copies I’ll sell. This isn’t exactly Harry Potter, so let’s assume I’m only going to sell one copy per student in my class. Let’s assume there are 30 students, so that’s 30 sales. Let’s create a variable called sales. What I want to do is assign a value to my variable sales, and that value should be 30. We do this by using the assignment operator, which is &lt;-. Here’s how we do it:\n\nsales &lt;- 30\n\nWhen you hit enter, R doesn’t print out any output. It just gives you another command prompt. However, behind the scenes R has created a variable called sales and assign the value 30 to it. You can check that this has happened by asking R to print the variable on screen. And the simplest way to do that is to type the name of the variable and hit enter.\n\nsales\n\n[1] 30\n\n\nSo that’s nice to know. Anytime you can’t remember what R has got stored in a particular variable, you can just type the name of the variable and hit enter.\nOkay, so now we know how to assign variables. Actually, there’s a bit more you should know. Firstly, one of the curious features of R is that there are several different ways of making assignments. In addition to the &lt;- operator, we can also use -&gt; and =, and it’s pretty important to understand the differences between them. Let’s start by considering -&gt;, since that’s the easy one (we’ll discuss the use of = in Section 2.4.1). As you might expect from just looking at the symbol, it’s almost identical to &lt;-. It’s just that the arrow (i.e., the assignment) goes from left to right. So if I wanted to define my sales variable using -&gt;, I would write it like this:\n\n30 -&gt; sales\n\nThis has the same effect. And, just to be confusing, this also has the same effect:\n\nsales = 30\n\n… and so does this:\n\nassign(\"sales\", 30)\n\n\n\n\n\n\n\nCaution\n\n\n\nApart from superficial differences, these various approaches to assignment are functionally identical. Despite this equivalence, you are strongly encouraged to use the &lt;- operator. Because the use of &lt;- is so conventional within the R language, those familiar with R will have a much more difficult time reading R code that uses anything else. Soon enough, you will too will be familiar with R and will thus come to expect the use of &lt;-.\n\n\n\n\n2.3.2 Calculations using variables\nOkay, let’s get back to my original story. In my quest to become rich, I’ve written this textbook. To figure out how good a strategy is, I’ve started creating some variables in R. In addition to defining a sales variable that counts the number of copies I’m going to sell, I can also create a variable called royalty, indicating how much money I get per copy. Let’s say that my royalties are about $7 per book:\n\nsales &lt;- 30\nroyalty &lt;- 7\n\nThe nice thing about variables (in fact, the whole point of having variables) is that we can do anything with a variable that we ought to be able to do with the information that it stores. That is, since R allows me to multiply 30 by 7\n\n30 * 7\n\n[1] 210\n\n\nit also allows me to multiply sales by royalty\n\nsales * royalty\n\n[1] 210\n\n\nAs far as R is concerned, the sales * royalty command is the same as the 30 * 7 command. Not surprisingly, I can assign the output of this calculation to a new variable, which I’ll call revenue. And when we do this, the new variable revenue gets the value 35. So let’s do that, and then get R to print out the value of revenue so that we can verify that it’s done what we asked:\n\nrevenue &lt;- sales * royalty\nrevenue\n\n[1] 210\n\n\nThat’s fairly straightforward. A slightly more subtle thing we can do is reassign the value of my variable, based on its current value. For instance, suppose that one of my students (no doubt under the influence of psychotropic drugs) loves the book so much that he or she donates an extra $550 to me. The simplest way to capture this is by a command like this:\n\nrevenue &lt;- revenue + 550\nrevenue\n\n[1] 760\n\n\nIn this calculation, R has taken the old value of revenue (i.e., 210) and added 550 to that value, producing a value of 760 This new value is assigned to the revenue variable, overwriting its previous value. In any case, we now know that I’m expecting to make $760 off this. Pretty sweet, I thinks to myself. Or at least, that’s what I thinks until I do a few more calculation and work out what the implied hourly wage I’m making off this looks like.\n\n\n2.3.3 Rules and conventions for naming variables\nIn the examples that we’ve seen so far, my variable names (sales and revenue) have just been English-language words written using lowercase letters. However, R allows a lot more flexibility when it comes to naming your variables, as the following list of rules illustrates:\n\nVariable names can only use the upper case alphabetic characters A-Z as well as the lower case characters a-z. You can also include numeric characters 0-9 in the variable name, as well as the period . or underscore _ character. In other words, you can use SaL.e_s as a variable name (though I can’t think why you would want to), but you can’t use Sales?.\nVariable names cannot include spaces: therefore my sales is not a valid name, but my.sales is.\nVariable names are case sensitive: that is, Sales and sales are different variable names.\nVariable names must start with a letter or a period. You can’t use something like _sales or 1sales as a variable name. You can use .sales as a variable name if you want, but it’s not usually a good idea. By convention, variables starting with a . are used for special purposes, so you should avoid doing so.\nVariable names cannot be one of the reserved keywords. These are special names that R needs to keep “safe” from us mere users, so you can’t use them as the names of variables. The keywords are: if, else, repeat, while, function, for, in, next, break, TRUE, FALSE, NULL, Inf, NaN, NA, NA_integer_, NA_real_, NA_complex_, and finally, NA_character_. Don’t feel especially obliged to memorize these: if you make a mistake and try to use one of the keywords as a variable name, R will complain about it like the whiny little automaton it is.\n\nIn addition to those rules that R enforces, there are some informal conventions that people tend to follow when naming variables. One of them you’ve already seen: i.e., don’t use variables that start with a period. But there are several others. You aren’t obliged to follow these conventions, and there are many situations in which it’s advisable to ignore them, but it’s generally a good idea to follow them when you can:\n\nUse informative variable names. As a general rule, using meaningful names like sales and revenue is preferred over arbitrary ones like variable1 and variable2. Otherwise it’s very hard to remember what the contents of different variables are, and it becomes hard to understand what your commands actually do.\nUse short variable names. Typing is a pain and no-one likes doing it. So we much prefer to use a name like sales over a name like sales.for.this.book.that.you.are.reading. Obviously there’s a bit of a tension between using informative names (which tend to be long) and using short names (which tend to be meaningless), so use a bit of common sense when trading off these two conventions.\nUse one of the conventional naming styles for multi-word variable names. Suppose I want to name a variable that stores “my new salary”. Obviously I can’t include spaces in the variable name, so how should I do this? There are two main conventions that you sometimes see R users employing. First, there is “camel case” in which you use capital letters at the beginning of each constituent word (except the first one), which gives you myNewSalary as the variable name. Second, there is “snake case” in which you separate words using underscores, as in my_new_salary. Finally, you may also see some R users separating words using periods, which would give you my.new.salary.\n\n\n\n\n\n\n\nCaution\n\n\n\nThough you may sometimes see R users separating words within a variable name using periods (e.g., my.new.salary), it is syntactically ambiguous for those who know other programming languages. Many languages use periods to indicate hierarchical relationships (e.g., obj.func will refer to the func that belongs to obj). I thus strongly discourage such practices because it makes your code difficult to read/understand. Camel case is recommended."
  },
  {
    "objectID": "programming.html#sec-usingfunctions",
    "href": "programming.html#sec-usingfunctions",
    "title": "2  R as a Calculator",
    "section": "2.4 Functions",
    "text": "2.4 Functions\nThe symbols +, -, *, etc. are examples of operators. As we’ve seen, you can do quite a lot of calculations just by using these operators. However, in order to do more advanced calculations (and later on, to do actual statistics), you’re going to need to start using functions. We’ll see more detail about functions and how they work later, but for now let’s just dive in and use a few. To get started, suppose I wanted to take the square root of 225. The square root, in case your high school math is a bit rusty, is just the opposite of squaring a number. So, for instance, since “5 squared is 25” I can say that “5 is the square root of 25”. This is the usual notation:\n\\[\n\\sqrt{25} = 5\n\\]\nSometimes you’ll also see it written like this:\n\\(25^{0.5} = 5\\)\nThis second way of writing it is kind of useful to “remind” you of the mathematical fact that “square root of \\(x\\)” is actually the same as “raising \\(x\\) to the power of 0.5”. Personally, I’ve never found this to be terribly meaningful psychologically, though I have to admit it’s quite convenient mathematically. Anyway, it’s not important. What is important is that you remember what a square root is, since we’re going to need it later on.\nYou may be able to calculate the square root of 25 in your head. But it gets more difficult when the numbers get bigger, and pretty much impossible if they’re not whole numbers. This is where something like R comes in very handy. Let’s say I wanted to calculate \\(\\sqrt{225}\\), the square root of 225. There’s two ways I could do this using R. First, since the square root of 255 is the same thing as raising 225 to the power of 0.5, we could use the power operator ^, just like we did earlier:\n\n225^0.5\n\n[1] 15\n\n\nHowever, there’s a second way that we can do this, since R also provides a square root function, sqrt(). To calculate the square root of 255 using this function, what I do is insert the number 225 in the parentheses. That is, the command I type is this:\n\nsqrt(225)\n\n[1] 15\n\n\nAs you might expect from our previous discussion, the spaces in between the parentheses are purely cosmetic. We could have typed sqrt(225) or sqrt( 225   ) and gotten the same result. When we use a function to do something, we generally refer to this as calling the function, and the values that we type into the function (there can be more than one) are referred to as the arguments of that function.\nObviously, the sqrt() function doesn’t really give us any new functionality, since we already knew how to do square root calculations by using the power operator ^, though it maybe be more explicit, clearer, and thus easier to read to use sqrt(). However, there are lots of other functions in R: in fact, almost everything of interest that I’ll talk about in this book is an R function of some kind. For example, one function that we will need to use in this book is the absolute value function. Compared to the square root function, it’s extremely simple: it just converts negative numbers to positive numbers, and leaves positive numbers alone. Mathematically, the absolute value of \\(x\\) is written \\(|x|\\) or sometimes \\(\\mbox{abs}(x)\\). Calculating absolute values in R is pretty easy, since R provides the abs() function that you can use for this purpose. When you feed it a positive number…\n\nabs(21)\n\n[1] 21\n\n\nthe absolute value function does nothing to it at all. But when you feed it a negative number, it spits out the positive version of the same number, like this:\n\nabs(-13)\n\n[1] 13\n\n\nIn all honesty, there’s nothing that the absolute value function does that you couldn’t do just by looking at the number and erasing the minus sign if there is one. However, there’s a few places later in the book where we have to use absolute values, so I thought it might be a good idea to explain the meaning of the term early on.\nBefore moving on, it’s worth noting that – in the same way that R allows us to put multiple operations together into a longer command, like 1 + (2*4) for instance – it also lets us put functions together and even combine functions with operators if we so desire. For example, the following is a perfectly legitimate command:\n\nsqrt( 1 + abs(-8) )\n\n[1] 3\n\n\nWhen R executes this command, starts out by calculating the value of abs(-8), which produces an intermediate value of 8. Having done so, the command simplifies to sqrt( 1 + 8 ). To solve the square root it first needs to add 1 + 8 to get 9, at which point it evaluates sqrt(9), and so it finally outputs a value of 3.\n\n2.4.1 Function arguments, their names and their defaults\nThere’s two more fairly important things that you need to understand about how functions work in R, and that’s the use of “named” arguments, and “default values” for arguments. Not surprisingly, that’s not to say that this is the last we’ll hear about how functions work, but they are the last things we desperately need to discuss in order to get you started. To understand what these two concepts are all about, I’ll introduce another function. The round() function can be used to round some value to the nearest whole number. For example, I could type this:\n\nround(3.1415)\n\n[1] 3\n\n\nPretty straightforward, really. However, suppose I only wanted to round it to two decimal places: that is, I want to get 3.14 as the output. The round() function supports this, by allowing you to input a second argument to the function that specifies the number of decimal places that you want to round the number to. In other words, I could do this:\n\nround(3.14165, 2)\n\n[1] 3.14\n\n\nWhat’s happening here is that I’ve specified two arguments: the first argument is the number that needs to be rounded (i.e., 3.14165), the second argument is the number of decimal places that it should be rounded to (i.e., 2), and the two arguments are separated by a comma. In this simple example, it’s quite easy to remember which one argument comes first and which one comes second, but for more complicated functions this is not easy. Fortunately, most R functions make use of argument names. For the round() function, for example the number that needs to be rounded is specified using the x argument, and the number of decimal points that you want it rounded to is specified using the digits argument. Because we have these names available to us, we can specify the arguments to the function by name. We do so like this:\n\nround(x=3.1415, digits=2)\n\n[1] 3.14\n\n\nNotice that this is kind of similar in spirit to variable assignment, except that = is used here, rather than &lt;-. In both cases we’re specifying specific values to be associated with a label. However, there are some differences between what we were doing earlier on when creating variables, and what we’re doing here when specifying arguments, and so as a consequence it’s important that you use = in this context.\nAs you can see, specifying the arguments by name involves a lot more typing, but it’s also explicit and thus a lot easier to read. Because of this, the commands in this book will usually specify arguments by name, since that makes it clearer to you what I’m doing. However, one important thing to note is that when specifying the arguments using their names, it doesn’t matter what order you type them in. But if you don’t use the argument names, then you have to input the arguments in the correct order. In other words, these three commands all produce the same output…\n\nround(x=3.1415, 2)\n\n[1] 3.14\n\nround(x=3.1415, digits=2)\n\n[1] 3.14\n\nround(digits=2, x=3.1415)\n\n[1] 3.14\n\n\nbut this one does not…\n\nround(2, 3.14165)\n\n[1] 2\n\n\nHow do you find out what the correct order is? There’s a few different ways, but the easiest one is to look at the help documentation for the function (e.g., ? round). However, if you’re ever unsure, it’s probably best to actually type in the argument name.\nOkay, so that’s the first thing I said you’d need to know: argument names. The second thing you need to know about is default values. Notice that the first time I called the round() function I didn’t actually specify the digits argument at all, and yet R somehow knew that this meant it should round to the nearest whole number. How did that happen? The answer is that the digits argument has a default value of 0, meaning that if you decide not to specify a value for digits then R will act as if you had typed digits = 0. This is quite handy: the vast majority of the time when you want to round a number you want to round it to the nearest whole number, and it would be pretty annoying to have to specify the digits argument every single time. On the other hand, sometimes you actually do want to round to something other than the nearest whole number, and it would be even more annoying if R didn’t allow this! Thus, by having digits = 0 as the default value, we get the best of both worlds."
  },
  {
    "objectID": "programming.html#sec-vectors",
    "href": "programming.html#sec-vectors",
    "title": "2  R as a Calculator",
    "section": "2.5 Storing many numbers as a vector",
    "text": "2.5 Storing many numbers as a vector\nAt this point we’ve covered functions in enough detail to get us safely through most of the rest of the book, so let’s return to our discussion of variables. When variables were introduced in Section 2.3 we saw how we can use variables to store a single number. In this section, we’ll extend this idea and look at how to store multiple numbers within the one variable. In R, a variable stores multiple values is called a vector. So let’s create one.\n\n2.5.1 Creating a vector\nLet’s stick to my silly “get rich quick by textbook writing” example. Suppose the textbook company (if there actually was one, that is) sends sales data on a monthly basis. Since my class start in late February, we might expect most of the sales to occur towards the start of the year. Let’s suppose that I have 100 sales in February, 200 sales in March and 50 sales in April, and no other sales for the rest of the year. What I would like to do is have a variable – let’s call it sales.by.month – that stores all this sales data. The first number stored should be 0 since I had no sales in January, the second should be 100, and so on. The simplest way to do this in R is to use the combine function, c(). To do so, all we have to do is type all the numbers you want to store in a comma separated list, like this:\n\nsales.by.month &lt;- c(0, 100, 200, 50, 0, 0, 0, 0, 0, 0, 0, 0)\nsales.by.month\n\n [1]   0 100 200  50   0   0   0   0   0   0   0   0\n\n\nTo use the correct terminology here, we have a single variable here called sales.by.month: this variable is a vector that consists of 12 elements.\n\n\n2.5.2 A handy digression\nNow that we’ve learned how to put information into a vector, the next thing to understand is how to pull that information back out again. However, before I do so it’s worth taking a slight detour. If you’ve been following along, typing all the commands into R yourself, it’s possible that the output that you saw when we printed out the sales.by.month vector was slightly different to what I showed above. This would have happened if the window (or the RStudio panel) that contains the R console is really, really narrow. If that were the case, you might have seen output that looks something like this:\n\nsales.by.month\n\n [1]   0 100 200  50\n [5]   0   0   0   0\n [9]   0   0   0   0\n\n\nBecause there wasn’t much room on the screen, R has printed out the results over three lines. But that’s not the important thing to notice. The important point is that the first line has a [1] in front of it, whereas the second line starts with [5] and the third with [9]. It’s pretty clear what’s happening here. For the first row, R has printed out the 1st element through to the 4th element, so it starts that row with a [1]. For the second row, R has printed out the 5th element of the vector through to the 8th one, and so it begins that row with a [5] so that you can tell where it’s up to at a glance. It might seem a bit odd to you that R does this, but in some ways it’s a kindness, especially when dealing with larger data sets!\n\n\n2.5.3 Getting information out of vectors\nTo get back to the main story, let’s consider the problem of how to get information out of a vector. At this point, you might have a sneaking suspicion that the answer has something to do with the [1] and [9] things that R has been printing out. And of course you are correct. Suppose I want to pull out the February sales data only. February is the second month of the year, so let’s try this:\n\nsales.by.month[2]\n\n[1] 100\n\n\nYep, that’s the February sales all right. But there’s a subtle detail to be aware of here: notice that R outputs [1] 100, not [2] 100. This is because R is being extremely literal. When we typed in sales.by.month[2], we asked R to find exactly one thing, and that one thing happens to be the second element of our sales.by.month vector. So, when it outputs [1] 100 what R is saying is that the first number that we just asked for is 100. This behavior makes more sense when you realize that we can use this trick to create new variables. For example, I could create a february.sales variable like this:\n\nfebruary.sales &lt;- sales.by.month[2]\nfebruary.sales\n\n[1] 100\n\n\nObviously, the new variable february.sales should only have one element and so when I print it out this new variable, the R output begins with a [1] because 100 is the value of the first (and only) element of february.sales. The fact that this also happens to be the value of the second element of sales.by.month is irrelevant. We’ll pick this topic up again shortly (Section 2.9).\n\n\n2.5.4 Altering the elements of a vector\nSometimes you’ll want to change the values stored in a vector. Imagine my surprise when the publisher rings me up to tell me that the sales data for May are wrong. There were actually an additional 25 books sold in May, but there was an error or something so they hadn’t told me about it. How can I fix my sales.by.month variable? One possibility would be to assign the whole vector again from the beginning, using c(). But that’s a lot of typing. Also, it’s a little wasteful: why should R have to redefine the sales figures for all 12 months, when only the 5th one is wrong? Fortunately, we can tell R to change only the 5th element, using this trick:\n\nsales.by.month[5] &lt;- 25\nsales.by.month\n\n [1]   0 100 200  50  25   0   0   0   0   0   0   0\n\n\nAnother way to edit variables is to use the edit() or fix() functions. I won’t discuss them in detail right now, but you can check them out on your own.\n\n\n2.5.5 Useful things to know about vectors\nBefore moving on, I want to mention a couple of other things about vectors. Firstly, you often find yourself wanting to know how many elements there are in a vector (usually because you’ve forgotten). You can use the length() function to do this. It’s quite straightforward:\n\nlength(x = sales.by.month)\n\n[1] 12\n\n\nSecondly, you often want to alter all of the elements of a vector at once. For instance, suppose I wanted to figure out how much money I made in each month. Since I’m earning an exciting $7 per book (no seriously, that’s actually pretty close to what authors get on the very expensive textbooks that you’re expected to purchase), what I want to do is multiply each element in the sales.by.month vector by 7. R makes this pretty easy, as the following example shows:\n\nsales.by.month * 7\n\n [1]    0  700 1400  350  175    0    0    0    0    0    0    0\n\n\nIn other words, when you multiply a vector by a single number, all elements in the vector get multiplied. The same is true for addition, subtraction, division and taking powers. So that’s neat. On the other hand, suppose I wanted to know how much money I was making per day, rather than per month. Since not every month has the same number of days, I need to do something slightly different. Firstly, I’ll create two new vectors:\n\ndays.per.month &lt;- c(31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31)\nprofit &lt;- sales.by.month * 7\n\nObviously, the profit variable is the same one we created earlier, and the days.per.month variable is pretty straightforward. What I want to do is divide every element of profit by the corresponding element of days.per.month. Again, R makes this pretty easy:\n\nprofit / days.per.month\n\n [1]  0.000000 25.000000 45.161290 11.666667  5.645161  0.000000  0.000000\n [8]  0.000000  0.000000  0.000000  0.000000  0.000000\n\n\nI still don’t like all those zeros, but that’s not what matters here. Notice that the second element of the output is 25, because R has divided the second element of profit (i.e. 700) by the second element of days.per.month (i.e. 28). Similarly, the third element of the output is equal to 1400 divided by 31, and so on. We’ll talk more about calculations involving vectors later on, but that’s enough detail for now."
  },
  {
    "objectID": "programming.html#sec-text",
    "href": "programming.html#sec-text",
    "title": "2  R as a Calculator",
    "section": "2.6 Storing text data",
    "text": "2.6 Storing text data\nA lot of the time your data will be numeric in nature, but not always. Sometimes your data really needs to be described using text, not using numbers. To address this, we need to consider the situation where our variables store text. To create a variable that stores the word “hello”, we can type this:\n\ngreeting &lt;- \"hello\"\ngreeting\n\n[1] \"hello\"\n\n\nWhen interpreting this, it’s important to recognise that the quote marks here aren’t part of the string itself. They’re just something that we use to make sure that R knows to treat the characters that they enclose as a piece of text data, known as a character string. In other words, R treats \"hello\" as a string containing the word “hello”; but if I had typed hello instead, R would go looking for a variable by that name! You can also use 'hello' to specify a character string.\nOkay, so that’s how we store the text. Next, it’s important to recognise that when we do this, R stores the entire word \"hello\" as a single element: our greeting variable is not a vector of five different letters. Rather, it has only the one element, and that element corresponds to the entire character string \"hello\". To illustrate this, if I actually ask R to find the first element of greeting, it prints the whole string:\n\ngreeting[1]\n\n[1] \"hello\"\n\n\nOf course, there’s no reason why I can’t create a vector of character strings. For instance, if we were to continue with the example of my attempts to look at the monthly sales data for my book, one variable I might want would include the names of all 12 months. To do so, I could type in a command like this\n\nmonths &lt;- c(\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n            \"July\", \"August\", \"September\", \"October\", \"November\", \n            \"December\")\n\nThis is a character vector containing 12 elements, each of which is the name of a month. So if I wanted R to tell me the name of the fourth month, all I would do is this:\n\nmonths[4]\n\n[1] \"April\"\n\n\n\n2.6.1 Working with text\nWorking with text data is somewhat more complicated than working with numeric data. There is much to discuss here, but for purposes of the current chapter we only need this bare bones sketch. The only other thing I want to do before moving on is show you an example of a function that can be applied to text data. So far, most of the functions that we have seen (i.e., sqrt(), abs() and round()) only make sense when applied to numeric data (e.g., you can’t calculate the square root of “hello”), and we’ve seen one function that can be applied to pretty much any variable or vector (i.e., length()). So it might be nice to see an example of a function that can be applied to text.\nThe function I’m going to introduce you to is called nchar(), and what it does is count the number of individual characters that make up a string. Recall earlier that when we tried to calculate the length() of our greeting variable it returned a value of 1: the greeting variable contains only the one string, which happens to be \"hello\". But what if I want to know how many letters there are in the word? Sure, I could count them, but that’s boring, and more to the point it’s a terrible strategy if what I wanted to know was the number of letters in War and Peace. That’s where the nchar() function is helpful:\n\nnchar( x = greeting )\n\n[1] 5\n\n\nThat makes sense, since there are in fact 5 letters in the string \"hello\". Better yet, you can apply nchar() to whole vectors. So, for instance, if I want R to tell me how many letters there are in the names of each of the 12 months, I can do this:\n\nnchar( x = months )\n\n [1] 7 8 5 5 3 4 4 6 9 7 8 8\n\n\nSo that’s nice to know. The nchar() function can do a bit more than this, and there’s a lot of other functions that you can do to extract more information from text or do all sorts of fancy things. However, the goal here is not to teach any of that! The goal right now is just to see an example of a function that actually does work when applied to text."
  },
  {
    "objectID": "programming.html#logicals",
    "href": "programming.html#logicals",
    "title": "2  R as a Calculator",
    "section": "2.7 Storing “true or false” data",
    "text": "2.7 Storing “true or false” data\nTime to move onto a third kind of data. A key concept in that a lot of R relies on is the idea of a logical value or (Boolean value). A logical value is an assertion about whether something is true or false. This is implemented in R in a pretty straightforward way. There are two logical values, namely TRUE and FALSE. Despite the simplicity, a logical values are very useful things. Let’s see how they work.\n\n2.7.1 Assessing mathematical truths\nIn George Orwell’s classic book 1984, one of the slogans used by the totalitarian Party was “two plus two equals five”, the idea being that the political domination of human freedom becomes complete when it is possible to subvert even the most basic of truths. It’s a terrifying thought, especially when the protagonist Winston Smith finally breaks down under torture and agrees to the proposition. “Man is infinitely malleable”, the book says. I’m pretty sure that this isn’t true of humans but it’s definitely not true of R. R is not infinitely malleable. It has rather firm opinions on the topic of what is and isn’t true, at least as regards basic mathematics. If I ask it to calculate 2 + 2, it always gives the same answer, and it’s not bloody 5:\n\n2 + 2\n\n[1] 4\n\n\nOf course, so far R is just doing the calculations. I haven’t asked it to explicitly assert that \\(2+2 = 4\\) is a true statement. If I want R to make an explicit judgement, I can use a command like this:\n\n2 + 2 == 4\n\n[1] TRUE\n\n\nWhat I’ve done here is use the equality operator, ==, to force R to make a “true or false” judgement. Okay, let’s see what R thinks of the Party slogan:\n\n2+2 == 5\n\n[1] FALSE\n\n\nBooyah! Freedom and ponies for all! Or something like that. Anyway, it’s worth having a look at what happens if I try to force R to believe that two plus two is five by making an assignment statement like 2 + 2 = 5 or 2 + 2 &lt;- 5. When I do this, here’s what happens:\n\n2 + 2 = 5\n\nError in 2 + 2 = 5: target of assignment expands to non-language object\n\n\nR doesn’t like this very much. It recognizes that 2 + 2 is not a variable (that’s what the “non-language object” part is saying), and it won’t let you try to “reassign” it. While R is pretty flexible, and actually does let you do some quite remarkable things to redefine parts of R itself, there are just some basic, primitive truths that it refuses to give up. It won’t change the laws of addition, and it won’t change the definition of the number 2.\nThat’s probably for the best.\n\n\n2.7.2 Logical operations\nSo now we’ve seen logical operations at work, but so far we’ve only seen the simplest possible example. You probably won’t be surprised to discover that we can combine logical operations with other operations and functions in a more complicated way, like this:\n\n3*3 + 4*4 == 5*5\n\n[1] TRUE\n\n\nor this\n\nsqrt( 25 ) == 5\n\n[1] TRUE\n\n\nNot only that, but as Table Table 2.1 illustrates, there are several other logical operators that you can use, corresponding to some basic mathematical concepts.\n\n\nTable 2.1: ?(caption)\n\n\n\n\n(a) Some logical operators. Technically I should be calling these “binary relational operators”, but quite frankly I don’t want to. It’s my book so no-one can make me.\n\n\noperation\noperator\nexample input\nanswer\n\n\n\n\nless than\n&lt;\n2 &lt; 3\nTRUE\n\n\nless than or equal to\n&lt;=\n2 &lt;= 2\nTRUE\n\n\ngreater than\n&gt;\n2 &gt; 3\nFALSE\n\n\ngreater than or equal to\n&gt;=\n2 &gt;= 2\nTRUE\n\n\nequal to\n==\n2 == 3\nFALSE\n\n\nnot equal to\n!=\n2 != 3\nTRUE\n\n\n\n\n\n\nHopefully these are all pretty self-explanatory: for example, the less than operator &lt; checks to see if the number on the left is less than the number on the right. If it’s less, then R returns an answer of TRUE:\n\n99 &lt; 100\n\n[1] TRUE\n\n\nbut if the two numbers are equal, or if the one on the right is larger, then R returns an answer of FALSE, as the following two examples illustrate:\n\n100 &lt; 100\n\n[1] FALSE\n\n100 &lt; 99\n\n[1] FALSE\n\n\nIn contrast, the less than or equal to operator &lt;= will do exactly what it says. It returns a value of TRUE if the number of the left hand side is less than or equal to the number on the right hand side. So if we repeat the previous two examples using &lt;=, here’s what we get:\n\n100 &lt;= 100\n\n[1] TRUE\n\n100 &lt;= 99\n\n[1] FALSE\n\n\nAnd at this point I hope it’s pretty obvious what the greater than operator &gt; and the greater than or equal to operator &gt;= do! Next on the list of logical operators is the not equal to operator != which – as with all the others – does what it says it does. It returns a value of TRUE when things on either side are not identical to each other. Therefore, since \\(2+2\\) isn’t equal to \\(5\\), we get:\n\n2 + 2 != 5\n\n[1] TRUE\n\n\nWe’re not quite done yet. There are three more logical operations that are worth knowing about, listed in Table Table 2.2.\n\n\nTable 2.2: ?(caption)\n\n\n\n\n(a) Some more logical operators.\n\n\noperation\noperator\nexample input\nanswer\n\n\n\n\nnot\n!\n!(1==1)\nFALSE\n\n\nor\n|\n(1==1) | (2==3)\nTRUE\n\n\nand\n&\n(1==1) & (2==3)\nFALSE\n\n\n\n\n\n\nThese are the not operator !, the and operator &, and the or operator |. Like the other logical operators, their behavior is more or less exactly what you’d expect given their names. For instance, if I ask you to assess the claim that either \\(2+2 = 4\\) or \\(2+2 = 5\\), then you’d say that claim is true. Since it’s an “either-or” statement, all we need is for one of the two parts to be true. That’s what the | operator does:\n\n(2+2 == 4) | (2+2 == 5)\n\n[1] TRUE\n\n\nOn the other hand, if I ask you to assess the claim that both \\(2+2 = 4\\) and \\(2+2 = 5\\), then you’d say that claim is false. Since this is an and statement we need both parts to be true. And that’s what the & operator does:\n\n(2+2 == 4) & (2+2 == 5)\n\n[1] FALSE\n\n\nFinally, there’s the not operator, which is simple but annoying to describe in English. If I ask you to assess my claim that “it is not true that \\(2+2 = 5\\)”, then you would say that claim is true; because my claim is that “\\(2+2 = 5\\) is false”. And I’m right. If we write this as an R command we get this:\n\n! (2+2 == 5)\n\n[1] TRUE\n\n\nIn other words, since 2+2 == 5 is a FALSE statement, it must be the case that !(2+2 == 5) is a TRUE one. Essentially, what we’ve really done is claim that “not false” is the same thing as “true”. Obviously, this isn’t really quite right in real life. But logical values encode a black and white world: any given logical statement is either true or false. No shades of gray are allowed. We can actually see this much more explicitly, like this:\n\n! FALSE\n\n[1] TRUE\n\n\nOf course, in our \\(2+2 = 5\\) example, we didn’t really need to use “not” ! and “equals to” == as two separate operators. We could have just used the “not equals to” operator != like this:\n\n2+2 != 5\n\n[1] TRUE\n\n\nBut there are many situations where you really do need to use the ! operator. We’ll see some later on.\n\n\n2.7.3 Storing and using logical data\nUp to this point, I’ve introduced numeric data (Section 2.3 and Section 2.5) and character data (Section 2.6). So you might not be surprised to discover that these TRUE and FALSE values that R has been producing are actually a third kind of data, called logical data. That is, when I asked R if 2 + 2 == 5 and it said [1] FALSE in reply, it was actually producing information that we can store in variables. For instance, I could create a variable called is.the.Party.correct, which would store R’s opinion:\n\nis.the.Party.correct &lt;- 2 + 2 == 5\nis.the.Party.correct\n\n[1] FALSE\n\n\nAlternatively, you can assign the value directly, by typing TRUE or FALSE in your command. Like this:\n\nis.the.Party.correct &lt;- FALSE\nis.the.Party.correct\n\n[1] FALSE\n\n\nBetter yet, because it’s kind of tedious to type TRUE or FALSE over and over again, R provides you with a shortcut: you can use T and F instead (but it’s case sensitive: t and f won’t work)."
  },
  {
    "objectID": "programming.html#true-and-false",
    "href": "programming.html#true-and-false",
    "title": "2  R as a Calculator",
    "section": "2.8 TRUE and FALSE",
    "text": "2.8 TRUE and FALSE\nTRUE and FALSE are reserved keywords in R, so you can trust that they always mean what they say they do. Unfortunately, the shortcut versions T and F do not have this property. It’s even possible to create variables that set up the reverse meanings, by typing commands like T &lt;- FALSE and F &lt;- TRUE. This is kind of insane, and something that is generally thought to be a design flaw in R. Anyway, the long and short of it is that it’s safer to use TRUE and FALSE.:::\nSo this works:\n\nis.the.Party.correct &lt;- F\nis.the.Party.correct\n\n[1] FALSE\n\n\nbut this doesn’t:\n\nis.the.Party.correct &lt;- f\n\nError in eval(expr, envir, enclos): object 'f' not found\n\n\n\n2.8.1 Vectors of logicals\nThe next thing to mention is that you can store vectors of logical values in exactly the same way that you can store vectors of numbers (Section 2.5) and vectors of text data (Section 2.6). Again, we can define them directly via the c() function, like this:\n\nx &lt;- c(TRUE, TRUE, FALSE)\nx\n\n[1]  TRUE  TRUE FALSE\n\n\nor you can produce a vector of logicals by applying a logical operator to a vector. This might not make a lot of sense to you, so let’s unpack it slowly. First, let’s suppose we have a vector of numbers (i.e., a “non-logical vector”). For instance, we could use the sales.by.month vector that we were using in Section 2.5. Suppose I wanted R to tell me, for each month of the year, whether I actually sold a book in that month. I can do that by typing this:\n\nsales.by.month &gt; 0\n\n [1] FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nand again, I can store this in a vector if I want, as the example below illustrates:\n\nany.sales.this.month &lt;- sales.by.month &gt; 0\nany.sales.this.month\n\n [1] FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nIn other words, any.sales.this.month is a logical vector whose elements are TRUE only if the corresponding element of sales.by.month is greater than zero. For instance, since I sold zero books in January, the first element is FALSE.\n\n\n2.8.2 Applying logical operation to text\nIn a moment (Section 2.9) I’ll show you why these logical operations and logical vectors are so handy, but before I do so I want to very briefly point out that you can apply them to text as well as to logical data. It’s just that we need to be a bit more careful in understanding how R interprets the different operations. In this section I’ll talk about how the equal to operator == applies to text, since this is the most important one. Obviously, the not equal to operator != gives the exact opposite answers to == so I’m implicitly talking about that one too, but I won’t give specific commands showing the use of !=. There are a variety of other operators, but those will do for now.\nOkay, let’s see how it works. In one sense, it’s very simple. For instance, I can ask R if the word \"cat\" is the same as the word \"dog\", like this:\n\n\"cat\" == \"dog\"\n\n[1] FALSE\n\n\nThat’s pretty obvious, and it’s good to know that even R can figure that out. Similarly, R does recognize that a \"cat\" is a \"cat\":\n\n\"cat\" == \"cat\"\n\n[1] TRUE\n\n\nAgain, that’s exactly what we’d expect. However, what you need to keep in mind is that R is not at all tolerant when it comes to grammar and spacing. If two strings differ in any way whatsoever, R will say that they’re not equal to each other, as the following examples indicate:\n\n\" cat\" == \"cat\"\n\n[1] FALSE\n\n\"cat\" == \"CAT\"\n\n[1] FALSE\n\n\"cat\" == \"c a t\"\n\n[1] FALSE"
  },
  {
    "objectID": "programming.html#sec-indexing",
    "href": "programming.html#sec-indexing",
    "title": "2  R as a Calculator",
    "section": "2.9 Indexing vectors",
    "text": "2.9 Indexing vectors\nOne last thing to add before finishing up this chapter. So far, whenever I’ve had to get information out of a vector, all I’ve done is typed something like months[4]; and when I do this R prints out the fourth element of the months vector. In this section, I’ll show you two additional tricks for getting information out of the vector.\n\n2.9.1 Extracting multiple elements\nOne very useful thing we can do is pull out more than one element at a time. In the previous example, we only used a single number (i.e., 2) to indicate which element we wanted. Alternatively, we can use a vector. So, suppose I wanted the data for February, March and April. What I could do is use the vector c(2,3,4) to indicate which elements I want R to pull out. That is, I’d type this:\n\nsales.by.month[ c(2,3,4) ]\n\n[1] 100 200  50\n\n\nNotice that the order matters here. If I asked for the data in the reverse order (i.e., April first, then March, then February) by using the vector c(4,3,2), then R outputs the data in the reverse order:\n\nsales.by.month[ c(4,3,2) ]\n\n[1]  50 200 100\n\n\nA second thing to be aware of is that R provides you with handy shortcuts for very common situations. For instance, suppose that I wanted to extract everything from the 2nd month through to the 8th month. One way to do this is to do the same thing I did above, and use the vector c(2,3,4,5,6,7,8) to indicate the elements that I want. That works just fine\n\nsales.by.month[ c(2,3,4,5,6,7,8) ]\n\n[1] 100 200  50  25   0   0   0\n\n\nbut it’s kind of a lot of typing. To help make this easier, R lets you use 2:8 as shorthand for c(2,3,4,5,6,7,8), which makes things a lot simpler. First, let’s just check that this is true:\n\n2:8\n\n[1] 2 3 4 5 6 7 8\n\n\nNext, let’s check that we can use the 2:8 shorthand as a way to pull out the 2nd through 8th elements of sales.by.months:\n\nsales.by.month[2:8]\n\n[1] 100 200  50  25   0   0   0\n\n\nSo that’s kind of neat.\n\n\n2.9.2 Logical indexing\nAt this point, I can introduce an extremely useful tool called logical indexing. In the last section, I created a logical vector any.sales.this.month, whose elements are TRUE for any month in which I sold at least one book, and FALSE for all the others. However, that big long list of TRUEs and FALSEs is a little bit hard to read, so what I’d like to do is to have R select the names of the months for which I sold any books. Earlier on, I created a vector months that contains the names of each of the months. This is where logical indexing is handy. What I need to do is this:\n\nmonths[ sales.by.month &gt; 0 ]\n\n[1] \"February\" \"March\"    \"April\"    \"May\"     \n\n\nTo understand what’s happening here, it’s helpful to notice that sales.by.month &gt; 0 is the same logical expression that we used to create the any.sales.this.month vector in the last section. In fact, I could have just done this:\n\nmonths[ any.sales.this.month ]\n\n[1] \"February\" \"March\"    \"April\"    \"May\"     \n\n\nand gotten exactly the same result. In order to figure out which elements of months to include in the output, what R does is look to see if the corresponding element in any.sales.this.month is TRUE. Thus, since element 1 of any.sales.this.month is FALSE, R does not include \"January\" as part of the output; but since element 2 of any.sales.this.month is TRUE, R does include \"February\" in the output. Note that there’s no reason why I can’t use the same trick to find the actual sales numbers for those months. The command to do that would just be this:\n\nsales.by.month [ sales.by.month &gt; 0 ]\n\n[1] 100 200  50  25\n\n\nIn fact, we can do the same thing with text. Here’s an example. Suppose that – to continue the saga of the textbook sales – I later find out that the bookshop only had sufficient stocks for a few months of the year. They tell me that early in the year they had \"high\" stocks, which then dropped to \"low\" levels, and in fact for one month they were \"out\" of copies of the book for a while before they were able to replenish them. Thus I might have a variable called stock.levels which looks like this:\n\nstock.levels&lt;-c(\"high\", \"high\", \"low\", \"out\", \"out\", \"high\",\n                \"high\", \"high\", \"high\", \"high\", \"high\", \"high\")\n\nstock.levels\n\n [1] \"high\" \"high\" \"low\"  \"out\"  \"out\"  \"high\" \"high\" \"high\" \"high\" \"high\"\n[11] \"high\" \"high\"\n\n\nThus, if I want to know the months for which the bookshop was out of my book, I could apply the logical indexing trick, but with the character vector stock.levels, like this:\n\nmonths[stock.levels == \"out\"]\n\n[1] \"April\" \"May\"  \n\n\nAlternatively, if I want to know when the bookshop was either low on copies or out of copies, I could do this:\n\nmonths[stock.levels == \"out\" | stock.levels == \"low\"]\n\n[1] \"March\" \"April\" \"May\"  \n\n\nor this\n\nmonths[stock.levels != \"high\" ]\n\n[1] \"March\" \"April\" \"May\"  \n\n\nEither way, I get the answer I want.\nAt this point, I hope you can see why logical indexing is such a useful thing. It’s a very basic, yet very powerful way to manipulate data. Subsequent chapters will talk a lot more about how to manipulate data, since it’s a critical skill for real world research that is often overlooked in introductory statistics courses It does take a bit of practice to become completely comfortable using logical indexing, so it’s a good idea to play around with these sorts of commands. Try creating a few different variables of your own, and then ask yourself questions like “how do I get R to spit out all the elements that are [blah]”. Practice makes perfect, and it’s only by practicing logical indexing that you’ll perfect the art of yelling frustrated insults at your computer."
  },
  {
    "objectID": "programming.html#activities",
    "href": "programming.html#activities",
    "title": "2  R as a Calculator",
    "section": "2.10 Activities",
    "text": "2.10 Activities\n\nCompute \\(42+17\\)\nCompute \\(8-3\\)\nCompute \\((8-3)^2\\)\nCompute \\(\\frac{42+17}{(8-3)^2}\\)\nDefine a vector containing the numbers 29, 63, 7, 23, 84, 10 and 9.\nImagine this vector contains counts in units of months (29 months, 63 months, etc.). Compute a new vector that contains the same measurement but not in units of years. That is, divide all the entries in the previous vector by 12. Print these new measurements to the console.\nCreate two strings (character vectors). One should be \"R rules!\" and the other should be \"r rules!\". Determine whether these two vectors are equal.\nCreate a vector that consists of 6 values: 3 even and 3 odd.\nModify the third value in this vector so that it is now double it’s original value.\nModify this vector so that all the odd numbers are removed.\nCalculate the sum of the values currently in the vector.\nImagine a study in which participants must weigh less than 90 kg and be between 18 and 60 years of age. Define a vector of weights as weight &lt;- c(80, 75, 92, 105, 60) and a vector of ages age &lt;- c(50, 17, 39, 27, 90). Now calculate a vector of logical values, each of which indicates whether the corresponding participant is eligible for the study.\nCalculate the sum of 0.1 and 0.2.\nCalculate 10 times the sum of 0.1 and 0.2.\nDetermine whether 10 times the sum of 0.1 and 0.2 is equal to 3."
  },
  {
    "objectID": "scripts.html#scripts",
    "href": "scripts.html#scripts",
    "title": "3  Basic Programming",
    "section": "3.1 Scripts",
    "text": "3.1 Scripts\nComputer programs come in quite a few different forms: the kind of program that we’re mostly interested in from the perspective of everyday data analysis using R is known as a script. The idea behind a script is that, instead of typing your commands into the R console one at a time, instead you write them all in a text file. Then, once you’ve finished writing them and saved the text file, you can get R to execute all the commands in your file by using the source() function. In a moment I’ll show you exactly how this is done, but first I’d better explain why you should care.\n\n3.1.1 Why use scripts?\nBefore discussing scripting and programming concepts in any more detail, it’s worth stopping to ask why you should bother. After all, if you look at the R commands that I’ve used everywhere else this book, you’ll notice that they’re all formatted as if I were typing them at the command line. Outside this chapter you won’t actually see any scripts. But do not be fooled by this. The reason for this is purely for pedagogical reasons. To teach statistics and data analysis, it is natural to chop everything up into tiny little slices: each section tends to focus on one kind of statistical concept, and only a smallish number of R functions. It is far easier to see what each function does in isolation, one command at a time. By presenting everything as if it were being typed at the command line, it avoids piecing together many, many different commands into one big script. From a teaching (and learning) perspective that’s the right thing to do. But from a data analysis perspective, it is not. When you start analyzing real world data sets, you will rapidly find yourself needing to write scripts.\nTo understand why scripts are so very useful, it may be helpful to consider the drawbacks to typing commands directly at the command prompt. The approach that we’ve been adopting so far, in which you type commands one at a time, and R sits there patiently in between commands, is referred to as the interactive style. Doing your data analysis this way is rather like having a conversation … a very annoying conversation between you and your data set, in which you and the data aren’t directly speaking to each other, and so you have to rely on R to pass messages back and forth. This approach makes a lot of sense when you’re just trying out a few ideas: maybe you’re trying to figure out what analyses are sensible for your data, or maybe just you’re trying to remember how the various R functions work, so you’re just typing in a few commands until you get the one you want. In other words, the interactive style is very useful as a tool for exploring your data. However, it has a number of drawbacks:\n\nIt’s difficult to save your work effectively. You can save the workspace, so that later on you can load any variables you created. You can save your plots as images. And you can even save the history or copy the contents of the R console to a file. Taken together, all these things let you create a reasonably decent record of what you did. But it does leave a lot to be desired. It seems like you ought to be able to save a single file that R could use (in conjunction with your raw data files) and reproduce everything (or at least, everything interesting) that you did during your data analysis.\nIt’s annoying to have to go back to the beginning when you make a mistake. Suppose you’ve just spent the last two hours typing in commands. Over the course of this time you’ve created lots of new variables and run lots of analyses. Then suddenly you realize that there was a nasty typo in the first command you typed, so all of your later numbers are wrong. Now you have to fix that first command, and then spend another hour or so combing through the R history to try and recreate what you did.\nYou can’t leave notes for yourself. Sure, you can scribble down some notes on a piece of paper, or even save a Word document that summarizes what you did. But what you really want to be able to do is write down an English translation of your R commands, preferably right “next to” the commands themselves. That way, you can look back at what you’ve done and actually remember what you were doing. In the simple exercises we’ve engaged in so far, it hasn’t been all that hard to remember what you were doing or why you were doing it, but only because everything we’ve done could be done using only a few commands, and you’ve never been asked to reproduce your analysis six months after you originally did it! When your data analysis starts involving hundreds of variables, and requires quite complicated commands to work, then you really, really need to leave yourself some notes to explain your analysis to, well, yourself.\nIt’s nearly impossible to reuse your analyses later, or adapt them to similar problems. Suppose that, sometime in January, you are handed a difficult data analysis problem. After working on it for ages, you figure out some really clever tricks that can be used to solve it. Then, in September, you get handed a really similar problem. You can sort of remember what you did, but not very well. You’d like to have a clean record of what you did last time, how you did it, and why you did it the way you did. Something like that would really help you solve this new problem.\nIt’s hard to do anything except the basics. There’s a nasty side effect of these problems. Typos are inevitable. Even the best data analyst in the world makes a lot of mistakes. So the chance that you’ll be able to string together dozens of correct R commands in a row are very small. So unless you have some way around this problem, you’ll never really be able to do anything other than simple analyses.\nIt’s difficult to share your work other people. Because you don’t have this nice clean record of what R commands were involved in your analysis, it’s not easy to share your work with other people. Sure, you can send them all the data files you’ve saved, and your history and console logs, and even the little notes you wrote to yourself, but odds are pretty good that no-one else will really understand what’s going on.\n\nIdeally, what you’d like to be able to do is start out with a data set (e.g., myrawdata.csv). What you want is a single document (e.g., mydataanalysis.R) that stores all of the commands that you’ve used in order to do your data analysis. Kind of similar to the R history but much more focused. It would only include the commands that you want to keep for later. Then, later on, instead of typing in all those commands again, you’d just tell R to run all of the commands that are stored in mydataanalysis.R. Also, in order to help you make sense of all those commands, what you’d want is the ability to add some notes or comments within the file, so that anyone reading the document for themselves would be able to understand what each of the commands actually does. But these comments wouldn’t get in the way: when you try to get R to run mydataanalysis.R it would be smart enough would recognize that these comments are for the benefit of humans, and so it would ignore them. Later on you could tweak a few of the commands inside the file (maybe in a new file called mynewdatanalaysis.R) so that you can adapt an old analysis to be able to handle a new problem. And you could email your friends and colleagues a copy of this file so that they can reproduce your analysis themselves.\nIn other words, what you want is a script.\n\n\n3.1.2 Our first script\nOkay then. Since scripts are so terribly awesome, let’s write one. One approach would be to use a simple text editor like Apple’s TextEdit or Microsoft’s Notepad (not something like Microsoft Word). However, we will instead use Rstudio. To create new script file in R studio, go to the “File” menu, select the “New” option, and then click on “R script”. This will open a new window within the “source” panel. Then you can type the commands you want and save it when you’re done. The nice thing about using Rstudio to do this is that it automatically changes the color of the text to indicate which parts of the code are comments and which are parts are actual R commands (these colors are called syntax highlighting, but they’re not actually part of the file – it’s just Rstudio trying to be helpful. To see an example of this, let’s create a script called hello.R script in Rstudio. To do this, go to the “File” menu again, and select “Open…”. Once you’ve opened the file, you should be looking at a black script. Then you can type (or copy and paste) the commands below into your script so that the contents of your script look like this:\n\n## --- hello.R\nx &lt;- \"hello world\"\nprint(x)\n\nThe line at the top is the name of our script, and not part of the script itself. Below that, you can see the two R commands that make up the script itself.\nSo how do we run the script? Assuming that the hello.R file has been saved to your working directory, then you can run the script using the following command:\n\nsource(\"hello.R\")\n\nIf the script file is saved in a different directory, then you need to specify the path to the file, in exactly the same way that you would have to when loading a data file using load().\n\nsource(file.path(projecthome,\"scripts\",\"hello.R\"))\n\nIn either case, this is the output we see when we run our script:\n\nx &lt;- \"hello world\"\nprint(x)\n\n[1] \"hello world\"\n\n\nIf we inspect the workspace using a command like who() or objects(), we discover that R has created the new variable x within the workspace, and not surprisingly x is a character string containing the text \"hello world\". And just like that, you’ve written your first program R. It really is that simple.\nBeyond being the primary tool we use to interact with R, using Rstudio for your text editor is convenient for other reasons too. In the top right hand corner of the source pane there’s a little button that reads “Source”. If you click on that, Rstudio will construct the relevant source() command for you, and send it straight to the R console. So you don’t even have to type in the source() command, which actually I think is a great thing, because it really bugs me having to type all those extra keystrokes every time I want to run my script. Anyway, Rstudio provide several other convenient little tools to help make scripting easier that you will discover along the way.\n\n\n3.1.3 Commenting your script\nWhen writing up your data analysis as a script, one thing that is generally a good idea is to include a lot of comments in the code. That way, if someone else tries to read it (or if you come back to it several days, weeks, months or years later) they can figure out what’s going on. As a beginner, I think it’s especially useful to comment thoroughly, partly because it gets you into the habit of commenting the code, and partly because the simple act of typing in an explanation of what the code does will help you keep it clear in your own mind what you’re trying to achieve. To illustrate this idea, consider the following script:\n\n## --- itngscript.R\n# A script to analyse nightgarden.Rdata_\n#  author: Dan Navarro_\n#  date: 22/11/2011_\n\n# Load the data, and tell the user that this is what we're\n# doing. \ncat( \"loading data from nightgarden.Rdata...\\n\" )\nload(file.path(projecthome,\"data\",\"nightgarden.Rdata\"))\n\n# Create a cross tabulation and print it out:\ncat( \"tabulating data...\\n\" )\nitng.table &lt;- table( speaker, utterance )\nprint( itng.table )\n\nThe comments at the top of the script explain the purpose of the script, who wrote it, and when it was written. Then, comments throughout the script file itself explain what each section of the code actually does. In real life people don’t tend to comment this thoroughly, but that’s because people are lazy and never fully appreciate who might try and decipher their scripts in the future (including the author themselves). You really want your script to explain itself. Nevertheless, as you’d expect R completely ignores all of the commented parts.\n\n\n3.1.4 Differences between scripts and the command line\nFor the most part, commands that you insert into a script behave in exactly the same way as they would if you typed the same thing in at the command line. The one major exception to this is that if you want a variable to be printed on screen, you need to explicitly tell R to print it. You can’t just type the name of the variable. For example, our original hello.R script produced visible output. The following script does not:\n\n## --- silenthello.R\nx &lt;- \"hello world\"\nx\n\nIt does still create the variable x when you source() the script, but it won’t print anything on screen.\nHowever, apart from the fact that scripts don’t use “auto-printing” as it’s called, there aren’t a lot of differences in the underlying mechanics. There are a few stylistic differences though. For instance, if you want to load a package at the command line, you would generally use the library() function. If you want do to it from a script, it’s conventional to use require() instead. The two commands are basically identical, the only difference being that if the package doesn’t exist, require() produces a warning whereas library() gives you an error. Stylistically, what this means is that if the require() command fails in your script, R will boldly continue on and try to execute the rest of the script. Often that’s what you’d like to see happen, so it’s better to use require(). Clearly, however, you can get by just fine using the library() command for everyday usage.\n\n\n3.1.5 Done!\nAt this point, you’ve learned the basics of scripting. You are now officially allowed to say that you can program in R, though you probably shouldn’t say it too loudly. There’s a lot more to learn, but nevertheless, if you can write scripts like these then what you are doing is in fact basic programming. The rest of this chapter is devoted to introducing some of the key commands that you need in order to make your programs more powerful; and to help you get used to thinking in terms of scripts, for the rest of this chapter I’ll write up most of my extracts as scripts."
  },
  {
    "objectID": "scripts.html#sec-loops",
    "href": "scripts.html#sec-loops",
    "title": "3  Basic Programming",
    "section": "3.2 Loops",
    "text": "3.2 Loops\nFor all the scripts that we’ve seen so far R starts at the top of the file and runs straight through to the end of the file. However, you actually have quite a lot of flexibility in how and when commands are executed. Depending on how you write the script, you can have R repeat several commands, or skip over different commands, and so on. This topic is referred to as flow control, and the first concept to discuss in this respect is the idea of a loop. The basic idea is very simple: a loop is a block of code (i.e., a sequence of commands) that R will execute over and over again until some termination criterion is met. Looping is a very powerful idea. There are three different ways to construct a loop in R, based on the while, for and repeat functions. I’ll only discuss the first two in this book.\n\n3.2.1 The while loop\nA while loop is a simple thing. The basic format of the loop looks like this:\n     while ( CONDITION ) {\n        STATEMENT1\n        STATEMENT2\n        ETC\n     }\nThe code corresponding to CONDITION needs to produce a logical value, either TRUE or FALSE. Whenever R encounters a while statement, it checks to see if the CONDITION is TRUE. If it is, then R goes on to execute all of the commands inside the curly brackets, proceeding from top to bottom as usual. However, when it gets to the bottom of those statements, it moves back up to the while statement. Then, like the mindless automaton it is, it checks to see if the CONDITION is TRUE. If it is, then R goes on to execute all … well, you get the idea. This continues endlessly until at some point the CONDITION turns out to be FALSE. Once that happens, R jumps to the bottom of the loop (i.e., to the } character), and then continues on with whatever commands appear next in the script.\nTo start with, let’s keep things simple, and use a while loop to calculate the smallest multiple of 17 that is greater than or equal to 1000. This is a very silly example since you can actually calculate it using simple arithmetic operations, but the point here isn’t to do something novel. The point is to show how to write a while loop. Here’s the script:\n\n## --- whileexample.R\nx &lt;- 0\nwhile ( x &lt; 1000 ) {\n  x &lt;- x + 17\n}\nprint( x )\n\nWhen we run this script, R starts at the top and creates a new variable called x and assigns it a value of 0. It then moves down to the loop, and “notices” that the condition here is x &lt; 1000. Since the current value of x is zero, the condition is true, so it enters the body of the loop (inside the curly braces). There’s only one command here which instructs R to increase the value of x by 17. R then returns to the top of the loop, and rechecks the condition. The value of x is now 17. R then returns to the while ( x &lt; 1000 ) { line and notices that x is still less than 1000, and so the loop continues. This cycle will continue for a total of 59 iterations, until finally x reaches a value of 1003 (i.e., \\(59 \\times 17 = 1003\\)). At this point, the loop stops, and R finally reaches the final line of the script, prints out the value of x on screen, and then halts.\n\n\n3.2.2 The for loop\nThe for loop is also pretty simple, though not quite as simple as the while loop. The basic format of this loop goes like this:\n     for ( VAR in VECTOR ) {\n        STATEMENT1\n        STATEMENT2\n        ETC\n     }\nIn a for loop, R runs a fixed number of iterations. We have a VECTOR which has several elements, each one corresponding to a possible value of the variable VAR. In the first iteration of the loop, VAR is given a value corresponding to the first element of VECTOR; in the second iteration of the loop VAR gets a value corresponding to the second value in VECTOR; and so on. Once we’ve exhausted all of the values in VECTOR, the loop terminates and the flow of the program continues down the script.\nOnce again, let’s use some very simple examples. Here is a program that just prints out the word “hello” three times and then stops:\n\n## --- forexample.R\nfor ( i in 1:3 ) {\n  print( \"hello\" )\n}\n\nThis is the simplest example of a for loop. The vector of possible values for the i variable just corresponds to the numbers from 1 to 3. Not only that, the body of the loop doesn’t actually depend on i at all.\nHowever, there’s nothing that stops you from using something non-numeric as the vector of possible values, as the following example illustrates. This time around, we’ll use a character vector to control our loop, which in this case will be a vector of words. And what we’ll do in the loop is get R to convert the word to upper case letters, calculate the length of the word, and print it out. Here’s the script:\n\n## --- forexample2.R\n\n#the words_\nwords &lt;- c(\"it\",\"was\",\"the\",\"dirty\",\"end\",\"of\",\"winter\")\n\n#loop over the words_\nfor ( w in words ) {\n\n  w.length &lt;- nchar( w )     # calculate the number of letters_\n  W &lt;- toupper( w )          # convert the word to upper case letters_\n  msg &lt;- paste( W, \"has\", w.length, \"letters\" )   # a message to print_\n  print( msg )               # print it_\n  \n}\n\nAnd here’s the output:\n\n\n[1] \"IT has 2 letters\"\n[1] \"WAS has 3 letters\"\n[1] \"THE has 3 letters\"\n[1] \"DIRTY has 5 letters\"\n[1] \"END has 3 letters\"\n[1] \"OF has 2 letters\"\n[1] \"WINTER has 6 letters\"\n\n\nPretty straightforward I hope.\n\n\n3.2.3 A more realistic example of a loop\nTo give you a sense of how you can use a loop in a more complex situation, let’s write a simple script to simulate the progression of a mortgage. Suppose we have a nice young couple who borrow $300000 from the bank, at an annual interest rate of 5%. The mortgage is a 30 year loan, so they need to pay it off within 360 months total. Our happy couple decide to set their monthly mortgage payment at $1600 per month. Will they pay off the loan in time or not? Only time will tell.1 Or, alternatively, we could simulate the whole process and get R to tell us. The script to run this is a fair bit more complicated.\n\n## --- mortgage.R\n\n# set up\nmonth &lt;- 0        # count the number of months\nbalance &lt;- 300000 # initial mortgage balance\npayments &lt;- 1600  # monthly payments\ninterest &lt;- 0.05  # 5% interest rate per year\ntotal.paid &lt;- 0   # track what you've paid the bank\n\n# convert annual interest to a monthly multiplier\nmonthly.multiplier &lt;- (1+interest) ^ (1/12)\n\n# keep looping until the loan is paid off...\nwhile ( balance &gt; 0 ) {\n  \n  # do the calculations for this month\n  month &lt;- month + 1  # one more month\n  balance &lt;- balance * monthly.multiplier  # add the interest\n  balance &lt;- balance - payments  # make the payments\n  total.paid &lt;- total.paid + payments # track the total paid\n  \n  # print the results on screen\n  cat( \"month\", month, \": balance\", round(balance), \"\\n\")\n  \n} # end of loop\n\n# print the total payments at the end\ncat(\"total payments made\", total.paid, \"\\n\" )\n\nTo explain what’s going on, let’s go through it carefully. In the first block of code (under #set up) all we’re doing is specifying all the variables that define the problem. The loan starts with a balance of $300,000 owed to the bank on month zero, and at that point in time the total.paid money is nothing. The couple is making monthly payments of $1600, at an annual interest rate of 5%. Next, we convert the annual percentage interest into a monthly multiplier. That is, the number that you have to multiply the current balance by each month in order to produce an annual interest rate of 5%. An annual interest rate of 5% implies that, if no payments were made over 12 months the balance would end up being \\(1.05\\) times what it was originally, so the annual multiplier is \\(1.05\\). To calculate the monthly multiplier, we need to calculate the 12th root of 1.05 (i.e., raise 1.05 to the power of 1/12). We store this value in as the monthly.multiplier variable, which as it happens corresponds to a value of about 1.004. All of which is a rather long winded way of saying that the annual interest rate of 5% corresponds to a monthly interest rate of about 0.4%.\nAll of that is really just setting the stage. It’s not the interesting part of the script. The more interesting part is the loop. The while statement tells R that it needs to keep looping until the balance reaches zero (or less, since it might be that the final payment of $1600 pushes the balance below zero). Then, inside the body of the loop, we have two different blocks of code. In the first bit, we do all the number crunching. Firstly we increase the value month by 1. Next, the bank charges the interest, so the balance goes up. Then, the couple makes their monthly payment and the balance goes down. Finally, we keep track of the total amount of money that the couple has paid so far, by adding the payments to the running tally. After having done all this number crunching, we tell R to issue the couple with a very terse monthly statement, which just indicates how many months they’ve been paying the loan and how much money they still owe the bank. Which is rather rude of us really. I’ve grown attached to this couple and I really feel they deserve better than that. But, that’s banks for you.\nIn any case, the key thing here is the tension between the increase in balance on and the decrease. As long as the decrease is bigger, then the balance will eventually drop to zero and the loop will eventually terminate. If not, the loop will continue forever! This is actually very bad programming on my part: I really should have included something to force R to stop if this goes on too long. However, I haven’t shown you how to evaluate “if” statements yet, so we’ll just have to hope that the author of the book has rigged the example so that the code actually runs. Hm. I wonder what the odds of that are? Anyway, assuming that the loop does eventually terminate, there’s one last line of code that prints out the total amount of money that the couple handed over to the bank over the lifetime of the loan.\nNow that I’ve explained everything in the script in tedious detail, let’s run it and see what happens:\n\n\nmonth 1 : balance 299622 \nmonth 2 : balance 299243 \nmonth 3 : balance 298862 \nmonth 4 : balance 298480 \nmonth 5 : balance 298096 \nmonth 6 : balance 297710 \nmonth 7 : balance 297323 \nmonth 8 : balance 296934 \nmonth 9 : balance 296544 \nmonth 10 : balance 296152 \nmonth 11 : balance 295759 \nmonth 12 : balance 295364 \nmonth 13 : balance 294967 \nmonth 14 : balance 294569 \nmonth 15 : balance 294169 \nmonth 16 : balance 293768 \nmonth 17 : balance 293364 \nmonth 18 : balance 292960 \nmonth 19 : balance 292553 \nmonth 20 : balance 292145 \nmonth 21 : balance 291735 \nmonth 22 : balance 291324 \nmonth 23 : balance 290911 \nmonth 24 : balance 290496 \nmonth 25 : balance 290079 \nmonth 26 : balance 289661 \nmonth 27 : balance 289241 \nmonth 28 : balance 288820 \nmonth 29 : balance 288396 \nmonth 30 : balance 287971 \nmonth 31 : balance 287545 \nmonth 32 : balance 287116 \nmonth 33 : balance 286686 \nmonth 34 : balance 286254 \nmonth 35 : balance 285820 \nmonth 36 : balance 285385 \nmonth 37 : balance 284947 \nmonth 38 : balance 284508 \nmonth 39 : balance 284067 \nmonth 40 : balance 283625 \nmonth 41 : balance 283180 \nmonth 42 : balance 282734 \nmonth 43 : balance 282286 \nmonth 44 : balance 281836 \nmonth 45 : balance 281384 \nmonth 46 : balance 280930 \nmonth 47 : balance 280475 \nmonth 48 : balance 280018 \nmonth 49 : balance 279559 \nmonth 50 : balance 279098 \nmonth 51 : balance 278635 \nmonth 52 : balance 278170 \nmonth 53 : balance 277703 \nmonth 54 : balance 277234 \nmonth 55 : balance 276764 \nmonth 56 : balance 276292 \nmonth 57 : balance 275817 \nmonth 58 : balance 275341 \nmonth 59 : balance 274863 \nmonth 60 : balance 274382 \nmonth 61 : balance 273900 \nmonth 62 : balance 273416 \nmonth 63 : balance 272930 \nmonth 64 : balance 272442 \nmonth 65 : balance 271952 \nmonth 66 : balance 271460 \nmonth 67 : balance 270966 \nmonth 68 : balance 270470 \nmonth 69 : balance 269972 \nmonth 70 : balance 269472 \nmonth 71 : balance 268970 \nmonth 72 : balance 268465 \nmonth 73 : balance 267959 \nmonth 74 : balance 267451 \nmonth 75 : balance 266941 \nmonth 76 : balance 266428 \nmonth 77 : balance 265914 \nmonth 78 : balance 265397 \nmonth 79 : balance 264878 \nmonth 80 : balance 264357 \nmonth 81 : balance 263834 \nmonth 82 : balance 263309 \nmonth 83 : balance 262782 \nmonth 84 : balance 262253 \nmonth 85 : balance 261721 \nmonth 86 : balance 261187 \nmonth 87 : balance 260651 \nmonth 88 : balance 260113 \nmonth 89 : balance 259573 \nmonth 90 : balance 259031 \nmonth 91 : balance 258486 \nmonth 92 : balance 257939 \nmonth 93 : balance 257390 \nmonth 94 : balance 256839 \nmonth 95 : balance 256285 \nmonth 96 : balance 255729 \nmonth 97 : balance 255171 \nmonth 98 : balance 254611 \nmonth 99 : balance 254048 \nmonth 100 : balance 253483 \nmonth 101 : balance 252916 \nmonth 102 : balance 252346 \nmonth 103 : balance 251774 \nmonth 104 : balance 251200 \nmonth 105 : balance 250623 \nmonth 106 : balance 250044 \nmonth 107 : balance 249463 \nmonth 108 : balance 248879 \nmonth 109 : balance 248293 \nmonth 110 : balance 247705 \nmonth 111 : balance 247114 \nmonth 112 : balance 246521 \nmonth 113 : balance 245925 \nmonth 114 : balance 245327 \nmonth 115 : balance 244727 \nmonth 116 : balance 244124 \nmonth 117 : balance 243518 \nmonth 118 : balance 242911 \nmonth 119 : balance 242300 \nmonth 120 : balance 241687 \nmonth 121 : balance 241072 \nmonth 122 : balance 240454 \nmonth 123 : balance 239834 \nmonth 124 : balance 239211 \nmonth 125 : balance 238585 \nmonth 126 : balance 237958 \nmonth 127 : balance 237327 \nmonth 128 : balance 236694 \nmonth 129 : balance 236058 \nmonth 130 : balance 235420 \nmonth 131 : balance 234779 \nmonth 132 : balance 234136 \nmonth 133 : balance 233489 \nmonth 134 : balance 232841 \nmonth 135 : balance 232189 \nmonth 136 : balance 231535 \nmonth 137 : balance 230879 \nmonth 138 : balance 230219 \nmonth 139 : balance 229557 \nmonth 140 : balance 228892 \nmonth 141 : balance 228225 \nmonth 142 : balance 227555 \nmonth 143 : balance 226882 \nmonth 144 : balance 226206 \nmonth 145 : balance 225528 \nmonth 146 : balance 224847 \nmonth 147 : balance 224163 \nmonth 148 : balance 223476 \nmonth 149 : balance 222786 \nmonth 150 : balance 222094 \nmonth 151 : balance 221399 \nmonth 152 : balance 220701 \nmonth 153 : balance 220000 \nmonth 154 : balance 219296 \nmonth 155 : balance 218590 \nmonth 156 : balance 217880 \nmonth 157 : balance 217168 \nmonth 158 : balance 216453 \nmonth 159 : balance 215735 \nmonth 160 : balance 215014 \nmonth 161 : balance 214290 \nmonth 162 : balance 213563 \nmonth 163 : balance 212833 \nmonth 164 : balance 212100 \nmonth 165 : balance 211364 \nmonth 166 : balance 210625 \nmonth 167 : balance 209883 \nmonth 168 : balance 209138 \nmonth 169 : balance 208390 \nmonth 170 : balance 207639 \nmonth 171 : balance 206885 \nmonth 172 : balance 206128 \nmonth 173 : balance 205368 \nmonth 174 : balance 204605 \nmonth 175 : balance 203838 \nmonth 176 : balance 203069 \nmonth 177 : balance 202296 \nmonth 178 : balance 201520 \nmonth 179 : balance 200741 \nmonth 180 : balance 199959 \nmonth 181 : balance 199174 \nmonth 182 : balance 198385 \nmonth 183 : balance 197593 \nmonth 184 : balance 196798 \nmonth 185 : balance 196000 \nmonth 186 : balance 195199 \nmonth 187 : balance 194394 \nmonth 188 : balance 193586 \nmonth 189 : balance 192775 \nmonth 190 : balance 191960 \nmonth 191 : balance 191142 \nmonth 192 : balance 190321 \nmonth 193 : balance 189496 \nmonth 194 : balance 188668 \nmonth 195 : balance 187837 \nmonth 196 : balance 187002 \nmonth 197 : balance 186164 \nmonth 198 : balance 185323 \nmonth 199 : balance 184478 \nmonth 200 : balance 183629 \nmonth 201 : balance 182777 \nmonth 202 : balance 181922 \nmonth 203 : balance 181063 \nmonth 204 : balance 180201 \nmonth 205 : balance 179335 \nmonth 206 : balance 178466 \nmonth 207 : balance 177593 \nmonth 208 : balance 176716 \nmonth 209 : balance 175836 \nmonth 210 : balance 174953 \nmonth 211 : balance 174065 \nmonth 212 : balance 173175 \nmonth 213 : balance 172280 \nmonth 214 : balance 171382 \nmonth 215 : balance 170480 \nmonth 216 : balance 169575 \nmonth 217 : balance 168666 \nmonth 218 : balance 167753 \nmonth 219 : balance 166836 \nmonth 220 : balance 165916 \nmonth 221 : balance 164992 \nmonth 222 : balance 164064 \nmonth 223 : balance 163133 \nmonth 224 : balance 162197 \nmonth 225 : balance 161258 \nmonth 226 : balance 160315 \nmonth 227 : balance 159368 \nmonth 228 : balance 158417 \nmonth 229 : balance 157463 \nmonth 230 : balance 156504 \nmonth 231 : balance 155542 \nmonth 232 : balance 154576 \nmonth 233 : balance 153605 \nmonth 234 : balance 152631 \nmonth 235 : balance 151653 \nmonth 236 : balance 150671 \nmonth 237 : balance 149685 \nmonth 238 : balance 148695 \nmonth 239 : balance 147700 \nmonth 240 : balance 146702 \nmonth 241 : balance 145700 \nmonth 242 : balance 144693 \nmonth 243 : balance 143683 \nmonth 244 : balance 142668 \nmonth 245 : balance 141650 \nmonth 246 : balance 140627 \nmonth 247 : balance 139600 \nmonth 248 : balance 138568 \nmonth 249 : balance 137533 \nmonth 250 : balance 136493 \nmonth 251 : balance 135449 \nmonth 252 : balance 134401 \nmonth 253 : balance 133349 \nmonth 254 : balance 132292 \nmonth 255 : balance 131231 \nmonth 256 : balance 130166 \nmonth 257 : balance 129096 \nmonth 258 : balance 128022 \nmonth 259 : balance 126943 \nmonth 260 : balance 125861 \nmonth 261 : balance 124773 \nmonth 262 : balance 123682 \nmonth 263 : balance 122586 \nmonth 264 : balance 121485 \nmonth 265 : balance 120380 \nmonth 266 : balance 119270 \nmonth 267 : balance 118156 \nmonth 268 : balance 117038 \nmonth 269 : balance 115915 \nmonth 270 : balance 114787 \nmonth 271 : balance 113654 \nmonth 272 : balance 112518 \nmonth 273 : balance 111376 \nmonth 274 : balance 110230 \nmonth 275 : balance 109079 \nmonth 276 : balance 107923 \nmonth 277 : balance 106763 \nmonth 278 : balance 105598 \nmonth 279 : balance 104428 \nmonth 280 : balance 103254 \nmonth 281 : balance 102074 \nmonth 282 : balance 100890 \nmonth 283 : balance 99701 \nmonth 284 : balance 98507 \nmonth 285 : balance 97309 \nmonth 286 : balance 96105 \nmonth 287 : balance 94897 \nmonth 288 : balance 93683 \nmonth 289 : balance 92465 \nmonth 290 : balance 91242 \nmonth 291 : balance 90013 \nmonth 292 : balance 88780 \nmonth 293 : balance 87542 \nmonth 294 : balance 86298 \nmonth 295 : balance 85050 \nmonth 296 : balance 83797 \nmonth 297 : balance 82538 \nmonth 298 : balance 81274 \nmonth 299 : balance 80005 \nmonth 300 : balance 78731 \nmonth 301 : balance 77452 \nmonth 302 : balance 76168 \nmonth 303 : balance 74878 \nmonth 304 : balance 73583 \nmonth 305 : balance 72283 \nmonth 306 : balance 70977 \nmonth 307 : balance 69666 \nmonth 308 : balance 68350 \nmonth 309 : balance 67029 \nmonth 310 : balance 65702 \nmonth 311 : balance 64369 \nmonth 312 : balance 63032 \nmonth 313 : balance 61688 \nmonth 314 : balance 60340 \nmonth 315 : balance 58986 \nmonth 316 : balance 57626 \nmonth 317 : balance 56261 \nmonth 318 : balance 54890 \nmonth 319 : balance 53514 \nmonth 320 : balance 52132 \nmonth 321 : balance 50744 \nmonth 322 : balance 49351 \nmonth 323 : balance 47952 \nmonth 324 : balance 46547 \nmonth 325 : balance 45137 \nmonth 326 : balance 43721 \nmonth 327 : balance 42299 \nmonth 328 : balance 40871 \nmonth 329 : balance 39438 \nmonth 330 : balance 37998 \nmonth 331 : balance 36553 \nmonth 332 : balance 35102 \nmonth 333 : balance 33645 \nmonth 334 : balance 32182 \nmonth 335 : balance 30713 \nmonth 336 : balance 29238 \nmonth 337 : balance 27758 \nmonth 338 : balance 26271 \nmonth 339 : balance 24778 \nmonth 340 : balance 23279 \nmonth 341 : balance 21773 \nmonth 342 : balance 20262 \nmonth 343 : balance 18745 \nmonth 344 : balance 17221 \nmonth 345 : balance 15691 \nmonth 346 : balance 14155 \nmonth 347 : balance 12613 \nmonth 348 : balance 11064 \nmonth 349 : balance 9509 \nmonth 350 : balance 7948 \nmonth 351 : balance 6380 \nmonth 352 : balance 4806 \nmonth 353 : balance 3226 \nmonth 354 : balance 1639 \nmonth 355 : balance 46 \nmonth 356 : balance -1554 \n\n\ntotal payments made 569600 \n\n\nSo our nice young couple have paid off their $300,000 loan in just 4 months shy of the 30 year term of their loan, at a bargain basement price of $568,046 (since 569600 - 1554 = 568046). A happy ending!"
  },
  {
    "objectID": "scripts.html#sec-if",
    "href": "scripts.html#sec-if",
    "title": "3  Basic Programming",
    "section": "3.3 Conditional statements",
    "text": "3.3 Conditional statements\nA second kind of flow control that programming languages provide is the ability to evaluate conditional statements. Unlike loops, which can repeat over and over again, a conditional statement only executes once, but it can switch between different possible commands depending on a CONDITION that is specified by the programmer. The power of these commands is that they allow the program itself to make choices, and in particular, to make different choices depending on the context in which the program is run. The most prominent of example of a conditional statement is the if statement, and the accompanying else statement. The basic format of an if statement in R is as follows:\n     if ( CONDITION ) {\n        STATEMENT1\n        STATEMENT2\n        ETC\n     }\nAnd the execution of the statement is pretty straightforward. If the CONDITION is true, then R will execute the statements contained in the curly braces. If the CONDITION is false, then it dose not. If you want to, you can extend the if statement to include an else statement as well, leading to the following syntax:\n     if ( CONDITION ) {\n        STATEMENT1\n        STATEMENT2\n        ETC\n     } else {\n        STATEMENT3\n        STATEMENT4\n        ETC\n     }     \nAs you’d expect, the interpretation of this version is similar. If the CONDITION is true, then the contents of the first block of code (i.e., STATEMENT1, STATEMENT2, ETC) are executed; but if it is false, then the contents of the second block of code (i.e., STATEMENT3, STATEMENT4, ETC) are executed instead.\nTo give you a feel for how you can use if and else to do something useful, the example that I’ll show you is a script that prints out a different message depending on what day of the week you run it. Here’s the script:\n\n## --- ifelseexample.R\n# find out what day it is...\ntoday &lt;- now()       # pull the date from the system clock\nday &lt;- weekdays(today) # what day of the week it is_\n\n\n# now make a choice depending on the day...\nif ( day == \"Monday\" ) {\n  print( \"I don't like Mondays\" )\n} else {\n  print( \"I'm a happy little automaton\" )\n}\n\nSince today happens to be a Thursday, when I run the script here’s what happens:\n\n\n[1] \"I'm a happy little automaton\"\n\n\nThere are other ways of making conditional statements in R. In particular, the ifelse() function and the switch() functions can be very useful in different contexts. However, my main aim in this chapter is to briefly cover the very basics, so I’ll move on."
  },
  {
    "objectID": "scripts.html#sec-writingfunctions",
    "href": "scripts.html#sec-writingfunctions",
    "title": "3  Basic Programming",
    "section": "3.4 Writing functions",
    "text": "3.4 Writing functions\nIn this section I want to talk about functions again. Functions were introduced in Section 2.4, but you’ve learned a lot about R since then, so we can talk about them in more detail. In particular, I want to show you how to create your own. To stick with the same basic framework that I used to describe loops and conditionals, here’s the syntax that you use to create a function:\n     FNAME &lt;- function ( ARG1, ARG2, ETC ) {\n        STATEMENT1\n        STATEMENT2\n        ETC\n        return( VALUE )\n     }\nWhat this does is create a function with the name FNAME, which has arguments ARG1, ARG2 and so forth. Whenever the function is called, R executes the statements in the curly braces, and then outputs the contents of VALUE to the user. Note, however, that R does not execute the commands inside the function in the workspace. Instead, what it does is create a temporary local environment: all the internal statements in the body of the function are executed there, so they remain invisible to the user. Only the final results in the VALUE are returned to the workspace.\nTo give a simple example of this, let’s create a function called quadruple() which multiplies its inputs by four. In keeping with the approach taken in the rest of the chapter, I’ll use a script to do this:\n\n## --- functionexample.R\nquadruple &lt;- function(x) {\n  y &lt;- x*4\n  return(y)\n} \n\nWhen we run this script, as follows\nNothing appears to have happened, but there is a new object created in the workspace called quadruple. Not surprisingly, if we ask R to tell us what kind of object it is, it tells us that it is a function:\n\nclass( quadruple )\n\n[1] \"function\"\n\n\nAnd now that we’ve created the quadruple() function, we can call it just like any other function. And if I want to store the output as a variable, I can do this:\n\nmy.var &lt;- quadruple(10)\nprint(my.var)\n\n[1] 40\n\n\nAn important thing to recognise here is that the two internal variables that the quadruple() function makes use of, x and y, stay “internal” to that function. If we inspect the contents of the workspace using ls(), everything else we have created in our workspace, including the quadruple() function itself as well as the my.var variable that we just created. But will will not see the x and y variables used inside our quadruple function. And, if we happened to have created variables named x or y earlier (e.g., before writing our quadruple function), the x and y variables we see in our workspace would not be the variables used by quadruple(). Got all that?\nNow that we know how to create our own functions in R, it’s probably a good idea to talk a little more about some of the other properties of functions that I’ve been glossing over. To start with, let’s take this opportunity to type the name of the function at the command line without the parentheses:\n\nquadruple\n\nfunction(x) {\n  y &lt;- x*4\n  return(y)\n}\n\n\nAs you can see, when you type the name of a function at the command line, R prints out the underlying source code that we used to define the function in the first place. In the case of the quadruple() function, this is quite helpful to us – we can read this code and actually see what the function does. For other functions, this is less helpful, as we saw back in Section 2.4 when we tried typing citation rather than citation().\n\n3.4.1 Function arguments revisited\nOkay, now that we are starting to get a sense for how functions are constructed, let’s have a look at two, slightly more complicated functions that I’ve created. The source code for these functions is contained within the functionexample2.R and functionexample3.R scripts. Let’s start by looking at the first one:\n\n## --- functionexample2.R\npow &lt;- function( x, y = 1) {\n  out &lt;- x^y  # raise x to the power y\n  return( out )\n}\n\nand if we type source(\"functionexample2.R\") to load the pow() function into our workspace, then we can make use of it. As you can see from looking at the code for this function, it has two arguments x and y, and all it does is raise x to the power of y. For instance, this command\n\npow(x=3, y=2)\n\n[1] 9\n\n\ncalculates the value of \\(3^2\\). The interesting thing about this function isn’t what it does, because R already has has perfectly good mechanisms for calculating powers. Rather, notice that when we defined the function, we specified y=1 when listing the arguments? That’s the default value for y. So if we enter a command without specifying a value for y, then the function assumes that we want y=1:\n\npow(x=3)\n\n[1] 3\n\n\nHowever, since we didn’t specify any default value for x when we defined the pow() function, we always need to input a value for x. If we don’t R will spit out an error message.\nSo now you know how to specify default values for an argument. The other thing I should point out while I’m on this topic is the use of the ... argument. The ... argument is a special construct in R which is only used within functions. It is used as a way of matching against multiple user inputs: in other words, ... is used as a mechanism to allow the user to enter as many inputs as they like. I won’t talk at all about the low-level details of how this works at all, but I will show you a simple example of a function that makes use of it. To that end, consider the following script:\n\n## --- functionexample3.R\ndoubleMax &lt;- function( ... ) {  \n  max.val &lt;- max( ... )   # find the largest value in ... \n  out &lt;- 2 * max.val      # double it\n  return( out )\n}\n\nIf we then typed source(\"functionexample3.R\"), R would create the doubleMax() function. You could type in as many inputs as you like. The doubleMax() function would identifies the largest value in the inputs, by passing all the user inputs to the max() function, and then double it. For example:\n\ndoubleMax( 1,2,5 )\n\n[1] 10\n\n\n\n\n3.4.2 There’s more to functions than this\nThere’s a lot of other details to functions that I’ve hidden in my description in this chapter. Experienced programmers will wonder exactly how the “scoping rules” work in R or want to know how to use a function to create variables in other environments, or if function objects can be assigned as elements of a list, and probably hundreds of other things besides. However, I don’t want to have this discussion get too cluttered with details, so I think it’s best – at least for the purposes of the current book – to stop here."
  },
  {
    "objectID": "scripts.html#sec-vectorised",
    "href": "scripts.html#sec-vectorised",
    "title": "3  Basic Programming",
    "section": "3.5 Implicit loops",
    "text": "3.5 Implicit loops\nThere’s one last topic I want to discuss in this chapter. In addition to providing the explicit looping structures via while and for, R also provides a collection of functions for implicit loops. What I mean by this is that these are functions that carry out operations very similar to those that you’d normally use a loop for. However, instead of typing out the whole loop, the whole thing is done with a single command. The main reason why this can be handy is that – due to the way that R is written – these implicit looping functions are usually about to do the same calculations much faster than the corresponding explicit loops. In most applications that beginners might want to undertake, this probably isn’t very important, since most beginners tend to start out working with fairly small data sets and don’t usually need to undertake extremely time consuming number crunching. However, because you often see these functions referred to in other contexts, it may be useful to very briefly discuss a few of them.\nThe first and simplest of these functions is sapply(). The two most important arguments to this function are X, which specifies a vector containing the data, and FUN, which specifies the name of a function that should be applied to each element of the data vector. The following example illustrates the basics of how it works:\n\nwords &lt;- c(\"along\", \"the\", \"loom\", \"of\", \"the\", \"land\")\nsapply( X = words, FUN = nchar )\n\nalong   the  loom    of   the  land \n    5     3     4     2     3     4 \n\n\nNotice how similar this is to the second example of a for loop in Section 3.2.2. The sapply() function has implicitly looped over the elements of words, and for each such element applied the nchar() function to calculate the number of letters in the corresponding word.\nThe second of these functions is tapply(), which has three key arguments. As before X specifies the data, and FUN specifies a function. However, there is also an INDEX argument which specifies a grouping variable.2 What the tapply() function does is loop over all of the different values that appear in the INDEX variable. Each such value defines a group: the tapply() function constructs the subset of X that corresponds to that group, and then applies the function FUN to that subset of the data. This probably sounds a little abstract, so let’s consider a specific example:\n\ngender &lt;- c( \"male\",\"male\",\"female\",\"female\",\"male\" )\nage &lt;- c( 10,12,9,11,13 )\ntapply( X = age, INDEX = gender, FUN = mean )\n\n  female     male \n10.00000 11.66667 \n\n\nIn this extract, what we’re doing is using gender to define two different groups of people, and using their ages as the data. We then calculate the mean() of the ages, separately for the males and the females. A closely related function is by(). It actually does the same thing as tapply(), but the output is formatted a bit differently. This time around the three arguments are called data, INDICES and FUN, but they’re pretty much the same thing. An example of how to use the by() function is shown in the following extract:\n\nby( data = age, INDICES = gender, FUN = mean )\n\ngender: female\n[1] 10\n------------------------------------------------------------ \ngender: male\n[1] 11.66667\n\n\nThe tapply() and by() functions are quite handy things to know about, and are pretty widely used. However, although I do make passing reference to the tapply() later on, I don’t make much use of them in this book.\nBefore moving on, I should mention that there are several other functions that work along similar lines, and have suspiciously similar names: lapply, mapply, apply, vapply, rapply and eapply. However, none of these come up anywhere else in this book, so all I wanted to do here is draw your attention to the fact that they exist."
  },
  {
    "objectID": "scripts.html#footnotes",
    "href": "scripts.html#footnotes",
    "title": "3  Basic Programming",
    "section": "",
    "text": "Okay, fine. This example is still a bit ridiculous, in three respects. Firstly, the bank absolutely will not let the couple pay less than the amount required to terminate the loan in 30 years. Secondly, a constant interest rate of 30 years is hilarious. Thirdly, you can solve this much more efficiently than through brute force simulation. However, we’re not exactly in the business of being realistic or efficient here.↩︎\nOr a list of such variables.↩︎"
  },
  {
    "objectID": "data.html#sec-load",
    "href": "data.html#sec-load",
    "title": "4  Working with Data",
    "section": "4.1 Loading and saving data",
    "text": "4.1 Loading and saving data\nThere are several different types of files that are likely to be relevant to us when doing data analysis. There are three in particular that are especially important from the perspective of this book:\n\nComma separated value (CSV) files are those with a .csv file extension. These are just regular old text files, and they can be opened with almost any software. This means that storing data in CSV files does not tie users to any particular software and keeps things simple.\nWorkspace files are those with a .Rdata file extension. This is the standard kind of file that R uses to store data and variables. They’re called “workspace files” because you can use them to save your whole workspace.\n\n\n4.1.1 Importing data from CSV files using read_csv\nOne quite commonly used data format is the humble “comma separated value” file, also called a CSV file, and usually bearing the file extension .csv. CSV files are just plain old-fashioned text files, and what they store is basically just a table of data. This is illustrated in Figure Figure 4.1, which shows a file called booksales.csv that I’ve created. As you can see, each row corresponds to a variable, and each row represents the book sales data for one month. The first row doesn’t contain actual data though: it has the names of the variables.\n\n\n\nFigure 4.1: The booksales.csv data file. On the left, I’ve opened the file in using a spreadsheet program (OpenOffice), which shows that the file is basically a table. On the right, the same file is open in a standard text editor (the TextEdit program on a Mac), which shows how the file is formatted. The entries in the table are wrapped in quote marks and separated by commas.\n\n\nIf RStudio were not available to you, the easiest way to open this file would be to use the read.csv() function. This function is pretty flexible, and I’ll talk a lot more about it’s capabilities in Section 4.1.2 for more details, but for now there’s only two arguments to the function that I’ll mention:\n\nfile. This should be a character string that specifies a path to the file that needs to be loaded. You can use an absolute path or a relative path to do so.\nheader. This is a logical value indicating whether or not the first row of the file contains variable names. The default value is TRUE.\n\nTherefore, to import the CSV file, the command I need is:\n\nbooks &lt;- read_csv(\"./data/booksales.csv\")\n\nThere are two very important points to notice here. Firstly, notice that I didn’t try to use the load() function, because that function is only meant to be used for .Rdata files. If you try to use load() on other types of data, you get an error. Secondly, notice that when I imported the CSV file I assigned the result to a variable, which I imaginatively called books file. There’s a reason for this. The idea behind an .Rdata file is that it stores a whole workspace. So, if you had the ability to look inside the file yourself you’d see that the data file keeps track of all the variables and their names. So when you load() the file, R restores all those original names. CSV files are treated differently: as far as R is concerned, the CSV only stores one variable, but that variable is big table. So when you import that table into the workspace, R expects you to give it a name.] Let’s have a look at what we’ve got:\n\nprint(books)\n\n# A tibble: 12 × 4\n   Month      Days Sales Stock.Levels\n   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n 1 January      31     0 high        \n 2 February     28   100 high        \n 3 March        31   200 low         \n 4 April        30    50 out         \n 5 May          31     0 out         \n 6 June         30     0 high        \n 7 July         31     0 high        \n 8 August       31     0 high        \n 9 September    30     0 high        \n10 October      31     0 high        \n11 November     30     0 high        \n12 December     31     0 high        \n\n\nYou can instead open the data in RStudio’s spreadsheet viewer:\n\nView(books)\n\nThe books data set is quite small, so just calling print() is fine. But for larger data sets, the View() and the spreadsheet viewer allows for a more thorough inspection.\nClearly, it’s worked, but the format of this output is a bit unfamiliar. We haven’t seen anything like this before. What you’re looking at is a data frame, which is a very important kind of variable in R, and one I’ll discuss at length in Chapter 6. For now, let’s just be happy that we imported the data and that it looks about right.\n\n\n\n\n\n\nNote\n\n\n\nIn a lot of books you’ll see the read.table() or read.csv() functions used for this purpose instead of read_csv(). They’re similar functions, but read_csv() is a function from the readr package, one of the many packages that make up “the tidyverse”. The tidyverse is a set of interoperable packages we will be using throughout this course. Further information about the tidyverse can be found in Chapter 5.\n\n\n\n\n4.1.2 Importing data from CSV files using RStudio\n\n\n\n\n\n\nWarning\n\n\n\nAs detailed above, read_csv() is how we will import CSV data files into R. This section details an alternative way to import CSV data files, but it will not produce the same kind of variable. Using RStudio’s built-in “Import Dataset” functionality will produce a dataframe, whereas read_csv produces a “tibble”. We will Further information about the difference between plain dataframes and tibbles can be found in Chapter 6.\n\n\nIn the environment panel in RStudio you should see a button called “Import Dataset”. Click on that, and it will give you a couple of options: select the “From Text File…” option, and it will open up a very familiar dialog box asking you to select a file: if you’re on a Mac, it’ll look like the usual Finder/Explorer window that you use to choose a file. I’m assuming that you’re familiar with your own computer, so you should have no problem finding the CSV file that you want to import! Find the one you want, then click on the “Open” button. When you do this, you’ll see a window that looks like the one in Figure 4.2.\n\n\n\nFigure 4.2: A dialog box on a Mac asking you to select the CSV file R should try to import. Mac users will recognise this immediately: it’s the usual way in which a Mac asks you to find a file. Windows users won’t see this: they’ll see the usual explorer window that Windows always gives you when it wants you to select a file.\n\n\nThe import data set window is relatively straightforward to understand.\n\n\n\nThe RStudio window for importing a CSV file into R.\n\n\nIn the top left corner, you need to type the name of the variable you R to create. By default, that will be the same as the file name: our file is called booksales.csv, so RStudio suggests the name booksales. If you’re happy with that, leave it alone. If not, type something else. Immediately below this are a few things that you can tweak to make sure that the data gets imported correctly:\n\nHeading. Does the first row of the file contain raw data, or does it contain headings for each variable? The booksales.csv file has a header at the top, so I selected “yes”.\nSeparator. What character is used to separate different entries? In most CSV files this will be a comma (it is “comma separated” after all). But you can change this if your file is different.\nDecimal. What character is used to specify the decimal point? In English speaking countries, this is almost always a period (i.e., .). That’s not universally true: many European countries use a comma. So you can change that if you need to.\nQuote. What character is used to denote a block of text? That’s usually going to be a double quote mark. It is for the booksales.csv file, so that’s what I selected.\n\nOne nice thing about the RStudio window is that it shows you the raw data file at the top of the window, and it shows you a preview of the data at the bottom. If the data at the bottom doesn’t look right, try changing some of the settings on the left hand side. Once you’re happy, click “Import”. When you do, two commands appear in the R console:\n\nbooksales &lt;- read.csv(\"~/Rbook/data/booksales.csv\")\nView(booksales)\n\nThe first of these commands is the one that loads the data. The second one will display a pretty table showing the data in RStudio.\nNote, however, that this variable looks a bit different from the sales variable we created using read_csv. This is because RStudio has used read.csv() rather than read_csv() as we did in the previous section. Again, the differences will be discussed more in Chapter 6.\n\n\n4.1.3 Loading workspace files using R\nWhen I used the list.files() command to list the contents of the /Users/dan/Rbook/data directory, the output referred to a file called booksales.Rdata. Let’s say I want to load the data from this file into my workspace. The way I do this is with the load() function. There are two arguments to this function, but the only one we’re interested in is\n\nfile. This should be a character string that specifies a path to the file that needs to be loaded. You can use an absolute path or a relative path to do so.\n\nUsing the absolute file path, the command would look like this:\n\nload( file = \"/home/christian/Documents/teaching/statslab/data/booksales.Rdata\" )\n\nbut this is long and ugly. Given that the working directory is /home/christian/Documents/teaching/statslab, I could use a relative file path, like so:\n\nload( file = \"./data/booksales.Rdata\" )\n\nAnother strategy would be to first change the working directory to whatever directory contains the desired file (setwd() or Session-&gt;Set Working Directory) and then load the file using only the file name (e.g., load(file = \"booksales.Rdata\")). This may seem tempting, but we will avoid doing this because changing the working directory can be confusing if you’re not paying close attention. Instead, we will assume that a) our working directory is the “statslab” directory we created previously (Section 5.4) and b) all of our data files (CSVs and workspaces) are located in the “data” subdirectory. Outside of this class, however, you may not be able to move data files and/or R scripts. In such cases, it may be easier to modify the working directory. Just be aware that you may need to change the working directory both before and after loading the desired file(s). Don’t get lost (you can always figure out what the current working directory is with getwd())!\n\n\n4.1.4 Loading workspace files using RStudio\nOkay, so how do we open an .Rdata file using the RStudio file panel? It’s terribly simple. First, use the file panel to find the folder that contains the file you want to load. If you look at Figure Figure 4.3, you can see that there are several .Rdata files listed. Let’s say I want to load the booksales.Rdata file. All I have to do is click on the file name. RStudio brings up a little dialog box asking me to confirm that I do want to load this file. I click yes. The following command then turns up in the console,\n\nload(\"./data/booksales.Rdata\")\n\nand the new variables will appear in the workspace (you’ll see them in the Environment panel in RStudio, or if you type who()). So easy it barely warrants having its own section.\n\n\n\nFigure 4.3: The file panel is the area shown in the lower right hand corner. It provides a very easy way to browse and navigate your computer using R. See main text for details.\n\n\n\n\n4.1.5 Saving a workspace file using save\nNot surprisingly, saving data is very similar to loading data. Although RStudio provides a simple way to save files (see below), it’s worth understanding the actual commands involved. There are two commands you can use to do this, save() and save.image(). If you’re happy to save all of the variables in your workspace into the data file, then you should use save.image(). And if you’re happy for R to save the file into the current working directory, all you have to do is this:\n\nsave.image( file = \"myfile.Rdata\" )\n\nSince file is the first argument, you can shorten this to save.image(\"myfile.Rdata\"); and if you want to save to a different directory, then (as always) you need to be more explicit about specifying the path to the file. Suppose, however, I have several variables in my workspace, and I only want to save some of them. For instance, I might have this as my workspace:\n\nwho()\n##   -- Name --   -- Class --   -- Size --\n##   data         data.frame    3 x 2     \n##   handy        character     1         \n##   junk         numeric       1        \n\nI want to save data and handy, but not junk. But I don’t want to delete junk right now, because I want to use it for something else later on. This is where the save() function is useful, since it lets me indicate exactly which variables I want to save. Here is one way I can use the save function to solve my problem:\n\nsave(data, handy, file = \"myfile.Rdata\")\n\nImportantly, you must specify the name of the file argument. The reason is that if you don’t do so, R will think that \"myfile.Rdata\" is actually a variable that you want to save, and you’ll get an error message. Finally, I should mention a second way to specify which variables the save() function should save, which is to use the list argument. You do so like this:\n\nvars_to_save &lt;- c(\"data\", \"handy\")   # the variables to be saved\nsave( file = \"booksales2.Rdata\", list = vars_to_save )   # the command to save them\n\n\n\n4.1.6 Saving a workspace file using RStudio\nRStudio allows you to save the workspace pretty easily. In the environment panel ((fig:workspace?)) you can see the “save” button. There’s no text, but it’s the same icon that gets used on every computer everywhere: it’s the one that looks like a floppy disk. You know, those things that haven’t been used in about 20 years.\n\n\n\nFigure 4.4: The RStudio Environment panel shows you the contents of the workspace. The view shown above is the list view. To switch to the grid view, click on the menu item on the top right that currently reads list. Select grid from the dropdown menu, and then it will switch to a view like the one shown in the other workspace figure\n\n\nAlternatively, go to the “Session” menu and click on the “Save Workspace As…” option. This will bring up the standard “save” dialog box for your operating system. Type in the name of the file that you want to save it to, and all the variables in your workspace will be saved to disk. You’ll see an R command like this:\n\nsave.image(\"~/Desktop/Untitled.RData\")\n\nPretty straightforward, really."
  },
  {
    "objectID": "data.html#sec-dataactivities",
    "href": "data.html#sec-dataactivities",
    "title": "4  Working with Data",
    "section": "4.2 Activities",
    "text": "4.2 Activities\n\nDownload data\nDownload the following file: TBD\nInside the “statslab” directory/folder you created at the end of the previous chapter (Section 5.4), create a sub-directory/folder and name it “data”\nUnzip the data files and place them in the “data” directory/folder you just created\nOpen the 2015 data file (nba_all_seasons.csv) in your favorite spreadsheet software (e.g., Microsoft Excel)\nBriefly review the content of this file."
  },
  {
    "objectID": "tidyverse.html#sec-tidy-data-ex",
    "href": "tidyverse.html#sec-tidy-data-ex",
    "title": "5  The Tidyverse",
    "section": "5.1 “Tidy” data",
    "text": "5.1 “Tidy” data\nLet’s now learn about the concept of “tidy” data format.\n\n5.1.1 Definition of “tidy” data\nYou have surely heard the word “tidy” in your life:\n\n“Tidy up your room!”\n“Write your homework in a tidy way so it is easier to provide feedback.”\nMarie Kondo’s best-selling book, The Life-Changing Magic of Tidying Up: The Japanese Art of Decluttering and Organizing, and Netflix TV series Tidying Up with Marie Kondo.\n“I am not by any stretch of the imagination a tidy person, and the piles of unread books on the coffee table and by my bed have a plaintive, pleading quality to me - ‘Read me, please!’” - Linda Grant\n\nWhat does it mean for your data to be “tidy”? While “tidy” has a clear English meaning of “organized,” the word “tidy” in data science using R means that your data follows a standardized format. We will follow Hadley Wickham’s definition of “tidy” data:\n\nA dataset is a collection of values, usually either numbers (if quantitative) or strings AKA text data (if qualitative/categorical). Values are organised in two ways. Every value belongs to a variable and an observation. A variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units. An observation contains all values measured on the same unit (like a person, or a day, or a city) across attributes.\n“Tidy” data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\n\n\n\n\nTidy data\n\n\nFor example, say you have the following table of stock prices:\n\n\n\nStock prices (non-tidy format)\n\n\nDate\nBoeing stock price\nAmazon stock price\nGoogle stock price\n\n\n\n\n2009-01-01\n$173.55\n$174.90\n$174.34\n\n\n2009-01-02\n$172.61\n$171.42\n$170.04\n\n\n\n\n\n\n\nAlthough the data are neatly organized in a rectangular spreadsheet-type format, they do not follow the definition of data in “tidy” format. While there are three variables corresponding to three unique pieces of information (date, stock name, and stock price), there are not three columns. In “tidy” data format, each variable should be its own column. Notice that both tables present the same information, but in different formats.\n\n\n\nStock prices (tidy format)\n\n\nDate\nStock Name\nStock Price\n\n\n\n\n2009-01-01\nBoeing\n$173.55\n\n\n2009-01-01\nAmazon\n$174.90\n\n\n2009-01-01\nGoogle\n$174.34\n\n\n2009-01-02\nBoeing\n$172.61\n\n\n2009-01-02\nAmazon\n$171.42\n\n\n2009-01-02\nGoogle\n$170.04\n\n\n\n\n\n\n\nNow we have the requisite three columns Date, Stock Name, and Stock Price. On the other hand, consider this data:\n\n\n\nExample of tidy data\n\n\nDate\nBoeing Price\nWeather\n\n\n\n\n2009-01-01\n$173.55\nSunny\n\n\n2009-01-02\n$172.61\nOvercast\n\n\n\n\n\n\n\nIn this case, even though the variable “Boeing Price” occurs just like in our non-“tidy” data, the data is “tidy” since there are three variables corresponding to three unique pieces of information: Date, Boeing price, and the Weather that particular day."
  },
  {
    "objectID": "tidyverse.html#sec-tidyverse-package",
    "href": "tidyverse.html#sec-tidyverse-package",
    "title": "5  The Tidyverse",
    "section": "5.2 tidyverse package",
    "text": "5.2 tidyverse package\nThe following four packages, which are among four of the most frequently used R packages for data science, will be heavily used throughout the book: ggplot2, dplyr, readr, and tidyr.\nThe ggplot2 package is for data visualization, dplyr is for data wrangling, readr is for importing CSV files into R (we used it in Section 4.1.1), and tidyr is for converting data to “tidy” format. There is a much quicker way to load these packages than by individually loading them: by installing and loading the tidyverse package. The tidyverse package acts as an “umbrella” package whereby installing/loading it will install/load multiple packages at once for you.\nAfter installing the tidyverse package as you would a normal package (see Section 1.5), running:\n\nlibrary(tidyverse)\n\naccomplishes the the same things as running:\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(stringr)\nlibrary(forcats)\n\nFor the remainder of this book, we’ll start every chapter by running library(tidyverse) instead of loading the various component packages individually. The tidyverse “umbrella” package gets its name from the fact that all the functions in all its packages are designed to have common inputs and outputs: data frames that are in “tidy” format. This standardization of input and output makes transitions between different functions in the different packages as seamless as possible. For more information (including the more advanced packages, purrr, tibble, stringr, and forcats), check out the tidyverse.org webpage for the package and the book ggplot2: Elegant Graphics ofr Data Analysis."
  },
  {
    "objectID": "tidyverse.html#sec-tidymodels-package",
    "href": "tidyverse.html#sec-tidymodels-package",
    "title": "5  The Tidyverse",
    "section": "5.3 tidymodels package",
    "text": "5.3 tidymodels package\nIn addition to the packages listed above, there are also a variety of packages that live in a sort of “extended” tidyverse. These include packages such as lubridate (for handling dates and times) and magrittr (providing pipe operators). One particularly useful package is modelr which is used for for creating analysis pipelines. However, the modelr package has been superseded by a collection of packages called tidymodels. Like tidyverse, tidymodels is an umbrella package encapsulating: rsample, parsnip, recipes, broom and other packages designed to facilitate machine learning-style data analysis. We will make particular use of a package called infer, which provides a high-level interface for performing statistical inference. For more information, check out www.tidymodels.org and the book Tidy Modeling with R."
  },
  {
    "objectID": "tidyverse.html#sec-rlangactivities",
    "href": "tidyverse.html#sec-rlangactivities",
    "title": "5  The Tidyverse",
    "section": "5.4 Activities",
    "text": "5.4 Activities\n\nOpen RStudio\nIf you have not done so already, install the tidyverse package using the instructions found in Section 1.5.1\nVerify installation of the tidyverse package by loading the package using the instructions found in Section 1.5.2\nCheck out all the cheatsheets available for tidyverse package by going to the Help menu and selecting “Cheat Sheets”. You can look over the readr cheat sheet for now, but remember this resource as we will be diving into the other tidyverse packages (e.g., dplyr, ggplot2) in future chapters."
  },
  {
    "objectID": "tibbles.html#what-is-a-tibble",
    "href": "tibbles.html#what-is-a-tibble",
    "title": "6  Dataframes",
    "section": "6.1 What is a tibble?",
    "text": "6.1 What is a tibble?\nA tibble is a type of dataframe commonly used in the tidyverse. Tibbles contain the same basic information as a corresponding dataframe, but tibbles are slightly different in a variety of minor ways. Let’s take a look at the iris data represented as a tibble. We’ll again confine our inspection to the first five rows:\n\n\n# A tibble: 5 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1          5.1         3.5          1.4         0.2 setosa \n2          4.9         3            1.4         0.2 setosa \n3          4.7         3.2          1.3         0.2 setosa \n4          4.6         3.1          1.5         0.2 setosa \n5          5           3.6          1.4         0.2 setosa \n\n\nNote that the data is identical to that seen above. However, the tibble provides some additional useful information. It tells us that this particular tibble has five columns and, because we only asked for the first five rows, also has five rows. In addition, it tells us about the data type of each column. The first four columns are of type dbl, which is double-precision floating point number (a decimal). The last column is of type fct, or factor. In R, factors are used to represent categorical variables, variables that have a fixed set of possible values (in this case, the set of species)."
  },
  {
    "objectID": "tibbles.html#tidying-your-data",
    "href": "tibbles.html#tidying-your-data",
    "title": "6  Dataframes",
    "section": "6.2 Tidying your data",
    "text": "6.2 Tidying your data\nFor the rest of this book, we will primarily deal with data that is already in “tidy” format as explained Chapter 5. However, many data sets you encounter in the world are in so-called “wide” format. If you wish to use the tidyverse packages, you will first have to convert these date sets to “tidy” format. To do so, we recommend using the pivot_longer() function in the tidyr package.\nTo illustrated, let’s load some data from the fivethirtyeight package. The fivethirtyeight package provides access to the data sets used in many articles published by the data journalism website, FiveThirtyEight.com. For a complete list of all data sets included in the fivethirtyeight package, check out the package webpage.\nLet’s focus our attention on the drinks dataframe:\n\n\n# A tibble: 5 × 5\n  country     beer_servings spirit_servings wine_servings total_litres_of_pure…¹\n  &lt;chr&gt;               &lt;int&gt;           &lt;int&gt;         &lt;int&gt;                  &lt;dbl&gt;\n1 Afghanistan             0               0             0                    0  \n2 Albania                89             132            54                    4.9\n3 Algeria                25               0            14                    0.7\n4 Andorra               245             138           312                   12.4\n5 Angola                217              57            45                    5.9\n# ℹ abbreviated name: ¹​total_litres_of_pure_alcohol\n\n\nAfter reading the help file (i.e., by running ?drinks), you’ll see that drinks is a dataframe containing results from a survey of beer, spirits, and wine consumption originally reported on FiveThirtyEight.com in Mona Chalabi’s article: “Dear Mona Followup: Where Do People Drink The Most Beer, Wine And Spirits?”.\nLet’s narrow down the data a bit. We’ll only consider 4 countries (the United States, China, Italy, and Saudi Arabia), omit the total_litres_of_pure_alcohol variable, and rename the other variables to something a bit more convenient. Don’t worry about the code here. We’ll get into all of these operations more in Chapter 7.\n\ndrinks_smaller &lt;- drinks %&gt;% \n  filter(country %in% c(\"USA\", \"China\", \"Italy\", \"Saudi Arabia\")) %&gt;% \n  select(-total_litres_of_pure_alcohol) %&gt;% \n  rename(beer = beer_servings, spirit = spirit_servings, wine = wine_servings)\ndrinks_smaller\n\n# A tibble: 4 × 4\n  country       beer spirit  wine\n  &lt;chr&gt;        &lt;int&gt;  &lt;int&gt; &lt;int&gt;\n1 China           79    192     8\n2 Italy           85     42   237\n3 Saudi Arabia     0      5     0\n4 USA            249    158    84\n\n\nNote that this data is not in “tidy” format. However, we can convert it to tidy format by using the pivot_longer() function from the tidyr package as follows:\n\ndrinks_smaller_tidy &lt;- drinks_smaller %&gt;% \n  pivot_longer(names_to = \"type\", \n               values_to = \"servings\", \n               cols = -country)\ndrinks_smaller_tidy\n\n# A tibble: 12 × 3\n   country      type   servings\n   &lt;chr&gt;        &lt;chr&gt;     &lt;int&gt;\n 1 China        beer         79\n 2 China        spirit      192\n 3 China        wine          8\n 4 Italy        beer         85\n 5 Italy        spirit       42\n 6 Italy        wine        237\n 7 Saudi Arabia beer          0\n 8 Saudi Arabia spirit        5\n 9 Saudi Arabia wine          0\n10 USA          beer        249\n11 USA          spirit      158\n12 USA          wine         84\n\n\nWe set the arguments to pivot_longer() as follows:\n\nnames_to here corresponds to the name of the variable in the new “tidy”/long dataframe that will contain the column names of the original data. Observe how we set names_to = \"type\". In the resulting drinks_smaller_tidy, the column type contains the three types of alcohol beer, spirit, and wine. Since type is a variable name that doesn’t appear in drinks_smaller, we use quotation marks around it. You’ll receive an error if you just use names_to = type here.\nvalues_to here is the name of the variable in the new “tidy” dataframe that will contain the values of the original data. Observe how we set values_to = \"servings\" since each of the numeric values in each of the beer, wine, and spirit columns of the drinks_smaller data corresponds to a value of servings. In the resulting drinks_smaller_tidy, the column servings contains the 4 \\(\\times\\) 3 = 12 numerical values. Note again that servings doesn’t appear as a variable in drinks_smaller so it again needs quotation marks around it for the values_to argument.\nThe third argument cols is the columns in the drinks_smaller dataframe you either want to or don’t want to “tidy.” Observe how we set this to -country indicating that we don’t want to “tidy” the country variable in drinks_smaller and rather only beer, spirit, and wine. Since country is a column that appears in drinks_smaller we don’t put quotation marks around it.\n\nThe third argument here of cols is a little nuanced, so let’s consider code that’s written slightly differently but that produces the same output:\n\n#|eval: false\ndrinks_smaller %&gt;% \n  pivot_longer(names_to = \"type\", \n               values_to = \"servings\", \n               cols = c(beer, spirit, wine))\n\n# A tibble: 12 × 3\n   country      type   servings\n   &lt;chr&gt;        &lt;chr&gt;     &lt;int&gt;\n 1 China        beer         79\n 2 China        spirit      192\n 3 China        wine          8\n 4 Italy        beer         85\n 5 Italy        spirit       42\n 6 Italy        wine        237\n 7 Saudi Arabia beer          0\n 8 Saudi Arabia spirit        5\n 9 Saudi Arabia wine          0\n10 USA          beer        249\n11 USA          spirit      158\n12 USA          wine         84\n\n\nNote that the third argument now specifies which columns we want to “tidy” with c(beer, spirit, wine), instead of the columns we don’t want to “tidy” using -country. We use the c() function to create a vector of the columns in drinks_smaller that we’d like to “tidy.” Note that since these three columns appear one after another in the drinks_smaller dataframe, we could also do the following for the cols argument:\n\ndrinks_smaller %&gt;% \n  pivot_longer(names_to = \"type\", \n               values_to = \"servings\", \n               cols = beer:wine)\n\n# A tibble: 12 × 3\n   country      type   servings\n   &lt;chr&gt;        &lt;chr&gt;     &lt;int&gt;\n 1 China        beer         79\n 2 China        spirit      192\n 3 China        wine          8\n 4 Italy        beer         85\n 5 Italy        spirit       42\n 6 Italy        wine        237\n 7 Saudi Arabia beer          0\n 8 Saudi Arabia spirit        5\n 9 Saudi Arabia wine          0\n10 USA          beer        249\n11 USA          spirit      158\n12 USA          wine         84\n\ndrinks_smaller_tidy &lt;- drinks_smaller %&gt;%\n  gather(type, servings, -country)\n\nLet’s see what our “tidy” formatted dataframe, drinks_smaller_tidy, looks like.\n\n\n# A tibble: 12 × 3\n   country      type   servings\n   &lt;chr&gt;        &lt;chr&gt;     &lt;int&gt;\n 1 China        beer         79\n 2 Italy        beer         85\n 3 Saudi Arabia beer          0\n 4 USA          beer        249\n 5 China        spirit      192\n 6 Italy        spirit       42\n 7 Saudi Arabia spirit        5\n 8 USA          spirit      158\n 9 China        wine          8\n10 Italy        wine        237\n11 Saudi Arabia wine          0\n12 USA          wine         84\n\n\nConverting “wide” format data to “tidy” format often confuses new R users. The only way to learn to get comfortable with the pivot_longer() function is with practice, practice, and more practice using different data sets. For example, run ?pivot_longer and look at the examples in the bottom of the help file.\nIf however you want to convert a “tidy” dataframe to “wide” format, you will need to use the pivot_wider() function instead. Run ?pivot_wider and look at the examples in the bottom of the help file for examples.\nYou can also view examples of both pivot_longer() and pivot_wider() on the tidyverse.org webpage. There’s a nice example to check out the different functions available for data tidying and a case study using data from the World Health Organization on that webpage. Furthermore, each week the R4DS Online Learning Community posts a data set in the weekly #TidyTuesday event that might serve as a nice place for you to find other data to explore and transform."
  },
  {
    "objectID": "tibbles.html#sec-tibblesactivities",
    "href": "tibbles.html#sec-tibblesactivities",
    "title": "6  Dataframes",
    "section": "6.3 Activities",
    "text": "6.3 Activities\n\nLet’s load data we downloaded back in Section 4.2. Run the following:\n\n\nnba &lt;- read_csv(\"./data/nba_all_seasons.csv\")\n\n\nNow let’s take a quick peek at what’s in there by running the following:\n\n\nhead(nba)\n\n\nConsider the following questions:\n\nHow many columns are there in nba?\nHow many rows are there in nba?\nWhat is the data type of the player_height column?\nWhat is the data type of the player_weight column?\nWhat is the data type of the pts column?\nWhat is the data type of the draft_year column?\nWhat is the data type of the draft_round column?\nWhat is the data type of the draft_number column?\n\nLet’s take a moment to talk about what this data set contains. This data reflects statistics for players in the National Basketball Association (NBA). The data covers seasons from 1996-1997 to 2021-2022 (NBA seasons run from approximately October to June of the following year). Each row represents a player . Columns include each player’s name (player_name), the team the player played for (team_abbreviation), where the player went to college (college), the player’s height (player_height) and weight (player_weight), the year, round, and order in which the player was drafted (draft_year, draft_round, and draft_number) as well as a variety of statistics about the player’s performance (e.g., points scored, pts).\n\n\nDo any of these data types seem incorrect? Which ones?\nLet’s take a closer look. Open the data in RStudio’s spreadsheet viewer by running the following:\n\n\nView(nba)\n\n\nInspect the college column. Do you see any unusual values?\nGiven what you see in the tibble and the description above, do you think this data is in tidy format?"
  },
  {
    "objectID": "dplyr.html#sec-whatisdplyr",
    "href": "dplyr.html#sec-whatisdplyr",
    "title": "7  Manipulating Data",
    "section": "7.1 What is dplyr",
    "text": "7.1 What is dplyr\nIn the tidyverse, the package responsible for such activities is called dplyr. This package contains a set of functions, each of which manipulates data in a particular way. Each of these functions is very useful on its own. However, the real power of dplyr comes from the fact that you can repeatedly applying dplyr functions, each operating on the result from the previous. These “chains” of functions allow you to compose complex data manipulation operations that ultimately transform your data into whatever you need. For this reason, dplyr is sometimes said to instantiate a “grammar” of data manipulation, a grammar which is made up of several “verbs” (functions). These function include:\n\nfilter(): select a subset of rows from a data frame\narrange(): sort a data frame’s rows\nmutate() create new columns/variables based on existing columns/variables\nsummarize(): aggregate one or more columns/variables with a summary statistic (e.g., the mean)\ngroup_by(): assign rows to groups, such that each group shares some values in common\n\nBecause dplyr is part of the tidyverse, these all work similarly. Specifically, each dplyr function:\n\nTakes a dataframe as its first argument\nTakes additional arguments that often indicate which columns are to be operated on\nReturns a modified dataframe\n\nLet’s see some of these characteristics in action. To do so, we’ll first load the data we downloaded back in Section 4.2. Run the following:\n\nnba &lt;- read_csv(\"./data/nba_all_seasons.csv\", na = c(\"Undrafted\"))\n\nNote that we have utilized the na argument here. This tells read_csv that we would like any values found in the file that match \"Undrafted\" to be treated as “missing”. R uses NA to represent missing values and so any \"Undrafted\" values will be converted in to NA."
  },
  {
    "objectID": "dplyr.html#filter",
    "href": "dplyr.html#filter",
    "title": "7  Manipulating Data",
    "section": "7.2 filter",
    "text": "7.2 filter\nThe filter() function allows you to specify criteria about the values of a variable in your data set and then filters out only the rows that match that criteria.\nThe team_abbreviation for the New York Knicks is \"NYK\". Run the following and look at the results in RStudio’s spreadsheet viewer to ensure that only players on the Knicks heading to Portland are chosen here:\n\nfilter(nba, team_abbreviation==\"NYK\")\n\n# A tibble: 410 × 22\n    ...1 player_name team_abbreviation   age player_height player_weight college\n   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;             &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;  \n 1    81 Herb Willi… NYK                  39          211.         118.  Ohio S…\n 2   146 Allan Hous… NYK                  26          198.          90.7 Tennes…\n 3   195 Buck Willi… NYK                  37          203.         102.  Maryla…\n 4   204 Charles Oa… NYK                  33          206.         111.  Virgin…\n 5   209 Chris Chil… NYK                  29          190.          88.5 Boise …\n 6   212 Chris Jent  NYK                  27          201.          99.8 Ohio S…\n 7   218 Charlie Wa… NYK                  26          188.          86.2 Florid…\n 8   230 Scott Broo… NYK                  31          180.          74.8 Califo…\n 9   293 Walter McC… NYK                  23          208.         104.  Kentuc…\n10   347 Larry John… NYK                  28          201.         119.  Nevada…\n# ℹ 400 more rows\n# ℹ 15 more variables: country &lt;chr&gt;, draft_year &lt;dbl&gt;, draft_round &lt;dbl&gt;,\n#   draft_number &lt;dbl&gt;, gp &lt;dbl&gt;, pts &lt;dbl&gt;, reb &lt;dbl&gt;, ast &lt;dbl&gt;,\n#   net_rating &lt;dbl&gt;, oreb_pct &lt;dbl&gt;, dreb_pct &lt;dbl&gt;, usg_pct &lt;dbl&gt;,\n#   ts_pct &lt;dbl&gt;, ast_pct &lt;dbl&gt;, season &lt;chr&gt;\n\n\nIf you prefer a more thorough inspection, you can open the data in RStudio’s spreadsheet viewer:\n\nView(filter(nba, team_abbreviation==\"NYK\"))\n\nIn either case, we are asking for a test of equality, keeping any rows where team_abbreviation==\"NYK\" is true and removing (filtering) any rows where team_abbreviation==\"NYK\" is false. To do so, we use the double equal sign ==, not a single equal sign =. In other words filter(nba, team_abbreviation = \"NYK\") will yield an error. This is a convention across many programming languages. If you are new to coding, you’ll probably forget to use the double equal sign == a few times before you get the hang of it.\nThe equality operator is not the only operator available to us. Others include:\n\n&gt; corresponds to “greater than”\n&lt; corresponds to “less than”\n&gt;= corresponds to “greater than or equal to”\n&lt;= corresponds to “less than or equal to”\n!= corresponds to “not equal to.” The ! is used in many programming languages to indicate “not.”\n\nFurthermore, you can combine multiple criteria using operators that make comparisons:\n\n| corresponds to “or”\n& corresponds to “and”\n\nLet’s look at an example that uses the ! “not” operator to pick rows that don’t match a criteria. As mentioned earlier, the ! can be read as “not.” Here we are filtering rows corresponding to players thare are not from the United States:\n\nfilter(nba, country!=\"USA\")\n\nLet’s combine two different requirements using the |:\n\nfilter(nba, country==\"Jamaica\" | college==\"Michigan\")\n\nThis will select players that are either from Jamica or went to Michigan (or both). We can also request the inverse of this.\n\nfilter(nba, !(country==\"Jamaica\" | college==\"Michigan\"))\n\nThis will select players that are not from Jamica and those players that did not go to Michigan. Note the use of parentheses around `(country==\"Jamaica\" | college==\"Michigan\"). If we used the !, but did not use the parentheses, we would only applying the “not” to the first test (country==“Jamaica”), not to the combination of (country==\"Jamaica\" | college==\"Michigan\"). You can try it and compare the results:\n\nfilter(nba, !country==\"Jamaica\" | college==\"Michigan\")\n\nThis request, in contrast to the one above, is for players that are not from Jamaica or went to Michigan (or both). So be very careful about the order of operations and use parentheses liberally. It helps to minimize errors and makes your code more explicit and therefore more readable.\nLet’s see a slightly more complicated request that combines several different operators:\n\nfilter(nba, team_abbreviation==\"NYK\" & (country!=\"USA\" | college==\"Michigan\") & age &gt;= 25)\n\nHere we have filtered the data to retain all rows corresponding to players from the NY Knicks, who are age 25 or older, and either went to Michigan or are not from the United States. You can this this yourself and verify that the output matches these requirements (remember that you can use View() if you wish to inspect the entire result).\nLet’s request players that went to either Michigan, Duke, or Georgetown.\n\nfilter(nba, college==\"Michigan\" | college==\"Duke\" | college==\"Georgetown\")\n\nThis works, but as we progressively include more collegs, this will get unwieldy to write. A slightly shorter approach uses the %in% operator along with the c() function. Recall from Subsection Section 2.5 that the c() function “combines” or “concatenates” values into a single vector of values.\n\nfilter(nba, college %in% c(\"Michigan\", \"Duke\", \"Georgetown\", \"UCLA\", \"Kentucky\"))\n\nWhat this code is doing is filtering our for all flights where college is in the vector of airports c(\"Michigan\", \"Duke\", \"Georgetown\", \"UCLA\", \"Kentucky\"). This approach produces results that are similar to a sequence of | operators, but takes much less energy to write (and read). The %in% operator is useful for looking for matches commonly in one vector/variable compared to another.\nAs a final note, we recommend that filter() should often be among the first tidyverse “verbs” you consider applying to your data. This cleans your dataset to only those rows you care about, or to put it another way, it narrows down the scope of your data to just the observations you care about."
  },
  {
    "objectID": "dplyr.html#pipe",
    "href": "dplyr.html#pipe",
    "title": "7  Manipulating Data",
    "section": "7.3 pipe",
    "text": "7.3 pipe\nBefore we go any further, let’s first introduce a nifty tool that gets loaded with the dplyr package: the pipe operator %&gt;%. The pipe operator allows us to combine multiple tidyverse operations into a single sequential chain of actions.\nLet’s start with a hypothetical example. Say you would like to perform a hypothetical sequence of operations on a hypothetical data frame x using hypothetical functions f(), g(), and h():\n\nTake x then\nUse x as an input to a function f() then\nUse the output of f(x) as an input to a function g() then\nUse the output of g(f(x)) as an input to a function h()\n\nOne way to achieve this sequence of operations is by using nesting parentheses as follows:\n\nh(g(f(x)))\n\nThis code isn’t so hard to read since we are applying only three functions: f(), then g(), then h() and each of the functions has a short name. Further, each of these functions also only has one argument. However, you can imagine that this will get progressively more difficult to read as the number of functions applied in your sequence increases and the arguments in each function grow more numerous. This is where the pipe operator %&gt;% comes in handy. The pipe takes the output of one function and then “pipes” it to be the input of the next function. Furthermore, a helpful trick is to read %&gt;% as “then” or “and then”. For example, you can obtain the same output as the hypothetical sequence of functions as follows:\n\nx %&gt;% \n  f() %&gt;% \n  g() %&gt;% \n  h()\n\nYou would read this sequence as:\n\nTake x then\nUse this output as the input to the next function f() then\nUse this output as the input to the next function g() then\nUse this output as the input to the next function h()\n\nThough both approaches achieve the same goal, the latter is much more human-readable because you can clearly read the sequence of operations line-by-line. But what are the hypothetical x, f(), g(), and h()? Throughout this chapter on data wrangling:\n\nThe starting value, x, will be a data frame. For example, the nba data frame we have been exploring so far.\nThe sequence of functions, here f(), g(), and h(), will mostly be a sequence of any number of the data wrangling verb-named functions we listed above. For example, the filter(college == \"Michigan\") function and argument specified we previewed earlier.\nThe result will be the transformed/modified data frame that you want.\n\nSo instead of\n\nfilter(nba, team_abbreviation==\"NYK\")\n\nwe can instead write:\n\nnba %&gt;%\n    filter(team_abbreviation==\"NYK\")\n\nThe benefits of this may not be immediately obvious. But the ability to form a chain of data wrangling operations by combining tidyverse functions (verbs) into a single sequence will be utilized extensively and is made possible by the pipe operator %&gt;%."
  },
  {
    "objectID": "dplyr.html#sec-summarize",
    "href": "dplyr.html#sec-summarize",
    "title": "7  Manipulating Data",
    "section": "7.4 summarize",
    "text": "7.4 summarize\nThe next common task when working with data frames is to compute summary statistics. Summary statistics are single numerical values that summarize a large number of values. Commonly known examples of summary statistics include the mean (i.e., the “average”), the median, and the mode. Other examples of summary statistics that might not immediately come to mind include the sum, the smallest value also called the minimum, the largest value also called the maximum, and the standard deviation.\nLet’s calculate two summary statistics of the draft_round variable in the nba data frame: the mean and standard deviation. To compute these summary statistics, we need the mean() and sd() summary functions in R. Summary functions in R take in many values and return a single value. More precisely, we’ll use the mean() and sd() summary functions within the summarize() function from the dplyr package. The summarize() function takes in a data frame and returns a data frame with only one row corresponding to the value of the summary statistic(s).\nWe’ll save the results in a new data frame called summary_round that will have two columns/variables: the mean and the std_dev:\n\nsummary_round &lt;- nba %&gt;% \n  summarize(mean = mean(draft_round), std_dev = sd(draft_round))\nsummary_round\n\n# A tibble: 1 × 2\n   mean std_dev\n  &lt;dbl&gt;   &lt;dbl&gt;\n1    NA      NA\n\n\nWhy are the values returned NA? NA is how R encodes missing values where NA indicates “not available” or “not applicable.” If a value for a particular row and a particular column does not exist, NA is stored instead. Values can be missing for many reasons. Perhaps the data was collected but someone forgot to enter it. Perhaps the data was not collected at all because it was too difficult to do so. Perhaps there was an erroneous value that someone entered that has been corrected to read as missing. You’ll often encounter issues with missing values when working with real data.\nGoing back to our summary_round output, by default any time you try to calculate a summary statistic of a variable that has one or more NA missing values in R, NA is returned. If what you wish to calculate is the summary of all the valid, non-missing values, you can set the na.rm argument to TRUE, where rm is short for “remove”. The code that follows computes the mean and standard deviation of all non-missing values of age:\n\nsummary_round &lt;- nba %&gt;% \n  summarize(mean = mean(draft_round, na.rm = TRUE), \n            std_dev = sd(draft_round, na.rm = TRUE))\nsummary_round\n\n# A tibble: 1 × 2\n   mean std_dev\n  &lt;dbl&gt;   &lt;dbl&gt;\n1  1.30   0.507\n\n\nNotice how the na.rm = TRUE are used as arguments to the mean() and sd() summary functions individually, and not to the summarize() function.\nHowever, one needs to be cautious whenever ignoring missing values as we’ve just done. We will consider the possible ramifications of blindly sweeping rows with missing values “under the rug”. This is in fact why the na.rm argument to any summary statistic function in R is set to FALSE by default. In other words, R does not ignore rows with missing values by default. R is alerting you to the presence of missing data and you should be mindful of this missingness and any potential causes of this missingness throughout your analysis.\nWhat are other summary functions we can use inside the summarize() verb to compute summary statistics? You can use any function in R that takes many values and returns just one. Here are just a few:\n\nmean(): the average\nsd(): the standard deviation, which is a measure of spread\nmin() and max(): the minimum and maximum values, respectively\nIQR(): interquartile range\nsum(): the total amount when adding multiple numbers\nn(): a count of the number of rows in each group. This particular summary function will make more sense when group_by() is covered below.\n\n\n\n\n\n\n\nSometimes missingness is data\n\n\n\nImagine a public health researcher is studying the effect of smoking on lung cancer for a large number of patients who are observed every five years. She notices that a large number of patients have missing data points, particularly in later observations. The researcher takes the approach outlined above, choosing to ignore these patients in her analysis. How might this be misleading?\n\n\nLet’s see just a couple more examples of summaries. Here we ask for the number of unique college that appear in the data set:\n\nnba %&gt;%\n    summarise(n_colleges = n_distinct(college))\n\n# A tibble: 1 × 1\n  n_colleges\n       &lt;int&gt;\n1        345\n\n\nHere we combine a filter operation with a summarize operation to calculate the average number of points scored by players in the 2000-2001 NBA season:\n\nnba %&gt;%\n    filter(season == \"2000-01\") %&gt;%\n    summarize(total_points = mean(pts))\n\n# A tibble: 1 × 1\n  total_points\n         &lt;dbl&gt;\n1         7.81"
  },
  {
    "objectID": "dplyr.html#sec-groupby",
    "href": "dplyr.html#sec-groupby",
    "title": "7  Manipulating Data",
    "section": "7.5 group_by",
    "text": "7.5 group_by\nSay instead of a single mean number of points for the entire NBA, you would like a mean number of points separately for each college players attended. In other words, we would like to compute the mean number of points split by college. We can do this by “grouping” the pts measurements by the values of another variable, in this case by the values of the variable college . Run the following code:\n\nnba %&gt;%\n    group_by(college) %&gt;% \n    summarize(mean = mean(pts),\n            std_dev = sd(pts))\n\n# A tibble: 345 × 3\n   college                   mean std_dev\n   &lt;chr&gt;                    &lt;dbl&gt;   &lt;dbl&gt;\n 1 \"\"                        9.9    3.79 \n 2 \"Alabama\"                 9.21   5.83 \n 3 \"Alabama A&M\"             2.2   NA    \n 4 \"Alabama Huntsville\"      2.05   0.778\n 5 \"Alabama-Birmingham\"      2.4    1.42 \n 6 \"Albany State (GA)\"       0.45   0.212\n 7 \"American International\"  8.34   2.71 \n 8 \"American University\"     6      8.49 \n 9 \"Arizona\"                 8.93   5.83 \n10 \"Arizona St.\"             0     NA    \n# ℹ 335 more rows\n\n\nThis code is similar to the previous code that created summary_round, but with an extra group_by(month) added before the summarize(). Grouping the nba dataset by college and then applying the summarize() functions yields a data frame that displays the mean and standard deviation temperature split by the different colleges that appear in the data set.\nIt is important to note that the group_by() function doesn’t change data frames by itself. Rather it changes the meta-data, or data about the data, specifically the grouping structure. It is only after we apply the summarize() function that the data frame changes.\nFor example, when we run this code:\n\nnba\n\n# A tibble: 12,305 × 22\n    ...1 player_name team_abbreviation   age player_height player_weight college\n   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;             &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;  \n 1     0 Dennis Rod… CHI                  36          198.          99.8 Southe…\n 2     1 Dwayne Sch… LAC                  28          216.         118.  Florida\n 3     2 Earl Curet… TOR                  39          206.          95.3 Detroi…\n 4     3 Ed O'Bannon DAL                  24          203.         101.  UCLA   \n 5     4 Ed Pinckney MIA                  34          206.         109.  Villan…\n 6     5 Eddie John… HOU                  38          201.          97.5 Illino…\n 7     6 Eddie Jones LAL                  25          198.          86.2 Temple \n 8     7 Elden Camp… LAL                  28          213.         113.  Clemson\n 9     8 Eldridge R… ATL                  29          193.          86.2 Washin…\n10     9 Elliot Per… MIL                  28          183.          72.6 Memphis\n# ℹ 12,295 more rows\n# ℹ 15 more variables: country &lt;chr&gt;, draft_year &lt;dbl&gt;, draft_round &lt;dbl&gt;,\n#   draft_number &lt;dbl&gt;, gp &lt;dbl&gt;, pts &lt;dbl&gt;, reb &lt;dbl&gt;, ast &lt;dbl&gt;,\n#   net_rating &lt;dbl&gt;, oreb_pct &lt;dbl&gt;, dreb_pct &lt;dbl&gt;, usg_pct &lt;dbl&gt;,\n#   ts_pct &lt;dbl&gt;, ast_pct &lt;dbl&gt;, season &lt;chr&gt;\n\n\nObserve that the first line of the output reads # A tibble: 12305 x 22. This is an example of meta-data, in this case the number of observations/rows and variables/columns in nba. The actual data itself are the subsequent table of values. Now let’s pipe the nba data frame into group_by(college):\n\nnba %&gt;% \n  group_by(college)\n\n# A tibble: 12,305 × 22\n# Groups:   college [345]\n    ...1 player_name team_abbreviation   age player_height player_weight college\n   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;             &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;  \n 1     0 Dennis Rod… CHI                  36          198.          99.8 Southe…\n 2     1 Dwayne Sch… LAC                  28          216.         118.  Florida\n 3     2 Earl Curet… TOR                  39          206.          95.3 Detroi…\n 4     3 Ed O'Bannon DAL                  24          203.         101.  UCLA   \n 5     4 Ed Pinckney MIA                  34          206.         109.  Villan…\n 6     5 Eddie John… HOU                  38          201.          97.5 Illino…\n 7     6 Eddie Jones LAL                  25          198.          86.2 Temple \n 8     7 Elden Camp… LAL                  28          213.         113.  Clemson\n 9     8 Eldridge R… ATL                  29          193.          86.2 Washin…\n10     9 Elliot Per… MIL                  28          183.          72.6 Memphis\n# ℹ 12,295 more rows\n# ℹ 15 more variables: country &lt;chr&gt;, draft_year &lt;dbl&gt;, draft_round &lt;dbl&gt;,\n#   draft_number &lt;dbl&gt;, gp &lt;dbl&gt;, pts &lt;dbl&gt;, reb &lt;dbl&gt;, ast &lt;dbl&gt;,\n#   net_rating &lt;dbl&gt;, oreb_pct &lt;dbl&gt;, dreb_pct &lt;dbl&gt;, usg_pct &lt;dbl&gt;,\n#   ts_pct &lt;dbl&gt;, ast_pct &lt;dbl&gt;, season &lt;chr&gt;\n\n\nObserve that now there is additional meta-data: # Groups:   college [345] indicating that the grouping structure meta-data has been set based on the unique values of the variable college. On the other hand, observe that the data has not changed: it is still a table of 12305 \\(\\times\\) 22 values.\nOnly by combining a group_by() with another data wrangling operation, in this case summarize(), will the data actually be transformed.\n\nnba %&gt;% \n  group_by(college) %&gt;% \n  summarize(mean = mean(pts))\n\n# A tibble: 345 × 2\n   college                   mean\n   &lt;chr&gt;                    &lt;dbl&gt;\n 1 \"\"                        9.9 \n 2 \"Alabama\"                 9.21\n 3 \"Alabama A&M\"             2.2 \n 4 \"Alabama Huntsville\"      2.05\n 5 \"Alabama-Birmingham\"      2.4 \n 6 \"Albany State (GA)\"       0.45\n 7 \"American International\"  8.34\n 8 \"American University\"     6   \n 9 \"Arizona\"                 8.93\n10 \"Arizona St.\"             0   \n# ℹ 335 more rows\n\n\nIf you would like to remove this grouping structure meta-data, we can pipe the resulting data frame into the ungroup() function:\n\nnba %&gt;% \n  group_by(college) %&gt;% \n  ungroup()\n\n# A tibble: 12,305 × 22\n    ...1 player_name team_abbreviation   age player_height player_weight college\n   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;             &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;  \n 1     0 Dennis Rod… CHI                  36          198.          99.8 Southe…\n 2     1 Dwayne Sch… LAC                  28          216.         118.  Florida\n 3     2 Earl Curet… TOR                  39          206.          95.3 Detroi…\n 4     3 Ed O'Bannon DAL                  24          203.         101.  UCLA   \n 5     4 Ed Pinckney MIA                  34          206.         109.  Villan…\n 6     5 Eddie John… HOU                  38          201.          97.5 Illino…\n 7     6 Eddie Jones LAL                  25          198.          86.2 Temple \n 8     7 Elden Camp… LAL                  28          213.         113.  Clemson\n 9     8 Eldridge R… ATL                  29          193.          86.2 Washin…\n10     9 Elliot Per… MIL                  28          183.          72.6 Memphis\n# ℹ 12,295 more rows\n# ℹ 15 more variables: country &lt;chr&gt;, draft_year &lt;dbl&gt;, draft_round &lt;dbl&gt;,\n#   draft_number &lt;dbl&gt;, gp &lt;dbl&gt;, pts &lt;dbl&gt;, reb &lt;dbl&gt;, ast &lt;dbl&gt;,\n#   net_rating &lt;dbl&gt;, oreb_pct &lt;dbl&gt;, dreb_pct &lt;dbl&gt;, usg_pct &lt;dbl&gt;,\n#   ts_pct &lt;dbl&gt;, ast_pct &lt;dbl&gt;, season &lt;chr&gt;\n\n\nObserve how the # Groups:   college [345] meta-data is no longer present.\nLet’s now revisit the n() counting summary function we briefly introduced previously. Recall that the n() function counts rows. This is opposed to the sum() summary function that returns the sum of a numerical variable. For example, suppose we’d like to count how many NBA players went to each of the different colleges, we can run this:\n\nby_college &lt;- nba %&gt;% \n  group_by(college) %&gt;% \n  summarize(count = n())\nby_college\n\n# A tibble: 345 × 2\n   college                  count\n   &lt;chr&gt;                    &lt;int&gt;\n 1 \"\"                           5\n 2 \"Alabama\"                  119\n 3 \"Alabama A&M\"                1\n 4 \"Alabama Huntsville\"         2\n 5 \"Alabama-Birmingham\"         7\n 6 \"Albany State (GA)\"          2\n 7 \"American International\"     5\n 8 \"American University\"        2\n 9 \"Arizona\"                  279\n10 \"Arizona St.\"                1\n# ℹ 335 more rows\n\n\nWe see that there are 119 rows in which college==\"Alabama\" and 279 rows in which college==\"Arizona\". Note there is a subtle but important difference between sum() and n(); while sum() returns the sum of a numerical variable (adding), n() returns a count of the number of rows/observations (counting).\n\n7.5.1 Grouping by more than one variable\nYou are not limited to grouping by one variable. Say you want to know the number of players coming from each college for each NBA season. We can also group by a second variable season using group_by(college, season):\n\nby_college_annually &lt;- nba %&gt;% \n  group_by(college, season) %&gt;% \n  summarize(count = n())\n\n`summarise()` has grouped output by 'college'. You can override using the\n`.groups` argument.\n\nby_college_annually\n\n# A tibble: 3,637 × 3\n# Groups:   college [345]\n   college   season  count\n   &lt;chr&gt;     &lt;chr&gt;   &lt;int&gt;\n 1 \"\"        2017-18     2\n 2 \"\"        2018-19     2\n 3 \"\"        2020-21     1\n 4 \"Alabama\" 1996-97     9\n 5 \"Alabama\" 1997-98    10\n 6 \"Alabama\" 1998-99     8\n 7 \"Alabama\" 1999-00     6\n 8 \"Alabama\" 2000-01     7\n 9 \"Alabama\" 2001-02     6\n10 \"Alabama\" 2002-03     4\n# ℹ 3,627 more rows\n\n\nObserve that there are now 3637 rows to by_college_annually because there are 12305 unique colleges.\nWhy do we group_by(college, season) and not group_by(college) and then group_by(season)? Let’s investigate:\n\nby_college_annually_incorrect &lt;- nba %&gt;% \n  group_by(college) %&gt;% \n  group_by(season) %&gt;% \n  summarize(count = n())\nby_college_annually_incorrect\n\n# A tibble: 26 × 2\n   season  count\n   &lt;chr&gt;   &lt;int&gt;\n 1 1996-97   441\n 2 1997-98   439\n 3 1998-99   439\n 4 1999-00   438\n 5 2000-01   441\n 6 2001-02   440\n 7 2002-03   428\n 8 2003-04   442\n 9 2004-05   464\n10 2005-06   458\n# ℹ 16 more rows\n\n\nWhat happened here is that the second group_by(season) overwrote the grouping structure meta-data of the earlier group_by(college), so that in the end we are only grouping by season. The lesson here is if you want to group_by() two or more variables, you should include all the variables at the same time in the same group_by() adding a comma between the variable names."
  },
  {
    "objectID": "dplyr.html#sec-mutate",
    "href": "dplyr.html#sec-mutate",
    "title": "7  Manipulating Data",
    "section": "7.6 mutate",
    "text": "7.6 mutate\nAnother common transformation of data is to create/compute new variables based on existing ones. For example, the heights in our nba data set are measured in units of centimeters. But say you are more comfortable thinking of inches instead of centimeters. The formula to convert centimeters to inches is:\n\\[\n\\text{height in inches} = \\frac{\\text{height in centimeters.}}{2.54}\n\\]\nWe can apply this formula to the player_height variable using the mutate() function from the dplyr package, which takes existing variables and mutates them to create new ones.\n\nnba &lt;- nba %&gt;% \n  mutate(player_height_in_inch = player_height / 2.54)\n\nIn this code, we mutate() the nba data frame by creating a new variable player_height_in_inch = height / 2.54 and then overwrite the original nba data frame. Why did we overwrite the data frame nba, instead of assigning the result to a new data frame like nba_new? As a rough rule of thumb, as long as you are not losing original information that you might need later, it’s acceptable practice to overwrite existing data frames with updated ones, as we did here. On the other hand, why did we not overwrite the variable player_height, but instead created a new variable called player_height_in_inch? Because if we did this, we would have erased the original information contained in player_height of temperatures in centimeters that may still be valuable to us.\nLet’s now compute average heights in both inches and centimeters using the group_by() and summarize():\n\nsummary_college_height &lt;- nba %&gt;%\n    group_by(college) %&gt;% \n    summarize(mean_height_in_inch_in_cm = mean(player_height),\n              mean_height_in_inch_in_in = mean(player_height_in_inch))\nsummary_college_height\n\n# A tibble: 345 × 3\n   college                  mean_height_in_inch_in_cm mean_height_in_inch_in_in\n   &lt;chr&gt;                                        &lt;dbl&gt;                     &lt;dbl&gt;\n 1 \"\"                                            204.                      80.2\n 2 \"Alabama\"                                     200.                      78.6\n 3 \"Alabama A&M\"                                 211.                      83  \n 4 \"Alabama Huntsville\"                          185.                      73  \n 5 \"Alabama-Birmingham\"                          196.                      77.1\n 6 \"Albany State (GA)\"                           206.                      81  \n 7 \"American International\"                      196.                      77  \n 8 \"American University\"                         190.                      75  \n 9 \"Arizona\"                                     198.                      77.8\n10 \"Arizona St.\"                                 196.                      77  \n# ℹ 335 more rows\n\n\nLet’s consider another example. We can imagine placing players on a continuum, with one end representing the “star” players and the other end representing “support” players. We can quantify this dimension by by comparing a player’s average points per game (pts) to his average assists per game (ast). Let’s calculate this new variable using the mutate() function:\n\nnba &lt;- nba %&gt;% \n  mutate(star_support = pts - ast)\n\nLet’s take a look at only the pts, ast, and the resulting star_support variables for a few rows in our updated nba data frame.\n\n\n# A tibble: 5 × 3\n    pts   ast star_support\n  &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;\n1   5.7   3.1          2.6\n2   2.3   0.3          2  \n3  17.2   3.4         13.8\n4   1.5   2.9         -1.4\n5   3.7   0.6          3.1\n\n\nThe player in the third row scored an average of 17.2 points per game and made an average of 3.4 assists per game, so his star_support value is \\(17.2-3.5=13.8\\). On the other hand, the player in the fourth row averaged 1.5 points and 2.9 assists, so its star_support value is \\(1.5 - 2.9 = -1.4\\).\nLet’s look at some summary statistics of the star_support variable by considering multiple summary functions at once in the same summarize() code:\n\nstar_support_summary &lt;- nba %&gt;% \n  summarize(\n    min = min(star_support, na.rm = TRUE),\n    q1 = quantile(star_support, 0.25, na.rm = TRUE),\n    median = quantile(star_support, 0.5, na.rm = TRUE),\n    q3 = quantile(star_support, 0.75, na.rm = TRUE),\n    max = max(star_support, na.rm = TRUE),\n    mean = mean(star_support, na.rm = TRUE),\n    sd = sd(star_support, na.rm = TRUE),\n    missing = sum(is.na(star_support))\n  )\nstar_support_summary\n\n# A tibble: 1 × 8\n    min    q1 median    q3   max  mean    sd missing\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n1  -2.5   2.6    5.1     9  30.9  6.36  4.97       0\n\n\nWe see for example that the average value is 6.36 minutes, whereas the largest is 30.9! This summary contains quite a bit of information. However, it is often easier to visualize data to evaluate how values are distributed. We’ll take a look at that in Chapter 8.\nTo close out our discussion on the mutate() function to create new variables, note that we can create multiple new variables at once in the same mutate() code. Furthermore, within the same mutate() code we can refer to new variables we just created. Consider following:\n\nnba &lt;- nba %&gt;% \n  mutate(\n    player_height_in_inch = player_height / 2.54,\n    player_weight_in_lbs = player_weight / 2.2,\n    bmi = 703 * (player_weight_in_lbs / (player_height_in_inch^2)),\n  )"
  },
  {
    "objectID": "dplyr.html#arrange",
    "href": "dplyr.html#arrange",
    "title": "7  Manipulating Data",
    "section": "7.7 arrange",
    "text": "7.7 arrange\nOne of the most commonly performed data wrangling tasks is to sort a data frame’s rows in the alphanumeric order of one of the variables. The dplyr package’s arrange() function allows us to sort/reorder a data frame’s rows according to the values of the specified variable.\nSuppose we are interested in determining the average number of points per game scored by NBA players from different colleges:\n\nmean_pts &lt;- nba %&gt;% \n  group_by(college) %&gt;% \n  summarize(mean = mean(pts), \n            std_dev = sd(pts))\nmean_pts\n\n# A tibble: 345 × 3\n   college                   mean std_dev\n   &lt;chr&gt;                    &lt;dbl&gt;   &lt;dbl&gt;\n 1 \"\"                        9.9    3.79 \n 2 \"Alabama\"                 9.21   5.83 \n 3 \"Alabama A&M\"             2.2   NA    \n 4 \"Alabama Huntsville\"      2.05   0.778\n 5 \"Alabama-Birmingham\"      2.4    1.42 \n 6 \"Albany State (GA)\"       0.45   0.212\n 7 \"American International\"  8.34   2.71 \n 8 \"American University\"     6      8.49 \n 9 \"Arizona\"                 8.93   5.83 \n10 \"Arizona St.\"             0     NA    \n# ℹ 335 more rows\n\n\nObserve that by default the rows of the resulting mean_pts data frame are sorted in alphabetical order of college. Say instead we would like to see the same data, but sorted from the highest to the lowest average points (mean) instead:\n\nmean_pts %&gt;% \n  arrange(mean)\n\n# A tibble: 345 × 3\n   college                         mean std_dev\n   &lt;chr&gt;                          &lt;dbl&gt;   &lt;dbl&gt;\n 1 Arizona St.                     0     NA    \n 2 Chicago St.                     0     NA    \n 3 Denver                          0     NA    \n 4 Fairfield                       0     NA    \n 5 George Mason                    0     NA    \n 6 Lebanon Valley                  0     NA    \n 7 Lincoln Memorial                0     NA    \n 8 University of Colorado Boulder  0     NA    \n 9 Toledo                          0.2   NA    \n10 Albany State (GA)               0.45   0.212\n# ℹ 335 more rows\n\n\nThis is, however, the opposite of what we want. The rows are sorted with the least frequent destination airports displayed first. This is because arrange() always returns rows sorted in ascending order by default. To switch the ordering to be in “descending” order instead, we use the desc() function as so:\n\nmean_pts %&gt;% \n  arrange(desc(mean))\n\n# A tibble: 345 × 3\n   college                           mean std_dev\n   &lt;chr&gt;                            &lt;dbl&gt;   &lt;dbl&gt;\n 1 Davidson                          19.6    9.95\n 2 Lehigh                            18.4    7.06\n 3 Western Carolina                  16.1    7.50\n 4 Navy                              15.4    4.25\n 5 Marist                            15.4    1.92\n 6 Butler Community College          13.6    6.57\n 7 Louisiana Tech                    13.5    6.78\n 8 Trinity Valley Community College  13.5    6.63\n 9 Weber State                       13.3   11.6 \n10 Central Arkansas                  13.1    4.73\n# ℹ 335 more rows"
  },
  {
    "objectID": "dplyr.html#sec-tibblesactivities",
    "href": "dplyr.html#sec-tibblesactivities",
    "title": "7  Manipulating Data",
    "section": "7.8 Activities",
    "text": "7.8 Activities\nIf you haven’t already, load load nba data set we downloaded back in Section 4.2. And let’s pass c(\"Undrafted\") as the na argument:\n\nnba &lt;- read_csv(\"./data/nba_all_seasons.csv\", na = c(\"Undrafted\"))\n\n\nProduce a data frame consisting exclusively of players under the age of 25.\nProduce a data frame consisting exclusively of players who attended “Marquette”.\nProduce a data frame consisting exclusively of players who are either under the age of 25 or attended “Marquette”.\nProduce a data frame consisting exclusively of players under the age of 25 and attended “Marquette”.\nDetermine the average height of all players.\nDetermine the median height of all players who attended “Connecticut”.\nFor each college appearing in the data set, what is the average pts and ast of players who attended that college?\nTry calculating these school-wise averages separately for each season.\nCreate a new column in the data set called reb_diff and calculate it as the difference between the offensive rebound percentage (oreb_pct) and the defensive rebound percentage (dreb_pct).\nWhat is the standard deviation of reb_diff?\nFor each college appearing in the data set, what is the average ptsof players who attended that college. Which college has produced the highest average? Which college has produced the lowest average?"
  },
  {
    "objectID": "ggplot2.html#grammarofgraphics",
    "href": "ggplot2.html#grammarofgraphics",
    "title": "8  Visualizing Data",
    "section": "8.1 The grammar of graphics",
    "text": "8.1 The grammar of graphics\nWe start with a discussion of a theoretical framework for data visualization known as “the grammar of graphics.” This framework serves as the foundation for the ggplot2 package which we’ll use extensively in this chapter. In Chapter 7, we saw how dplyr provides a “grammar” of data manipulation, a grammar which is made up of several “verbs” (functions like filter and mutate). Similar to dplyr’s grammar of data manipulation, ggplto2 provides a a grammar of graphics that defines a set of rules for constructing statistical graphics by combining different types of layers. This grammar was created by Leland Wilkinson (Wilkinson 2012) and has been implemented in a variety of data visualization software platforms like R, but also Plotly and Tableau.\n\n8.1.1 Components of the grammar\nIn short, the grammar tells us that:\n\nA statistical graphic is a mapping of data variables to aesthetic attributes of geometric objects.\n\nSpecifically, we can break a graphic into the following three essential components:\n\ndata: the dataset containing the variables of interest.\ngeom: the geometric object in question. This refers to the type of object we can observe in a plot. For example: points, lines, and bars.\naes: aesthetic attributes of the geometric object. For example, x/y position, color, shape, and size. Aesthetic attributes are mapped to variables in the data set.\n\nYou might be wondering why we wrote the terms data, geom, and aes in a computer code type font. We’ll see very shortly that we’ll specify the elements of the grammar in R using these terms. However, let’s first break down the grammar with an example.\n\n\n8.1.2 An initial example\nLet’s take another look at our nba data set, this time via the grammar of graphics. Let’s specifically take a look at how things have changed over the years. As always, we need to load the data we downloaded back in Section 4.2. Run the following:\n\nnba &lt;- read_csv(\"./data/nba_all_seasons.csv\", na = c(\"Undrafted\"))\n\nWe need to do a bit of work before we can use season as a measure of time. This is because the season column is currently stored as a character vector, with values such as “2000-01” and “2011-12”. So we need two things: these character vectors need to be trimmed so that we retain only the first 4 characters in each vector and we then need to convert these character vectors to their corresponding numeric values. We’ll do that using the stringr package, yet another package that is part of the tidyverse.\n\nnba &lt;- nba %&gt;%\n    # select first 4 characters of `season`\n    mutate(season_int = substr(nba$season, start=1, stop=4))  %&gt;%\n    # convert to integer\n    mutate(season_int = as.integer(season_int))\n\nNow that we have time represented as a numeric column in our data set, we can use it to plot some data to visualize.\n\n\n`summarise()` has grouped output by 'team_abbreviation'. You can override using\nthe `.groups` argument.\n\n\n\n\n\nLet’s view this plot through the grammar of graphics. First, we have actually used two type of geometric object here: a line object and a point object. The point object provides the small circular data points. The line object provides the line segments connecting the points.\n\nThe data variable season_int gets mapped to the x-position aesthetic of the lines and the points.\nThe data variable pts gets mapped to the y-position aesthetic of the lines and the points.\nThe data variable team gets mapped to the color aesthetic of the lines and the points.\nThe data variable ast gets mapped to the size aesthetic of the points.\n\nThat being said, this is just an example. Plots can specify points, lines, bars, and a variety of other geometric objects.\nLet’s summarize the three essential components of the grammar.\n\n\n# A tibble: 7 × 3\n  geom  aes   `data variable`\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;          \n1 line  x     season_int     \n2 line  y     pts            \n3 line  color team           \n4 point x     season_int     \n5 point y     pts            \n6 point color team           \n7 point size  ast            \n\n\n\n\n8.1.3 Other components\nThere are other components of the grammar of graphics we can control as well. As you start to delve deeper into the grammar of graphics, you’ll start to encounter these topics more frequently. In this book, we’ll keep things simple and only work with these two additional components:\n\nfaceting breaks up a plot into several plots split by the values of another variable\nposition adjustments for barplots\n\nOther more complex components like scales and coordinate systems are left for a more advanced text such as R for Data Science. Generally speaking, the grammar of graphics allows for a high degree of customization of plots and also a consistent framework for easily updating and modifying them.\n\n\n8.1.4 ggplot2 package\nIn this book, we will use the ggplot2 package for data visualization, which is an implementation of the grammar of graphics for R. As we noted earlier, a lot of the previous section was written in a computer code type font. This is because the various components of the grammar of graphics are specified in the ggplot() function included in the ggplot2 package. For the purposes of this book, we’ll always provide the ggplot() function with the following arguments (i.e., inputs) at a minimum:\n\nThe data frame where the variables exist: the data argument.\nThe mapping of the variables to aesthetic attributes: the mapping argument which specifies the aesthetic attributes involved.\n\nAfter we’ve specified these components, we then add layers to the plot using the + sign. The most essential layer to add to a plot is the layer that specifies which type of geometric object we want the plot to involve: points, lines, bars, and others. Other layers we can add to a plot include the plot title, axes labels, visual themes for the plots, and facets (which we’ll see in Section 8.6).\nLet’s now put the theory of the grammar of graphics into practice."
  },
  {
    "objectID": "ggplot2.html#sec-FiveNG",
    "href": "ggplot2.html#sec-FiveNG",
    "title": "8  Visualizing Data",
    "section": "8.2 Five named graphs - the 5NG",
    "text": "8.2 Five named graphs - the 5NG\nIn order to keep things simple in this book, we will only focus on five different types of graphics, each with a commonly given name. We term these “five named graphs” or in abbreviated form, the 5NG:\n\nscatterplots\nlinegraphs\nhistograms\nboxplots\nbarplots\n\nWe’ll also present some variations of these plots, but with this basic repertoire of five graphics in your toolbox, you can visualize a wide array of different variable types. Note that certain plots are only appropriate for categorical variables, while others are only appropriate for numerical variables."
  },
  {
    "objectID": "ggplot2.html#sec-scatterplots",
    "href": "ggplot2.html#sec-scatterplots",
    "title": "8  Visualizing Data",
    "section": "8.3 5NG#1: Scatterplots",
    "text": "8.3 5NG#1: Scatterplots\nThe simplest of the 5NG are scatterplots. They allow you to visualize the relationship between two numerical variables. While you may already be familiar with scatterplots, let’s view them through the lens of the grammar of graphics. Specifically, we will visualize the relationship between the following two numerical variables in the nba data frame:\n\npts: average points per game each player scored\nast: average number of assists per game each player made\n\n\n8.3.1 Scatterplots via geom_point\nLet’s now go over the code that will create the desired scatterplot, while keeping in mind the grammar of graphics framework we introduced above. Let’s take a look at the code and break it down piece-by-piece.\n\nggplot(data = nba, mapping = aes(x = pts, y = ast)) + \n  geom_point()\n\nWithin the ggplot() function, we specify two of the components of the grammar of graphics as arguments (i.e., inputs):\n\nThe data as the nba data frame via data = nba.\nThe aesthetic mapping by setting mapping = aes(x = pts, y = ast). Specifically, the variable pts maps to the x position aesthetic, whereas the variable ast maps to the y position.\n\nWe then add a layer to the ggplot() function call using the + sign. The added layer in question specifies the third component of the grammar: the geometric object. In this case, the geometric object is set to be points by specifying geom_point(). After running these two lines of code in your console, you’ll notice two outputs: a warning message and this graphic.\n\n\n\n\n\nFigure 8.1: Assists versus points\n\n\n\n\nLet’s first unpack the graphic in Figure 8.1. Observe that a positive relationship exists between pts and ast: as the number of points increases, the number of assists also increases. Observe also the large mass of points clustered near (0, 0), the point indicating players have no points and no assists (e.g., what would be expected from a player that doesn’t play very much).\nBefore we continue, let’s make a few more observations about this code that created the scatterplot. Note that the + sign comes at the end of lines, and not at the beginning. You’ll get an error in R if you put it at the beginning of a line. When adding layers to a plot, you are encouraged to start a new line after the + (by pressing the Return/Enter button on your keyboard) so that the code for each layer is on a new line. As we add more and more layers to plots, you’ll see this will greatly improve the legibility of your code.\nTo stress the importance of adding the layer specifying the geometric object, consider Figure 8.2 where no layers are added. Because the geometric object was not specified, we have a blank plot which is not very useful!\n\nggplot(data = nba, mapping = aes(x = pts, y = ast))\n\n\n\n\nFigure 8.2: Assists versus points\n\n\n\n\n\n\n8.3.2 Overplotting\nThe large mass of points near (0, 0) in Figure can cause some confusion since it is hard to tell the true number of points that are actually in this lower corner. This is the result of a phenomenon called overplotting. As one may guess, this corresponds to points being plotted on top of each other over and over again. When overplotting occurs, it is difficult to know the number of points being plotted. There are two methods to address the issue of overplotting. Either by\n\nAdjusting the transparency of the points or\nAdding a little random “jitter”, or random “nudges”, to each of the points.\n\nMethod 1: Changing the transparency\nThe first way of addressing overplotting is to change the transparency/opacity of the points by setting the alpha argument in geom_point(). We can change the alpha argument to be any value between 0 and 1, where 0 sets the points to be 100% transparent and 1 sets the points to be 100% opaque. By default, alpha is set to 1. In other words, if we don’t explicitly set an alpha value, R will use alpha = 1.\nNote how the following code is identical to the code in Section @ref(scatterplots) that created the scatterplot with overplotting, but with alpha = 0.05 added to the geom_point() function:\n\nggplot(data = nba, mapping = aes(x = pts, y = ast)) + \n  geom_point(alpha = 0.05)\n\n\n\n\nFigure 8.3: Assists versus points\n\n\n\n\nThe key feature to note in Figure 8.3 is that the transparency of the points is cumulative: areas with a high-degree of overplotting are darker, whereas areas with a lower degree are less dark. Note furthermore that there is no aes() surrounding alpha = 0.05. This is because we are not mapping a variable to an aesthetic attribute, but rather merely changing the default setting of alpha. In fact, you’ll receive an error if you try to change the second line to read geom_point(aes(alpha = 0.05)).\nMethod 2: Jittering the points\nThe second way of addressing overplotting is by jittering all the points. This means giving each point a small “nudge” in a random direction. You can think of “jittering” as shaking the points around a bit on the plot. Let’s illustrate using a simple example first. Say we have a data frame with 4 identical rows of x and y values: (0,0), (0,0), (0,0), and (0,0). In Figure 8.4, we present both the regular scatterplot of these 4 points (on the left) and its jittered counterpart (on the right).\n\n\n\n\n\nFigure 8.4: Regular and jittered scatterplots\n\n\n\n\nIn the left scatterplot, observe that the 4 points are superimposed on top of each other. While we know there are 4 values being plotted, this fact might not be apparent to others. In the right scatterplot, the points are jittered and it is now plainly evident that this plot involves four points since each point is given a random “nudge.”\nKeep in mind, however, that jittering is strictly a visualization tool; even after creating a jittered scatterplot, the original values saved in the data frame remain unchanged.\nTo create a jittered scatterplot, instead of using geom_point(), we use geom_jitter(). Observe how the following code is very similar to the code that created Figure 8.1, but with geom_point() replaced with geom_jitter().\n\n\n\n\n\nFigure 8.5: Assists versus points jittered scatterplot\n\n\n\n\nIn order to specify how much jitter to add, we adjusted the width and height arguments to geom_jitter(). This corresponds to how hard you’d like to shake the plot in horizontal x-axis units and vertical y-axis units, respectively. In this case, both axes are in counts (number of points, number of assists). How much jitter should we add using the width and height arguments? On the one hand, it is important to add just enough jitter to break any overlap in points, but on the other hand, not so much that we completely alter the original pattern in points.\nAs can be seen in the resulting Figure 8.5, in this case jittering doesn’t really provide much new insight. In this particular case, it can be argued that changing the transparency of the points by setting alpha proved more effective. When would it be better to use a jittered scatterplot? When would it be better to alter the points’ transparency? There is no single right answer that applies to all situations. You need to make a subjective choice and own that choice. At the very least when confronted with overplotting, however, we suggest you make both types of plots and see which one better emphasizes the point you are trying to make.\n\n\n8.3.3 Summary\nScatterplots display the relationship between two numerical variables. They are among the most commonly used plots because they can provide an immediate way to see the trend in one numerical variable versus another. However, if you try to create a scatterplot where either one of the two variables is not numerical, you might get strange results. Be careful!\nWith medium to large datasets, you may need to play around with the different modifications to scatterplots we saw such as changing the transparency/opacity of the points or by jittering the points. This tweaking is often a fun part of data visualization, since you’ll have the chance to see different relationships emerge as you tinker with your plots."
  },
  {
    "objectID": "ggplot2.html#sec-linegraphs",
    "href": "ggplot2.html#sec-linegraphs",
    "title": "8  Visualizing Data",
    "section": "8.4 5NG#2: Linegraphs",
    "text": "8.4 5NG#2: Linegraphs\nThe next of the five named graphs are linegraphs. Linegraphs show the relationship between two numerical variables when the variable on the x-axis is ordinal; there is an inherent ordering to the variable.\nThe most common examples of linegraphs have some notion of time on the x-axis: hours, days, weeks, years, etc. Since time is naturally ordinal, we connect consecutive observations of the variable on the y-axis with a line. Linegraphs that have some notion of time on the x-axis are also called time series plots.\n\n8.4.1 Linegraphs via geom_line\nLet’s a linegraph to visualize a single NBA player’s number of points scored across seasons. To do so, we’ll use geom_line(), instead of using geom_point() as we did for the scatterplots above:\n\nggplot(\n    data = nba %&gt;%\n        filter(player_name == \"Stephen Curry\"),\n    mapping = aes(x = season_int, y = pts)\n) +\n    geom_line()\n\n\n\n\nStephen Curry points over time\n\n\n\n\nLet’s break down this code piece-by-piece in terms of the grammar of graphics. Within the ggplot() function call, we specify two of the components of the grammar of graphics as arguments:\n\nThe data. Here we have provided a filtered version of our nba data set, selecting only those row where player_name==\"Stephen Curry\".\nThe aesthetic mapping by setting mapping = aes(x = season_int, y = pts). Specifically, the variable season_int maps to the x position aesthetic, whereas the variable pts maps to the y position aesthetic.\n\nWe add a layer to the ggplot() function call using the + sign. The layer in question specifies the third component of the grammar: the geometric object in question. In this case, the geometric object is a line set by specifying geom_line().\n\n\n8.4.2 Summary\nLinegraphs, just like scatterplots, display the relationship between two numerical variables. However, it is preferred to use linegraphs over scatterplots when the variable on the x-axis (i.e., the explanatory variable) has an inherent ordering, such as some notion of time."
  },
  {
    "objectID": "ggplot2.html#sec-histograms",
    "href": "ggplot2.html#sec-histograms",
    "title": "8  Visualizing Data",
    "section": "8.5 5NG#3: Histograms",
    "text": "8.5 5NG#3: Histograms\nLet’s consider the pts variable in the nba data frame once again, but unlike with the linegraphs in Section 8.4, let’s say we don’t care about its relationship with time, but rather we only care about how the values of pts distribute. In other words:\n\nWhat are the smallest and largest values?\nWhat is the “center” or “most typical” value?\nHow do the values spread out?\nWhat are frequent and infrequent values?\n\nOne way to visualize this distribution of this single variable pts is to plot them on a horizontal line as we do in Figure 8.6:\n\n\n\n\n\nFigure 8.6: Plot of players’ points per-game point averages.\n\n\n\n\nThis gives us a bit of an idea of how the values of pts are distributed: note that values range from zero to approximately 20. In addition, there appear to be more values falling between approximately 3 and 10 than there are values falling above this range. However, because of the high degree of overplotting in the points, it’s hard to get a sense of exactly how many values are between 5 and 10.\nWhat is commonly produced instead of Figure 8.6 is known as a histogram. A histogram is a plot that visualizes the distribution of a numerical value as follows:\n\nWe first cut up the x-axis into a series of bins, where each bin represents a range of values.\nFor each bin, we count the number of observations that fall in the range corresponding to that bin.\nThen for each bin, we draw a bar whose height marks the corresponding count.\n\nLet’s drill-down on an example of a histogram, shown in Figure 8.7.\n\n\n\n\n\nFigure 8.7: Example histogram.\n\n\n\n\nLet’s focus only on values between 10 points and 20 points for now. Observe that there are five bins of equal width between 10 points and 20 points. Thus we have five bins of width 2 points each: one bin for the 10-12 range, another bin for the 13-14 range, etc.\n\nThe bin for the 10-12 range has a height of around 150. In other words, around 150 players scored a season average of between 10 and 20 points.\nThe bin for the 13-14 range has a height of around 100. In other words, around 100. players scored a season average of between 13 and 14 points.\nThe bin for the 15-16 range has a height of around 50. In other words, around 50 players scored a season average of between 15 and 16 points.\nAnd so on…\n\n\n8.5.1 Histograms via geom_histogram\nLet’s now present the ggplot() code to plot your first histogram! Unlike with scatterplots and linegraphs, there is now only one variable being mapped in aes(): the single numerical variable pts. The y-aesthetic of a histogram, the count of the observations in each bin, gets computed for you automatically. Furthermore, the geometric object layer is now a geom_histogram(). After running the following code, you’ll see the histogram in Figure 8.8 as well as a warning message. We’ll discuss the warning message first.\n\nggplot(data = nba %&gt;%\n        group_by(player_name) %&gt;%\n        summarize(pts = mean(pts)),\n       mapping = aes(x = pts)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 8.8: Histogram of average pts per game.\n\n\n\n\nThe warning is telling us that the histogram was constructed using bins = 30 for 30 equally spaced bins. This is known the default value for this argument (Section 2.4.1). We’ll see in the next section how to change the number of bins to another value than the default.\nNow let’s unpack the resulting histogram in Figure 8.9. Observe that values greater than 20 are rather rare. However, because of the large number of bins, it’s hard to get a sense for which range of temperatures is spanned by each bin; everything is one giant amorphous blob. So let’s add white vertical borders demarcating the bins by adding a color = \"white\" argument to geom_histogram() and ignore the warning about setting the number of bins to a better value:\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 8.9: Histogram of average pts per game.\n\n\n\n\nWe now have an easier time associating ranges of temperatures to each of the bins in Figure 8.10. We can also vary the color of the bars by setting the fill argument. For example, you can set the bin colors to be “blue steel” by setting fill = \"steelblue\":\n\nggplot(data = nba %&gt;%\n        group_by(player_name) %&gt;%\n        summarize(pts = mean(pts)),\n       mapping = aes(x = pts)) +\n  geom_histogram(color = \"white\", fill = \"steelblue\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 8.10: Histogram of average pts per game.\n\n\n\n\nIf you’re curious, run colors() to see all 657 possible choice of colors in R!\n\n\n8.5.2 Adjusting the bins\nObserve in Figure 8.10 that in the 10-20 range there appear to be roughly 111 bins. Thus each bin has width 20-10 divided by 11, or 0.91 points, which is not a very easily interpretable range to work with. Let’s improve this by adjusting the number of bins in our histogram in one of two ways:\n\nBy adjusting the number of bins via the bins argument to geom_histogram().\nBy adjusting the width of the bins via the binwidth argument to geom_histogram().\n\nUsing the first method, we have the power to specify how many bins we would like to cut the x-axis up in. As mentioned in the previous section, the default number of bins is 30. We can override this default, to say 25 bins, as follows:\n\nggplot(data = nba %&gt;%\n        group_by(player_name) %&gt;%\n        summarize(pts = mean(pts)),\n       mapping = aes(x = pts)) +\n  geom_histogram(bins = 25, color = \"white\")\n\nUsing the second method, instead of specifying the number of bins, we specify the width of the bins by using the binwidth argument in the geom_histogram() layer. For example, let’s set the width of each bin to be 5 points.\n\nggplot(data = nba %&gt;%\n        group_by(player_name) %&gt;%\n        summarize(pts = mean(pts)),\n       mapping = aes(x = pts)) +\n  geom_histogram(binwidth = 5, color = \"white\")\n\nWe compare both resulting histograms side-by-side in Figure 8.11.\n\n\n\n\n\nFigure 8.11: Setting histogram bins in two ways.\n\n\n\n\n\n\n8.5.3 Summary\nHistograms, unlike scatterplots and linegraphs, present information on only a single numerical variable. Specifically, they are visualizations of the distribution of the numerical variable in question."
  },
  {
    "objectID": "ggplot2.html#sec-facets",
    "href": "ggplot2.html#sec-facets",
    "title": "8  Visualizing Data",
    "section": "8.6 Facets",
    "text": "8.6 Facets\nBefore continuing with the next of the 5NG, let’s briefly introduce a new concept called faceting. Faceting is used when we’d like to split a particular visualization by the values of another variable. This will create multiple copies of the same type of plot with matching x and y axes, but whose content will differ.\nFor example, suppose we were interested in looking at how the histogram of players’ average points per game changed across seasons. We could “split” this histogram so that we had a separate histogram of pts for each of several values of season_int. We do this by adding facet_wrap(~ season_int) layer. Note the ~ is a “tilde” and can generally be found on the key next to the “1” key on US keyboards. The tilde is required and you’ll receive the error Error in as.quoted(facets) : object 'season_int' not found if you don’t include it here.\n\nggplot(\n    data = nba %&gt;%\n        group_by(player_name, season_int) %&gt;%\n        summarize(pts = mean(pts), season_int=first(season_int)),\n    mapping = aes(x = pts)\n) +\n    geom_histogram(binwidth = 5, color = \"white\") +\n    facet_wrap( ~ season_int)\n\n`summarise()` has grouped output by 'player_name'. You can override using the\n`.groups` argument.\n\n\n\n\n\nFigure 8.12: Faceted histogram of points per game.\n\n\n\n\nWe can also specify the number of rows and columns in the grid by using the nrow and ncol arguments inside of facet_wrap(). For example, say we would like our faceted histogram to have 4 rows instead of 3. We simply add an nrow = 4 argument to facet_wrap(~ season_int).\n\nggplot(\n    data = nba %&gt;%\n        group_by(player_name, season_int) %&gt;%\n        summarize(pts = mean(pts), season_int=first(season_int)),\n    mapping = aes(x = pts)\n) +\n    geom_histogram(binwidth = 5, color = \"white\") +\n    facet_wrap( ~ season_int, nrow = 4)\n\n`summarise()` has grouped output by 'player_name'. You can override using the\n`.groups` argument.\n\n\n\n\n\nFigure 8.13: Faceted histogram of points per game."
  },
  {
    "objectID": "ggplot2.html#sec-boxplots",
    "href": "ggplot2.html#sec-boxplots",
    "title": "8  Visualizing Data",
    "section": "8.7 5NG#4: Boxplots",
    "text": "8.7 5NG#4: Boxplots\nThough faceted histograms are one type of visualization used to compare the distribution of a numerical variable split by the values of another variable, another type of visualization that achieves a similar goal is a side-by-side boxplot. A boxplot is constructed from the information provided in the five-number summary of a numerical variable.\nLet’s again consider the distribution of points. For now, let’s confine ourselves to the 1996 season to keep things simple.\n\nbase_plot &lt;- nba %&gt;%\n        filter(season_int %in% c(1996)) %&gt;%\n    ggplot(mapping = aes(x = factor(season_int), y=pts))\nbase_plot + geom_jitter(width = 0.1, height = 0, alpha = 0.3)\n\n\n\n\nFigure 8.14: Points from 1996 represented as jittered points.\n\n\n\n\nThese observations have the following five-number summary:\n\nMinimum: 0\nFirst quartile (25th percentile): 3 points\nMedian (second quartile, 50th percentile): 6 points\nThird quartile (75th percentile): 12 points\nMaximum: 29.6\n\nIn the leftmost plot of Figure 8.15, let’s mark these 5 values with dashed horizontal lines on top of the actual data points. In the middle plot of Figure 8.15 let’s add the boxplot. In the rightmost plot of Figure 8.15, let’s remove the points and the dashed horizontal lines for clarity’s sake.\n\n\n\n\n\nFigure 8.15: Building up a boxplot of points\n\n\n\n\nWhat the boxplot does is visually summarize the points by cutting them into quartiles at the dashed lines, where each quartile contains four equally-size groups of observations. Thus\n\n25% of points fall below the bottom edge of the box, which is the first quartile of 3 points. In other words, 25% of observations were below 3 points.\n25% of points fall between the bottom edge of the box and the solid middle line, which is the median of 6 points. Thus, 25% of observations were between 3 points and 6 points and 50% of observations were below 6 points.\n25% of points fall between the solid middle line and the top edge of the box, which is the third quartile of 12 points. It follows that 25% of observations were between 6 points and 12 points and 75% of observations were below 12 points.\n25% of points fall above the top edge of the box. In other words, 25% of observations were above 12 points.\nThe middle 50% of points lie within the interquartile range (IQR) between the first and third quartile. Thus, the IQR for this example is 3 - 12 = -9 point. The interquartile range is one measure of a numerical variable’s spread.\n\nFurthermore, in the rightmost plot of Figure 8.15, we see the whiskers of the boxplot. The whiskers stick out from either end of the box all the way to the minimum and maximum observations of 0 and 29.6 points, respectively. However, the whiskers don’t always extend to the smallest and largest observed values as they do here. They in fact extend no more than 1.5 \\(\\times\\) the interquartile range from either end of the box. In this case, we see a small number of observations that lie more than 1.5 \\(\\times\\) -9 points = -13.5 points the top of the box. These observations are are called outliers.\n\n8.7.1 Boxplots via geom_boxplot\nLet’s now create a side-by-side boxplot of players’ average points per game split by the different seasons as we did previously with the faceted histograms. We do this by mapping the season_int variable to the x-position aesthetic, the pts variable to the y-position aesthetic, and by adding a geom_boxplot() layer:\n\nggplot(data = nba, mapping = aes(x = season_int, y = pts)) +\n  geom_boxplot()\n\nWarning: Continuous x aesthetic\nℹ did you forget `aes(group = ...)`?\n\n\n\n\n\nFigure 8.16: Invalid boxplot specification.\n\n\n\n\nObserve in Figure 8.16 that this plot does not provide information about points separated by season. The warning message clues us in as to why. It is telling us that we have a “continuous”, or numerical variable, on the x-position aesthetic. Boxplots, however, require a categorical variable to be mapped to the x-position aesthetic.\nWe can convert the numerical variable season_int into a factor categorical variable by using the factor() function. So after applying factor(season_int), season_int goes from having numerical values just the 1995, 1996, etc. to having an associated ordering. With this ordering, ggplot() now knows how to work with this variable to produce the needed plot.\n\nggplot(data = nba, mapping = aes(x = factor(season_int), y = pts)) +\n  geom_boxplot()\n\n\n\n\nFigure 8.17: Side-by-side boxplot of average points per game split by season.\n\n\n\n\nThe resulting Figure 8.17 shows 26 separate “box and whiskers” plots similar to the rightmost plot of Figure 8.15 of only data from 1996. Thus the different boxplots are shown “side-by-side.”\n\nThe “box” portions of the visualization represent the 1st quartile, the median (the 2nd quartile), and the 3rd quartile.\nThe height of each box (the value of the 3rd quartile minus the value of the 1st quartile) is the interquartile range (IQR). It is a measure of the spread of the middle 50% of values, with longer boxes indicating more variability.\nThe “whisker” portions of these plots extend out from the bottoms and tops of the boxes and represent points less than the 25th percentile and greater than the 75th percentiles, respectively. They’re set to extend out no more than \\(1.5 \\times IQR\\) units away from either end of the boxes. We say “no more than” because the ends of the whiskers have to correspond to observed points per game averages The length of these whiskers show how the data outside the middle 50% of values vary, with longer whiskers indicating more variability.\nThe dots representing values falling outside the whiskers are called outliers. These can be thought of as potentially anomalous (“out-of-the-ordinary”) values.\n\nIt is important to keep in mind that the definition of an outlier is somewhat arbitrary and not absolute. In this case, they are defined by the length of the whiskers, which are no more than \\(1.5 \\times IQR\\) units long for each boxplot. Looking at this side-by-side plot we can easily compare scorring distributions across seasons by drawing imaginary horizontal lines across the plot. Furthermore, the heights of the boxes as quantified by the interquartile ranges are informative too; they tell us about variability, or spread, of the different players’ averages recorded in a given season.\n\n\n8.7.2 Summary\nSide-by-side boxplots provide us with a way to compare the distribution of a numerical variable across multiple values of another variable. One can see where the median falls across the different groups by comparing the solid lines in the center of the boxes.\nTo study the spread of a numerical variable within one of the boxes, look at both the length of the box and also how far the whiskers extend from either end of the box. Outliers are even more easily identified when looking at a boxplot than when looking at a histogram as they are marked with distinct points."
  },
  {
    "objectID": "ggplot2.html#sec-geombar",
    "href": "ggplot2.html#sec-geombar",
    "title": "8  Visualizing Data",
    "section": "8.8 5NG#5: Barplots",
    "text": "8.8 5NG#5: Barplots\nBoth histograms and boxplots are tools to visualize the distribution of numerical variables. Another commonly desired task is to visualize the distribution of a categorical variable. This is a simpler task, as we are simply counting different categories within a categorical variable, also known as the levels of the categorical variable. Often the best way to visualize these different counts, also known as frequencies, is with barplots (also called barcharts).\nOne complication, however, is how your data is represented. Is the categorical variable of interest “pre-counted” or not? For example, run the following code that manually creates two data frames representing a collection of fruit: 3 apples and 2 oranges.\n\nfruits &lt;- tibble(\n  fruit = c(\"apple\", \"apple\", \"orange\", \"apple\", \"orange\")\n)\nfruits_counted &lt;- tibble(\n  fruit = c(\"apple\", \"orange\"),\n  number = c(3, 2)\n)\n\nWe see both the fruits and fruits_counted data frames represent the same collection of fruit. Whereas fruits just lists the fruit individually…\n\n\n# A tibble: 5 × 1\n  fruit \n  &lt;chr&gt; \n1 apple \n2 apple \n3 orange\n4 apple \n5 orange\n\n\n… fruits_counted has a variable count which represent the “pre-counted” values of each fruit.\n\n\n# A tibble: 2 × 2\n  fruit  number\n  &lt;chr&gt;   &lt;dbl&gt;\n1 apple       3\n2 orange      2\n\n\nDepending on how your categorical data is represented, you’ll need to add a different geometric layer type to your ggplot() to create a barplot, as we now explore.\n\n8.8.1 Barplots via geom_bar or geom_col\nLet’s generate barplots using these two different representations of the same basket of fruit: 3 apples and 2 oranges. Using the fruits data frame where all 5 fruits are listed individually in 5 rows, we map the fruit variable to the x-position aesthetic and add a geom_bar() layer:\n\nggplot(data = fruits, mapping = aes(x = fruit)) +\n  geom_bar()\n\n\n\n\nFigure 8.18: Barplot when counts are not pre-counted.\n\n\n\n\nHowever, using the fruits_counted data frame where the fruits have been “pre-counted”, we once again map the fruit variable to the x-position aesthetic, but here we also map the count variable to the y-position aesthetic, and add a geom_col() layer instead.\n\nggplot(data = fruits_counted, mapping = aes(x = fruit, y = number)) +\n  geom_col()\n\n\n\n\nFigure 8.19: Barplot when counts are pre-counted.\n\n\n\n\nCompare the barplots in Figures Figure 8.18 and Figure 8.19. They are identical because they reflect counts of the same five fruits. However, depending on how our categorical data is represented, either “pre-counted” or not, we must add a different geom layer. When the categorical variable whose distribution you want to visualize\n\nIs not pre-counted in your data frame, we use geom_bar().\nIs pre-counted in your data frame, we use geom_col() with the y-position aesthetic mapped to the variable that has the counts.\n\nLet’s now go back to the nba data frame and visualize the distribution of the categorical variable college. Specifically, we’ll visualize the number of players who graduated from different colleges. We’ll focus on the New York Knicks (team_abbreviation==NYK) and data from the 2006-2009 seasons.\nRecall from Chapter 7, you saw that each row in the nba data set corresponds to a player in a given year In other words, the nba data frame is more like the fruits data frame than the fruits_counted data frame because the numbers of players from each college have not been pre-counted. Thus we should use geom_bar() instead of geom_col() to create a barplot. Much like a geom_histogram(), there is only one variable in the aes() aesthetic mapping: the variable college gets mapped to the x-position. As a difference though, histograms typically have bars that touch whereas bar graphs typically have space between the bars.\n\nnba %&gt;%\n    group_by(player_name, college) %&gt;%\n    filter(team_abbreviation %in% c(\"NYK\"),\n           season_int &lt; 2010,\n       season_int &gt; 2005) %&gt;%\n    ggplot(mapping = aes(x = college)) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90))\n\n\n\n\nFigure 8.20: Number of players by colege using geom_bar().\n\n\n\n\nObserve in Figure 8.20 that there are many Knicks players who either did not attend any college (“None”), attended Arizona State, or attended Florida. If you don’t know which airlines correspond to which carrier codes, then run View(airlines) to see a directory of airlines. For example, B6 is JetBlue Airways. Alternatively, say you had a data frame where the number of flights for each carrier was pre-counted as in Table @ref(tab:flights-counted).\n\n\n\n\n\nNumber of players, pre-counted for each college\n\n\ncollege\nnumber\n\n\n\n\nArizona\n1\n\n\nArizona State\n1\n\n\nDePaul\n6\n\n\nDrexel\n2\n\n\nDuke\n2\n\n\nFlorida\n4\n\n\nFlorida A&M\n3\n\n\nFlorida State\n1\n\n\nGeorgia Tech\n2\n\n\nIndiana\n3\n\n\nIowa State\n1\n\n\nKansas State\n1\n\n\nKentucky\n3\n\n\nMaryland\n2\n\n\nMemphis\n1\n\n\nMichigan\n3\n\n\nMichigan State\n1\n\n\nNew Mexico\n1\n\n\nNone\n13\n\n\nOregon\n1\n\n\nSaint Louis\n1\n\n\nSouth Carolina\n2\n\n\nSyracuse\n1\n\n\nTemple\n2\n\n\nWashington\n3\n\n\n\n\n\n\nFigure 8.21: ?(caption)\n\n\n\nIn order to create a barplot visualizing the distribution of the categorical variable college in this case, we would now use geom_col() instead of geom_bar(), with an additional y = number in the aesthetic mapping on top of the x = carrier. The resulting barplot would be identical to Figure 8.20.\n\n\n8.8.2 Must avoid pie charts!\nOne of the most common plots used to visualize the distribution of categorical data is the pie chart. Though they may seem harmless, pie charts actually present a problem in that humans are unable to judge angles well. As Naomi Robbins describes in her book, Creating More Effective Graphs (robbins2013?), people tend to overestimate angles greater than 90 degrees and underestimate angles less than 90 degrees. In other words, it is difficult to determine the relative size of one piece of the pie compared to another. So stay away!\n\n\n8.8.3 Two categorical variables\nBarplots are a very common way to visualize the frequency of different categories, or levels, of a single categorical variable. Another use of barplots is to visualize the joint distribution of two categorical variables at the same time. Let’s examine the joint distribution of players by college as well as season. In other words, the number of players for each combination of college and season.\nFor example, the number of WestJet flights from JFK, the number of WestJet flights from LGA, the number of WestJet flights from EWR, the number of American Airlines flights from JFK, and so on. Recall the ggplot() code that created the barplot of carrier frequency in Figure @ref(fig:flightsbar):\nWe can now map the additional variable origin by adding a fill = origin inside the aes() aesthetic mapping.\n\nnba %&gt;%\n    group_by(player_name, college) %&gt;%\n    filter(team_abbreviation %in% c(\"NYK\"),\n           season_int &lt; 2010,\n           season_int &gt; 2005) %&gt;%\n    ggplot(mapping = aes(x = college, fill = factor(season_int))) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90))\n\n\n\n\nFigure 8.22: ?(caption)\n\n\n\n\nFigure 8.22 is an example of a stacked barplot. Though simple to make, in certain aspects it is not ideal. For example, it is not particularly easy to compare the heights of the different colors between the bars, corresponding to comparing the number of players from each season_int between the different teams.\nBefore we continue, let’s address some common points of confusion among new R users. First, the fill aesthetic corresponds to the color used to fill the bars, whereas the color aesthetic corresponds to the color of the outline of the bars. This is identical to how we added color to our histogram in Section 8.5.1: we set the outline of the bars to white by setting color = \"white\" and the colors of the bars to blue steel by setting fill = \"steelblue\". Observe in Figure 8.23 that mapping season_int to color and not fill yields grey bars with different colored outlines.\n\nnba %&gt;%\n    group_by(player_name, college) %&gt;%\n    filter(team_abbreviation %in% c(\"NYK\"),\n           season_int &lt; 2010,\n           season_int &gt; 2005) %&gt;%\n    ggplot(mapping = aes(x = college, color = factor(season_int))) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90))\n\n\n\n\nFigure 8.23: ?(caption)\n\n\n\n\nSecond, note that fill is another aesthetic mapping much like x-position; thus we were careful to include it within the parentheses of the aes() mapping. The following code, where the fill aesthetic is specified outside the aes() mapping will yield an error. This is a fairly common error that new ggplot users make:\n\n...\n    ggplot(mapping = aes(x = college), color = factor(season_int)) +\n    geom_bar() +\n\nAn alternative to stacked barplots are side-by-side barplots, also known as dodged barplots, as seen in Figure 8.24. The code to create a side-by-side barplot is identical to the code to create a stacked barplot, but with a position = \"dodge\" argument added to geom_bar(). In other words, we are overriding the default barplot type, which is a stacked barplot, and specifying it to be a side-by-side barplot instead.\n\nnba %&gt;%\n    group_by(player_name, college) %&gt;%\n    filter(team_abbreviation %in% c(\"NYK\"),\n           season_int &lt; 2010,\n           season_int &gt; 2005) %&gt;%\n    ggplot(mapping = aes(x = college, fill = factor(season_int))) +\n    geom_bar(position = \"dodge\") +\n    theme(axis.text.x = element_text(angle = 90))\n\n\n\n\nFigure 8.24: ?(caption)\n\n\n\n\nHere, the width of the bars for DuPaul, Florida, and None is different than the width of the bars for Arizona and Iowa State. We can make one tweak to the position argument to get them to be the same size in terms of width as the other bars by using the more robust position_dodge() function.\n\nnba %&gt;%\n    group_by(player_name, college) %&gt;%\n    filter(team_abbreviation %in% c(\"NYK\"),\n           season_int &lt; 2010,\n           season_int &gt; 2005) %&gt;%\n    ggplot(mapping = aes(x = college, fill = factor(season_int))) +\n    geom_bar(position = position_dodge(preserve = \"single\")) +\n    theme(axis.text.x = element_text(angle = 90))\n\n\n\n\nLastly, another type of barplot is a faceted barplot. Recall in Section 8.6 we visualized the distribution of players’ points split by season using facets. We apply the same principle to our barplot visualizing the frequency of college split by season_int: instead of mapping college to fill we include it as the variable to create small multiples of the plot across the levels of college.\n\nggplot(data = flights, mapping = aes(x = carrier)) +\n  geom_bar() +\n  facet_wrap(~ origin, ncol = 1)\n\n\nnba %&gt;%\n    group_by(player_name, college) %&gt;%\n    filter(team_abbreviation %in% c(\"NYK\"),\n           season_int &lt; 2010,\n           season_int &gt; 2005) %&gt;%\n    ggplot(mapping = aes(x = college)) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90)) +\n    facet_wrap(~season_int, ncol = 1)\n\n\n\n\nFigure 8.25: Faceted barplot comparing the number of flights by carrier and origin.\n\n\n\n\n\n\n8.8.4 Summary\nBarplots are a common way of displaying the distribution of a categorical variable, or in other words the frequency with which the different categories (also called levels) occur. They are easy to understand and make it easy to make comparisons across levels. Furthermore, when trying to visualize the relationship of two categorical variables, you have many options: stacked barplots, side-by-side barplots, and faceted barplots. Depending on what aspect of the relationship you are trying to emphasize, you will need to make a choice between these three types of barplots and own that choice."
  },
  {
    "objectID": "ggplot2.html#sec-data-vis-conclusion",
    "href": "ggplot2.html#sec-data-vis-conclusion",
    "title": "8  Visualizing Data",
    "section": "8.9 Conclusion",
    "text": "8.9 Conclusion\n\n8.9.1 Summary table\nLet’s recap all five of the five named graphs (5NG) in Table 8.1 summarizing their differences. Using these 5NG, you’ll be able to visualize the distributions and relationships of variables contained in a wide array of datasets. This will be even more the case as we start to map more variables to more of each geometric object’s aesthetic attribute options, further unlocking the awesome power of the ggplot2 package.\n\n\nTable 8.1: My Caption\n\n\n\n\n\n\n\n\nNamed graph\nShows\nGeometric object\nNotes\n\n\n\n\nScatterplot\nRelationship between 2 numerical variables\ngeom_point()\n\n\n\nLinegraph\nRelationship between 2 numerical variables\ngeom_line()\nUsed when there is a sequential order to x-variable, e.g., time\n\n\nHistogram\nDistribution of 1 numerical variable\ngeom_histogram()\nFacetted histograms show the distribution of 1 numerical variable split by the values of another variable\n\n\nBoxplot\nDistribution of 1 numerical variable split by the values of another variable\ngeom_boxplot()\nC\n\n\nBarplot\nDistribution of 1 categorical variable\ngeom_bar() when counts are not pre-counted, geom_col() when counts are pre-counted `\nStacked, side-by-side, and faceted barplots show the joint distribution of 2 categorical variables\n\n\n\n\n\n\n8.9.2 Function argument specification\nLet’s go over some important points about specifying the arguments (i.e., inputs) to functions. Run the following two segments of code:\n\n# Segment 1:\nggplot(data = nba, mapping = aes(x = team_abbreviation)) +\n  geom_bar()\n\n# Segment 2:\nggplot(flights, aes(x = team_abbreviation)) +\n  geom_bar()\n\nYou’ll notice that both code segments create the same barplot, even though in the second segment we omitted the data = and mapping = code argument names. This is because the ggplot() function by default assumes that the data argument comes first and the mapping argument comes second. As long as you specify the data frame in question first and the aes() mapping second, you can omit the explicit statement of the argument names data = and mapping =. That being said, explicit is better than implicit. Given the uniformity of the tidyverse packages, you will often see the data= argument name omitted (because the first argument of all tidyverse function is a tibble), but it is good practice to include the names of other arguments for readability and clarity purposes.\n\n\n\n\nWilkinson, Leland. 2012. The Grammar of Graphics. Springer."
  },
  {
    "objectID": "descriptives.html#sec-centraltendency",
    "href": "descriptives.html#sec-centraltendency",
    "title": "9  Descriptive Statistics",
    "section": "9.1 Measures of central tendency",
    "text": "9.1 Measures of central tendency\nDrawing pictures of the data, as we did in Section 8.5 is an excellent way to convey the “gist” of what the data is trying to tell you, it’s often extremely useful to try to condense the data into a few simple “summary” statistics. In most situations, the first thing that you’ll want to calculate is a measure of central tendency. That is, you’d like to know something about the “average” or “middle” of your data lies. The most commonly used measures are the mean, median and mode.\n\n9.1.1 The mean\nThe mean of a set of observations is just a normal, old-fashioned average: add all of the values up, and then divide by the total number of values. If we have five observations (56, 31, 56, 8 and 32), the mean of these observations is: \\[\n\\frac{56 + 31 + 56 + 8 + 32}{5} = \\frac{183}{5} = 36.60\n\\] Of course, this definition of the mean isn’t news to anyone: averages (i.e., means) are used so often in everyday life that this is pretty familiar stuff. However, since the concept of a mean is something that everyone already understands, I’ll use this as an excuse to start introducing some of the mathematical notation that statisticians use to describe this calculation, and talk about how the calculations would be done in R.\nHow do we get the magic computing box to do all this work for us? If you really wanted to, you could do this calculation directly in R.\n\n(56 + 31 + 56 + 8 + 32) / 5\n\n[1] 36.6\n\n\nHowever, that’s not the only way to do the calculations, and when the number of observations starts to become large, it’s easily the most tedious. We could use the sum() function to get the numerator for us\n\nsum(c(56, 31, 56, 8, 32)) / 5\n\n[1] 36.6\n\n\nThis is ok, but we can do it even more easily using the mean() function.\n\nmean(c(56, 31, 56, 8, 32))\n\n[1] 36.6\n\n\nAs you can see, this gives exactly the same answers as the previous calculations.\n\n\n9.1.2 The median\nThe second measure of central tendency that people use a lot is the median, and it’s even easier to describe than the mean. The median of a set of observations is just the middle value. As before let’s imagine we were interested only in the five values from before. To figure out the median, we sort these numbers into ascending order: \\[\n8, 31, \\mathbf{32}, 56, 56\n\\] From inspection, it’s obvious that the median value of these 5 observations is 32, since that’s the middle one in the sorted list (I’ve put it in bold to make it even more obvious). Easy stuff. But what should we do if we were interested in the first 6 games rather than the first 5? Since the sixth game in the season had a winning margin of 14 points, our sorted list is now \\[\n8, 14, \\mathbf{31}, \\mathbf{32}, 56, 56\n\\] and there are two middle numbers, 31 and 32. The median is defined as the average of those two numbers, which is of course 31.5. As before, it’s very tedious to do this by hand when you’ve got lots of numbers. To illustrate this, here’s what happens when you use R to sort the values. First, I’ll use the sort() function to display the values in increasing numerical order:\n\nsort(c(56, 31, 56, 8, 32, 14))\n\n[1]  8 14 31 32 56 56\n\n\nThe middle values are 30 and 31, so the median winning margin for 2010 was 30.5 points. In real life, of course, no-one actually calculates the median by sorting the data and then looking for the middle value. In real life, we use the median command:\n\nmedian(c(56, 31, 56, 8, 32, 14))\n\n[1] 31.5\n\n\nwhich outputs the value we expect.\n\n\n9.1.3 Mean or median?\nKnowing how to calculate means and medians is only a part of the story. You also need to understand what each one is saying about the data, and what that implies for when you should use each one. The mean is kind of like the “center of gravity” of the data set, whereas the median is the “middle value” in the data. What this implies that the appropriate measure of central tendency depends on what type of data you’ve got and what you’re trying to achieve. As a rough guide:\n\nIf your data are nominal scale, you probably shouldn’t be using either the mean or the median. Both the mean and the median rely on the idea that the numbers assigned to values are meaningful. If the numbering scheme is arbitrary, then it’s probably best to use the mode (Section 9.1.4) instead.\nIf your data are ordinal scale, you’re more likely to want to use the median than the mean. The median only makes use of the order information in your data (i.e., which numbers are bigger), but doesn’t depend on the precise numbers involved. That’s exactly the situation that applies when your data are ordinal scale. The mean, on the other hand, makes use of the precise numeric values assigned to the observations, so it’s not really appropriate for ordinal data.\nFor interval and ratio scale data, either one is generally acceptable. Which one you pick depends a bit on what you’re trying to achieve. The mean has the advantage that it uses all the information in the data (which is useful when you don’t have a lot of data), but it’s very sensitive to extreme values. Let’s expand on that last part a little. One consequence is that there’s systematic differences between the mean and the median when the histogram is asymmetric (or skewed, see Section 9.3). Many, many measurements are highly asymmetric, including those measured in dollars (e.g., home prices, income, etc.). If you’re interested in looking at the overall income among a group of people, the mean might be a good answer; but if you’re interested in a typical income among those same people, the median may be a better choice.\n\n\n\n9.1.4 Mode\nThe mode of a sample is very simple: it is the value that occurs most frequently within a set of measurements. To illustrate the mode using the nba data, let’s examine the teams represented in the data set Let’s take a look at the first few values of this variable.\n\nhead(nba$team_abbreviation)\n\n[1] \"CHI\" \"LAC\" \"TOR\" \"DAL\" \"MIA\" \"HOU\"\n\n\nThese are strings (or character vectors) that represent the team each player is plays for. We could read through all 400, and count the number of occasions on which each team name appears in our list of finalists, thereby producing a frequency table. However, that would be mindless and boring: exactly the sort of task that computers are great at. So let’s use the table() function (discussed in more detail in Section @ref(freqtables)) to do this task for us:\n\ntable(nba$team_abbreviation)\n\n\nATL BKN BOS CHA CHH CHI CLE DAL DEN DET GSW HOU IND LAC LAL MEM MIA MIL MIN NJN \n421 180 407 288  89 406 433 422 412 400 409 418 410 424 411 352 426 409 399 257 \nNOH NOK NOP NYK OKC ORL PHI PHX POR SAC SAS SEA TOR UTA VAN WAS \n143  32 159 410 239 411 420 399 407 398 413 182 428 397  72 422 \n\n\nIf you spend enough time staring at this frequency table, you may be able to figure out that there are more rows associated with “CLE” (the Cleveland Cavaliers) than any other team. Thus, the mode of the team_abbreviation data is \"CLE\".\nSomewhat surprisingly, neither the core functionality of R nor the tidyverse has a function for calculating the mode. However, you can find packages with such functionality (e.g., modeest). You can also find mode-calculating function that others have put together (e.g., on StackOverflow). Refer back to ?sec-functions to remind yourself how functions work."
  },
  {
    "objectID": "descriptives.html#sec-var",
    "href": "descriptives.html#sec-var",
    "title": "9  Descriptive Statistics",
    "section": "9.2 Measures of variability",
    "text": "9.2 Measures of variability\nThe statistics that we’ve discussed so far all relate to central tendency. That is, they all talk about which values are “in the middle” or “popular” in the data. However, central tendency is not the only type of summary statistic that we want to calculate. The second thing that we really want is a measure of the variability of the data. That is, how “spread out” are the data? How “far” away from the mean or median do the observed values tend to be? For now, let’s assume that the data are interval or ratio scale, so we’ll use the age variable in our nba data set. We’ll use this data to discuss several different measures of spread, each with different strengths and weaknesses.\n\n9.2.1 Range\nThe range of a variable is very simple: it’s the biggest value minus the smallest value. For the data, the maximum value of age is 44, and the minimum value is 18. We can calculate these values in R using the max() and min() functions:\n\nmax(nba$age)\n\n[1] 44\n\nmin(nba$age)\n\n[1] 18\n\n\nThe other possibility is to use the range() function; which outputs both the minimum value and the maximum value in a vector, like this:\n\nrange(nba$age)\n\n[1] 18 44\n\n\nAlthough the range is the simplest way to quantify the notion of “variability”, it’s one of the worst. Recall from our discussion of the mean that we want our summary measure to be robust. If the data set has one or two extremely bad values in it, we’d like our statistics not to be unduly influenced by these cases. If we look once again at our toy example of a data set containing very extreme outliers…\n\\[\n-100,2,3,4,5,6,7,8,9,10\n\\] … it is clear that the range is not robust, since this has a range of 110, but if the outlier were removed we would have a range of only 8. The problem with the range is that it only considers two numbers: the mininum and the maximum. It completely ignores every other value.\n\n\n9.2.2 Interquartile range\nThe interquartile range (IQR) is like the range, but instead of calculating the difference between the biggest and smallest value, it calculates the difference between the 25th quantile and the 75th quantile. Probably you already know what a quantile is (they’re more commonly called percentiles), but if not: the 10th percentile of a data set is the smallest number \\(x\\) such that 10% of the data is less than \\(x\\). In fact, we’ve already come across the idea: the median of a data set is its 50th quantile / percentile! R actually provides you with a way of calculating quantiles, using the (surprise, surprise) quantile() function. Let’s use it to calculate the median AFL winning margin:\n\nquantile(x=nba$age, probs=.5)\n\n50% \n 26 \n\n\nAnd not surprisingly, this agrees with the answer that we saw earlier with the median() function. Now, we can actually input lots of quantiles at once, by specifying a vector for the probs argument. So lets do that, and get the 25th and 75th percentile:\n\nquantile(x=nba$age, probs=c(.25,.75))\n\n25% 75% \n 24  30 \n\n\nAnd, by noting that \\(30 - 24 = 6\\), we can see that the interquartile range for the age variable is 6. Of course, that seems like too much work to do all that typing, so R has a built in function called IQR() that we can use:\n\nIQR(x=nba$age)\n\n[1] 6\n\n\nThough it’s obvious how to interpret the range, it’s a little less obvious how to interpret the IQR. The simplest way to think about it is like this: the interquartile range is the range spanned by the “middle half” of the data. That is, one quarter of the data falls below the 25th percentile, one quarter of the data is above the 75th percentile, leaving the “middle half” of the data lying in between the two. And the IQR is the range covered by that middle half.\n\n\n9.2.3 Mean absolute deviation\nThe two measures we’ve looked at so far, the range and the interquartile range, both rely on the idea that we can measure the spread of the data by looking at the quantiles of the data. However, this isn’t the only way to think about the problem. A different approach is to select a meaningful reference point (usually the mean or the median) and then report the “typical” deviations from that reference point. What do we mean by “typical” deviation? Usually, the mean or median value of these deviations! In practice, this leads to two different measures, the “mean absolute deviation (from the mean)” and the “median absolute deviation (from the median)”. From what I’ve read, the measure based on the median seems to be used in statistics, and does seem to be the better of the two, but to be honest I don’t think I’ve seen it used much in psychology. The measure based on the mean does occasionally show up in psychology though. In this section I’ll talk about the first one, and I’ll come back to talk about the second one later.\nSince the previous paragraph might sound a little abstract, let’s go through the mean absolute deviation from the mean a little more slowly. One useful thing about this measure is that the name actually tells you exactly how to calculate it. Let’s think about values of56, 31, 56, 8 and 32 again. Since our calculations rely on an examination of the deviation from some reference point (in this case the mean), the first thing we need to calculate is the mean. For these five observations, our mean is 36.6. The next step is to convert each of our observations into a deviation score. We do this by calculating the difference between the observation and the mean. For the first observation in our sample, this is equal to \\(56 - 36.6 = 19.4\\). Okay, that’s simple enough. The next step in the process is to convert these deviations to absolute deviations. We do this by converting any negative values to positive ones. Mathematically, we would denote the absolute value of \\(-3\\) as \\(|-3|\\), and so we say that \\(|-3| = 3\\). We use the absolute value function (abs() in R) here because we don’t really care whether the value is higher than the mean or lower than the mean, we’re just interested in how close it is to the mean. To help make this process as obvious as possible, the table below shows these calculations for all five observations:\nNow that we have calculated the absolute deviation score for every observation in the data set, all that we have to do to calculate the mean of these scores. Let’s do that:\n\\[\n\\frac{19.4 + 5.6 + 19.4 + 28.6 + 4.6}{5} = 15.52\n\\]\nAnd we’re done. The mean absolute deviation for these five scores is 15.52.\nHowever, while our calculations for this little example are at an end, we do have a couple of things left to talk about. Firstly, we should really try to write down a proper mathematical formula. But in order do to this I need some mathematical notation to refer to the mean absolute deviation. Irritatingly, “mean absolute deviation” and “median absolute deviation” have the same acronym (MAD), which leads to a certain amount of ambiguity, and since R tends to use MAD to refer to the median absolute deviation, I’d better come up with something different for the mean absolute deviation. Sigh. What I’ll do is use AAD instead, short for average absolute deviation. Now that we have some unambiguous notation, here’s the formula that describes what we just calculated: \\[\n\\mbox{}(X) = \\frac{1}{N} \\sum_{i = 1}^N |X_i - \\bar{X}|\n\\]\nThe last thing we need to talk about is how to calculate AAD in R. One possibility would be to do everything using low level commands, laboriously following the same steps that I used when describing the calculations above. However, that’s pretty tedious. You’d end up with a series of commands that might look like this:\n\nX &lt;- c(56, 31,56,8,32)   # enter the data\nX.bar &lt;- mean( X )       # step 1. the mean of the data\nAD &lt;- abs( X - X.bar )   # step 2. the absolute deviations from the mean\nAAD &lt;- mean( AD )        # step 3. the mean absolute deviations\nprint( AAD )             # print the results\n\n[1] 15.52\n\n\nEach of those commands is pretty simple, but there’s just too many of them. And because I find that to be too much typing. If you find youself needing to invoke these commands repeatedly, you might consider building function to do so (?sec-functions).\n\n\n9.2.4 Variance\nAlthough the mean absolute deviation measure has its uses, it’s not the best measure of variability to use. From a purely mathematical perspective, there are some solid reasons to prefer squared deviations rather than absolute deviations. If we do that, we obtain a measure is called the variance, which has a lot of really nice statistical properties that we’ll ignore for now and one massive psychological flaw that we’ll make a big deal out of in a moment. The variance of a data set is sometimes written as \\(\\mbox{Var}(X)\\), but it’s more commonly denoted \\(s^2\\) (the reason for this will become clearer shortly). The formula that we use to calculate the variance of a set of observations is as follows:\n\\[\n\\mbox{Var}(X) = \\frac{1}{N} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right)^2\n\\] \\[\\mbox{Var}(X) = \\frac{\\sum_{i=1}^N \\left( X_i - \\bar{X} \\right)^2}{N}\\]\nAs you can see, it’s basically the same formula that we used to calculate the mean absolute deviation, except that instead of using “absolute deviations” we use “squared deviations”. It is for this reason that the variance is sometimes referred to as the “mean square deviation”.\nNow that we’ve got the basic idea, let’s have a look at a concrete example. Once again, let’s use the five values we have been working with so far. If we follow the same approach that we took last time, we end up with the following table:\n\n\n\nBasic arithmetic operations in R. These five operators are used very frequently throughout the text, so it’s important to be familiar with them at the outset.\n\n\n\n\n\n\n\n\n\nNotation [English]\n\\(i\\) [which game]\n\\(X_i\\) [value]\n\\(X_i - \\bar{X}\\) [deviation from mean]\n\\((X_i - \\bar{X})^2\\) [absolute deviation]\n\n\n\n\n\n1\n56\n19.4\n376.36\n\n\n\n2\n31\n-5.6\n31.36\n\n\n\n3\n56\n19.4\n376.36\n\n\n\n4\n8\n-28.6\n817.96\n\n\n\n5\n32\n-4.6\n21.16\n\n\n\n\n\nThat last column contains all of our squared deviations, so all we have to do is average them. If we do that by typing all the numbers into R by hand…\n\n(376.36 + 31.36 + 376.36 + 817.96 + 21.16) / 5\n\n[1] 324.64\n\n\n… we end up with a variance of 324.64. Exciting, isn’t it? For the moment, let’s ignore the burning question that you’re all probably thinking (i.e., what the heck does a variance of 324.64 actually mean?) and instead talk a bit more about how to do the calculations in R, because this will reveal something very weird.\nAs always, we want to avoid having to type in a whole lot of numbers ourselves. And as it happens, we have the vector X lying around, which we created in the previous section. With this in mind, we can calculate the variance of X by using the following command,\n\nmean( (X - mean(X) )^2)\n\n[1] 324.64\n\n\nand as usual we get the same answer as the one that we got when we did everything by hand. However, I still think that this is too much typing. Fortunately, R has a built in function called var() which does calculate variances. So we could also do this…\n\nvar(X)\n\n[1] 405.8\n\n\nand you get the same… no, wait… you get a completely different answer. That’s just weird. Is R broken? Is this a typo?\nIt’s not a typo, and R is not making a mistake. To get a feel for what’s happening, let’s stop using the tiny data set containing only 5 data points, and switch to the full set ages that we’ve got stored in nba$age vector. First, let’s calculate the variance by using the formula that I described above:\n\nmean( (nba$age - mean(nba$age) )^2)\n\n[1] 18.79822\n\n\nNow let’s use the var() function:\n\nvar(nba$age)\n\n[1] 18.79975\n\n\nHm. These two numbers are very similar this time. That seems like too much of a coincidence to be a mistake. And of course it isn’t a mistake. In fact, it’s very simple to explain what R is doing here, but slightly trickier to explain why R is doing it. So let’s start with the “what”. What R is doing is evaluating a slightly different formula to the one I showed you above. Instead of averaging the squared deviations, which requires you to divide by the number of data points \\(N\\), R has chosen to divide by \\(N-1\\). In other words, the formula that R is using is this one\n\\[\n\\frac{1}{N-1} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right)^2\n\\] It’s easy enough to verify that this is what’s happening, as the following command illustrates:\n\nsum( (nba$age-mean(nba$age))^2 ) / (length(nba$age) - 1)\n\n[1] 18.79975\n\n\nThis is the same answer that R gave us originally when we calculated var(X) originally. So that’s the what. The real question is why R is dividing by \\(N-1\\) and not by \\(N\\). After all, the variance is supposed to be the mean squared deviation, right? So shouldn’t we be dividing by \\(N\\), the actual number of observations in the sample? Well, yes, we should. However, as we’ll discuss in Chapter @ref(estimation), there’s a subtle distinction between “describing a sample” and “making guesses about the population from which the sample came”. Up to this point, it’s been a distinction without a difference. Regardless of whether you’re describing a sample or drawing inferences about the population, the mean is calculated exactly the same way. Not so for the variance, or the standard deviation, or for many other measures besides. What I outlined to you initially (i.e., take the actual average, and thus divide by \\(N\\)) assumes that you literally intend to calculate the variance of the sample. Most of the time, however, you’re not terribly interested in the sample in and of itself. Rather, the sample exists to tell you something about the world. If so, you’re actually starting to move away from calculating a “sample statistic”, and towards the idea of estimating a “population parameter”. However, I’m getting ahead of myself. For now, let’s just take it on faith that R knows what it’s doing, and we’ll revisit the question later on.\nOkay, one last thing. This section so far has read a bit like a mystery novel. I’ve shown you how to calculate the variance, described the weird “\\(N-1\\)” thing that R does and hinted at the reason why it’s there, but I haven’t mentioned the single most important thing…how do you interpret the variance? Descriptive statistics are supposed to describe things, after all, and right now the variance is really just a gibberish number. Unfortunately, the reason why I haven’t given you the human-friendly interpretation of the variance is that there really isn’t one. This is the most serious problem with the variance. Although it has some elegant mathematical properties that suggest that it really is a fundamental quantity for expressing variation, it’s completely useless if you want to communicate with an actual human…variances are completely uninterpretable in terms of the original variable! All the numbers have been squared, and they don’t mean anything anymore. This is a huge issue. For instance, according to our calculations earlier, first value (i.e., 56) is “376.36 points-squared higher than the average”. This is exactly as stupid as it sounds; and so when we calculate a variance of 324.64, we’re in the same situation. You are probably familiar with lots of measurements (height, weight, dollars earned, etc.), but you rarely refer to “pounds squared” or “dollars squared”. It’s not a real unit of measurement, and since the variance is expressed in terms of this gibberish unit, it is totally meaningless to a human.\n\n\n9.2.5 Standard deviation\nOkay, suppose that you like the idea of using the variance because of those nice mathematical properties that I haven’t talked about, but – since you’re a human and not a robot – you’d like to have a measure that is expressed in the same units as the data itself (i.e., points, not points-squared). What should you do? The solution to the problem is obvious: take the square root of the variance, known as the standard deviation, also called the “root mean squared deviation”, or RMSD. This solves out problem fairly neatly: while nobody has a clue what “a variance of XYZ dollars-squared” really means, it’s much easier to understand “a standard deviation of $12”, since it’s expressed in the units the measurements were originally made in. It is traditional to refer to the standard deviation of a sample of data as \\(s\\), though “SD” and “std dev.” are also used at times. Because the standard deviation is equal to the square root of the variance, you probably won’t be surprised to see that the formula is:\n\\[\ns = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right)^2 }\n\\]\nand the R function that we use to calculate it is sd(). However, as you might have guessed from our discussion of the variance, what R actually calculates is slightly different to the formula given above. Just like the we saw with the variance, what R calculates is a version that divides by \\(N-1\\) rather than \\(N\\). For reasons that will make sense when we return to this topic in Chapter@refch:estimation I’ll refer to this new quantity as \\(\\hat\\sigma\\) (read as: “sigma hat”), and the formula for this is\n\\[\n\\hat\\sigma = \\sqrt{ \\frac{1}{N-1} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right)^2 }\n\\]\nWith that in mind, calculating standard deviations in R is simple:\n\nsd(X)\n\n[1] 20.14448\n\n\nInterpreting standard deviations is slightly more complex. Because the standard deviation is derived from the variance, and the variance is a quantity that has little to no meaning that makes sense to us humans, the standard deviation doesn’t have a simple interpretation. As a consequence, most of us just rely on a simple rule of thumb: in general, you should expect 68% of the data to fall within 1 standard deviation of the mean, 95% of the data to fall within 2 standard deviation of the mean, and 99.7% of the data to fall within 3 standard deviations of the mean. This rule tends to work pretty well most of the time, but it’s not exact: it’s actually calculated based on an assumption that the histogram is symmetric and “bell-shaped” (strictly speaking, it assumes measurements are normally-distributed). When data is not symmetric and/or not bell-shaped, the rule is approximately correct. As it turns out, 65.3% of the AFL margins data fall within one standard deviation of the mean. This is shown visually in Figure @ref(fig:aflsd).\n\n\nWarning: `position_stack()` requires non-overlapping x intervals\n\n\n\n\n\nFigure 9.1: An illustration of the standard deviation, applied to nba$age. The red bars in the histogram show how much of the data fall within one standard deviation of the mean.\n\n\n\n\n\n\n9.2.6 Median absolute deviation\nThe last measure of variability that I want to talk about is the median absolute deviation (MAD). The basic idea behind MAD is very simple, and is pretty much identical to the idea behind the mean absolute deviation (Section 9.2.3). The difference is that you use the median everywhere. If we were to frame this idea as a pair of R commands, they would look like this:\n\n# mean absolute deviation from the mean:\nmean( abs(nba$age - mean(nba$age)) )\n\n[1] 3.553141\n\n# *median* absolute deviation from the *median*:\nmedian( abs(nba$age - median(nba$age)) )\n\n[1] 3\n\n\nThis has a straightforward interpretation: every observation in the data set lies some distance away from the typical value (the median). So the MAD is an attempt to describe a typical deviation from a typical value in the data set. It wouldn’t be unreasonable to interpret the MAD value of 3 for our age variable by saying something like this:\n\nThe median age in our data is 26, indicating that a typical player was 26 years old. However, there was a fair amount of variation from player to player: the MAD value was 3, indicating that a typical player’s age would differ from this median value by about 3 years.\n\nAs you’d expect, R has a built in function for calculating MAD, and you will be shocked no doubt to hear that it’s called mad(). However, it’s a little bit more complicated than the functions that we’ve been using previously. If you want to use it to calculate MAD in the exact same way that I have described it above, the command that you need to use specifies two arguments: the data set itself x, and a constant that I’ll explain in a moment. For our purposes, the constant is 1, so our command becomes\n\nmad( x = nba$age, constant = 1 )\n\n[1] 3\n\n\nApart from the weirdness of having to type that constant = 1 part, this is pretty straightforward.\nOkay, so what exactly is this constant = 1 argument? I won’t go into all the details here, but here’s the gist. Although the “raw” MAD value that I’ve described above is completely interpretable on its own terms, that’s not actually how it’s used in a lot of real world contexts. Instead, what happens a lot is that the researcher actually wants to calculate the standard deviation. However, in the same way that the mean is very sensitive to extreme values, the standard deviation is vulnerable to the exact same issue. So, in much the same way that people sometimes use the median as a “robust” way of calculating “something that is like the mean”, it’s not uncommon to use MAD as a method for calculating “something that is like the standard deviation”. Unfortunately, the raw MAD value doesn’t do this. Our raw MAD value is 19.5, and our standard deviation was 26.07. However, what some clever person has shown is that, under certain assumptions (normally-distributed), you can multiply the raw MAD value by 1.4826 and obtain a number that is directly comparable to the standard deviation. As a consequence, the default value of constant is 1.4826, and so when you use the mad() command without manually setting a value, here’s what you get:\n\nmad(nba$age)\n\n[1] 4.4478\n\n\nI should point out, though, that if you want to use this “corrected” MAD value as a robust version of the standard deviation, you really are relying on the assumption that the data are (or at least, are “supposed to be” in some sense) symmetric and basically shaped like a bell curve. That’s really not true for our ages data, so in this case I wouldn’t try to use the MAD value this way.\n\n\n9.2.7 Which measure to use?\nWe’ve discussed quite a few measures of spread (range, IQR, MAD, variance and standard deviation), and hinted at their strengths and weaknesses. Here’s a quick summary:\n\nRange. Gives you the full spread of the data. It’s very vulnerable to outliers, and as a consequence it isn’t often used unless you have good reasons to care about the extremes in the data.\nInterquartile range. Tells you where the “middle half” of the data sits. It’s pretty robust, and complements the median nicely. This is used a lot.\nMean absolute deviation. Tells you how far “on average” the observations are from the mean. It’s very interpretable, but has a few minor issues (not discussed here) that make it less attractive to statisticians than the standard deviation. Used sometimes, but not often.\nVariance. Tells you the average squared deviation from the mean. It’s mathematically elegant, and is probably the “right” way to describe variation around the mean, but it’s completely uninterpretable because it doesn’t use the same units as the data. Almost never used except as a mathematical tool; but it’s buried “under the hood” of a very large number of statistical tools.\nStandard deviation. This is the square root of the variance. It’s fairly elegant mathematically, and it’s expressed in the same units as the data so it can be interpreted pretty well. In situations where the mean is the measure of central tendency, this is the default. This is by far the most popular measure of variation.\nMedian absolute deviation. The typical (i.e., median) deviation from the median value. In the raw form it’s simple and interpretable; in the corrected form it’s a robust way to estimate the standard deviation, for some kinds of data sets. Not used very often, but it does get reported sometimes.\n\nIn short, the IQR and the standard deviation are easily the two most common measures used to report the variability of the data; but there are situations in which the others are used. I’ve described all of them in this book because there’s a fair chance you’ll run into most of these somewhere."
  },
  {
    "objectID": "descriptives.html#sec-skewandkurtosis",
    "href": "descriptives.html#sec-skewandkurtosis",
    "title": "9  Descriptive Statistics",
    "section": "9.3 Skew and kurtosis",
    "text": "9.3 Skew and kurtosis\nThere are two more descriptive statistics that you will sometimes see reported in the psychological literature, known as skew and kurtosis. In practice, neither one is used anywhere near as frequently as the measures of central tendency and variability that we’ve been talking about. Skew is pretty important, so you do see it mentioned a fair bit; but I’ve actually never seen kurtosis reported in a scientific article to date.\n\n\n\n\n\nFigure 9.2: An illustration of skewness. On the left we have a right skewed variable, in the middle we have a data set with no skew, and on the right we have a left skewed data set.\n\n\n\n\nSince it’s the more interesting of the two, let’s start by talking about the skewness. Skewness is basically a measure of asymmetry, and the easiest way to explain it is by drawing some pictures. As Figure 9.2 illustrates, if the data tend to have a lot of extreme large values (i.e., the right tail is “longer” than the left tail) and not so many extremely small values (blue line), then we say that the data are left skewed (or negatively skewed). On the other hand, if there are more extremely small values than extremely large ones (red line) we say that the data are right skewed (or positively skewed). That’s the qualitative idea behind skewness. The actual formula for the skewness of a data set is as follows\n\\[\n\\mbox{skewness}(X) = \\frac{1}{N \\hat{\\sigma}^3} \\sum_{i=1}^N (X_i - \\bar{X})^3\n\\] where \\(N\\) is the number of observations, \\(\\bar{X}\\) is the sample mean, and \\(\\hat{\\sigma}\\) is the standard deviation (the “divide by \\(N-1\\)” version, that is).\nThe final measure that is sometimes referred to, though very rarely in practice, is the kurtosis of a data set. Put simply, kurtosis is a measure of the “pointiness” of a data set, or, equivalently, how thick the tails of the distribution are. We won’t go into it deeply here, but will instead point you to the very nice Wikipedia entry for more details about both the concept of kurtosis and how to quantify it."
  },
  {
    "objectID": "descriptives.html#sec-summary",
    "href": "descriptives.html#sec-summary",
    "title": "9  Descriptive Statistics",
    "section": "9.4 Getting an overall summary of a variable",
    "text": "9.4 Getting an overall summary of a variable\nUp to this point in the chapter I’ve explained several different summary statistics that are commonly used when analysing data, along with specific functions that you can use in R to calculate each one. However, it’s kind of annoying to have to separately calculate means, medians, standard deviations, skews etc. Wouldn’t it be nice if R had some helpful functions that would do all these tedious calculations at once? Something like summary() or describe(), perhaps? Why yes, yes it would. So much so that both of these functions exist. The summary() function is in the base package, so it comes with every installation of R. The describe() function is part of the psych package, which we loaded earlier in the chapter.\n\n9.4.1 “Summarising” a variable\nThe summary() function is an easy thing to use. The basic idea behind the summary() function is that it prints out some useful information about whatever object you specify as the object argument. Because of how R functions work, the behavior of the summary() function differs quite dramatically depending on the type of the object that you give it. Let’s start by giving it a numeric object:\n\nsummary( object = nba$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  18.00   24.00   26.00   27.08   30.00   44.00 \n\n\nFor numeric variables, we get a whole bunch of useful descriptive statistics. It gives us the minimum and maximum values (i.e., the range), the first and third quartiles (25th and 75th percentiles; i.e., the IQR), the mean and the median. In other words, it gives us a pretty good collection of descriptive statistics related to the central tendency and the spread of the data.\nOkay, what about if we feed it a logical vector instead? Let’s say I want to know something about how many “superstars” are in our data set. We might operationalize the concept of a superstar as a player averaging more than 30 points per game for a season. Let’s create a logical variable superstar in which each element is TRUE if the corresponding player was is a superstar according to this definition.\n\nsuperstar &lt;-  nba$pts &gt; 30\nhead(superstar, 10)\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nSo that’s what the superstar variable looks like. Now let’s ask R for a summary()\n\nsummary(object = superstar)\n\n   Mode   FALSE    TRUE \nlogical   12284      21 \n\n\nIn this context, the summary() function gives us a count of the number of TRUE values, the number of FALSE values, and the number of missing values (i.e., the NAs). Pretty reasonable.\nNext, let’s try to give it a factor. If you recall, I’ve previously transformed both the colleg and team_abbreviations variables to factors when plotting. Let’s use team_abbreviations here.\n\nsummary( object = factor(nba$team_abbreviation))\n\nATL BKN BOS CHA CHH CHI CLE DAL DEN DET GSW HOU IND LAC LAL MEM MIA MIL MIN NJN \n421 180 407 288  89 406 433 422 412 400 409 418 410 424 411 352 426 409 399 257 \nNOH NOK NOP NYK OKC ORL PHI PHX POR SAC SAS SEA TOR UTA VAN WAS \n143  32 159 410 239 411 420 399 407 398 413 182 428 397  72 422 \n\n\nFor factors, we get a frequency table, just like we got when we used the table() function. Interestingly, if we leave the team_abbreviation as a column of character vectors, we don’t get the same results:\n\nsummary( object = nba$team_abbreviation)\n\n   Length     Class      Mode \n    12305 character character \n\n\nWe briefly touched on factors in Chapter 6, but this example illustrates in why it is often a good idea to declare your nominal scale variables as factors rather than a character vector. Treating nba$team_abbreviations as a factor, R knows that it should treat it as a nominal scale variable, and so it gives you a much more detailed (and helpful) summary than it would have if I’d left it as a character vector.\n\n\n9.4.2 “Summarising” a data frame\nOkay what about data frames (tibbles)? When you pass a data frame to the summary() function, it produces a slightly condensed summary of each variable inside the data frame. To give you a sense of how this can be useful, let’s try this for nba:\n\nsummary(nba)\n\n      ...1       player_name        team_abbreviation       age       \n Min.   :    0   Length:12305       Length:12305       Min.   :18.00  \n 1st Qu.: 3076   Class :character   Class :character   1st Qu.:24.00  \n Median : 6152   Mode  :character   Mode  :character   Median :26.00  \n Mean   : 6152                                         Mean   :27.08  \n 3rd Qu.: 9228                                         3rd Qu.:30.00  \n Max.   :12304                                         Max.   :44.00  \n                                                                      \n player_height   player_weight      college            country         \n Min.   :160.0   Min.   : 60.33   Length:12305       Length:12305      \n 1st Qu.:193.0   1st Qu.: 90.72   Class :character   Class :character  \n Median :200.7   Median : 99.79   Mode  :character   Mode  :character  \n Mean   :200.6   Mean   :100.37                                        \n 3rd Qu.:208.3   3rd Qu.:108.86                                        \n Max.   :231.1   Max.   :163.29                                        \n                                                                       \n   draft_year    draft_round     draft_number          gp       \n Min.   :1963   Min.   :0.000   Min.   :  0.00   Min.   : 1.00  \n 1st Qu.:1997   1st Qu.:1.000   1st Qu.:  9.00   1st Qu.:31.00  \n Median :2004   Median :1.000   Median : 19.00   Median :57.00  \n Mean   :2004   Mean   :1.304   Mean   : 21.86   Mean   :51.29  \n 3rd Qu.:2010   3rd Qu.:2.000   3rd Qu.: 33.00   3rd Qu.:73.00  \n Max.   :2021   Max.   :8.000   Max.   :165.00   Max.   :85.00  \n NA's   :2224   NA's   :2274    NA's   :2277                    \n      pts              reb              ast           net_rating      \n Min.   : 0.000   Min.   : 0.000   Min.   : 0.000   Min.   :-250.000  \n 1st Qu.: 3.600   1st Qu.: 1.800   1st Qu.: 0.600   1st Qu.:  -6.400  \n Median : 6.700   Median : 3.000   Median : 1.200   Median :  -1.300  \n Mean   : 8.173   Mean   : 3.559   Mean   : 1.814   Mean   :  -2.256  \n 3rd Qu.:11.500   3rd Qu.: 4.700   3rd Qu.: 2.400   3rd Qu.:   3.200  \n Max.   :36.100   Max.   :16.300   Max.   :11.700   Max.   : 300.000  \n                                                                      \n    oreb_pct          dreb_pct        usg_pct           ts_pct      \n Min.   :0.00000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.02100   1st Qu.:0.096   1st Qu.:0.1490   1st Qu.:0.4800  \n Median :0.04100   Median :0.131   Median :0.1810   Median :0.5240  \n Mean   :0.05447   Mean   :0.141   Mean   :0.1849   Mean   :0.5111  \n 3rd Qu.:0.08400   3rd Qu.:0.180   3rd Qu.:0.2170   3rd Qu.:0.5610  \n Max.   :1.00000   Max.   :1.000   Max.   :1.0000   Max.   :1.5000  \n                                                                    \n    ast_pct          season         \n Min.   :0.0000   Length:12305      \n 1st Qu.:0.0660   Class :character  \n Median :0.1030   Mode  :character  \n Mean   :0.1314                     \n 3rd Qu.:0.1780                     \n Max.   :1.0000                     \n                                    \n\n\nHere, we see many descriptive statistics for the various columns in our data frame. We can also see that many of our variables (e.g., player_name, team_abbreviation, college, country, etc.) are character vectors and should, if we wish to do more processing, be converted to factors."
  },
  {
    "objectID": "descriptives.html#sec-zscore",
    "href": "descriptives.html#sec-zscore",
    "title": "9  Descriptive Statistics",
    "section": "9.5 Standard scores",
    "text": "9.5 Standard scores\nSuppose my friend is putting together a new questionnaire intended to measure “grumpiness”. The survey has 50 questions, which you can answer in a grumpy way or not. Across a big sample (hypothetically, let’s imagine a million people or so!) the data are fairly normally distributed, with the mean grumpiness score being 17 out of 50 questions answered in a grumpy way, and the standard deviation is 5. In contrast, when I take the questionnaire, I answer 35 out of 50 questions in a grumpy way. So, how grumpy am I? One way to think about would be to say that I have grumpiness of 35/50, so you might say that I’m 70% grumpy. But that’s a bit weird, when you think about it. If my friend had phrased her questions a bit differently, people might have answered them in a different way, so the overall distribution of answers could easily move up or down depending on the precise way in which the questions were asked. So, I’m only 70% grumpy with respect to this set of survey questions. Even if it’s a very good questionnaire, this isn’t very a informative statement.\nA simpler way around this is to describe my grumpiness by comparing me to other people. Shockingly, out of my friend’s sample of 1,000,000 people, only 159 people were as grumpy as me, suggesting that I’m in the top 0.016% of people for grumpiness. This makes much more sense than trying to interpret the raw data. This idea – that we should describe my grumpiness in terms of the overall distribution of the grumpiness of humans – is the qualitative idea that standardisation attempts to get at. One way to do this is to do exactly what I just did, and describe everything in terms of percentiles. However, the problem with doing this is that “it’s lonely at the top”. Suppose that my friend had only collected a sample of 1000 people (still a pretty big sample for the purposes of testing a new questionnaire, I’d like to add), and this time gotten a mean of 16 out of 50 with a standard deviation of 5, let’s say. The problem is that almost certainly, not a single person in that sample would be as grumpy as me.\nHowever, all is not lost. A different approach is to convert my grumpiness score into a standard score, also referred to as a \\(z\\)-score. The standard score is defined as the number of standard deviations above the mean that my grumpiness score lies. To phrase it in “pseudo-maths” the standard score is calculated like this:\n\\[\n\\mbox{standard score} = \\frac{\\mbox{raw score} - \\mbox{mean}}{\\mbox{standard deviation}}\n\\]\nIn actual math, the equation for the \\(z\\)-score is\n\\[\nz_i = \\frac{X_i - \\bar{X}}{\\hat\\sigma}\n\\] So, going back to the grumpiness data, we can now transform my raw grumpiness into a standardized grumpiness score. If the mean is 17 and the standard deviation is 5 then my standardized grumpiness score would be:\n\\[\nz = \\frac{35 - 17}{5} = 3.6\n\\] To interpret this value, recall the rough heuristic that I provided in Section 9.2.5, in which I noted that 99.7% of values are expected to lie within 3 standard deviations of the mean. So the fact that my grumpiness corresponds to a \\(z\\) score of 3.6 indicates that I’m very grumpy indeed. Without worrying too much about what it does we can use a function called pnorm() that allows us to be a bit more precise than this. Specifically, it allows us to calculate a theoretical percentile rank for my grumpiness:\n\npnorm( 3.6 )\n\n[1] 0.9998409\n\n\nAt this stage, this command doesn’t make too much sense, but don’t worry too much about it. It’s not important for now. But the output is fairly straightforward: it suggests that I’m grumpier than 99.98% of people. Sounds about right.\nIn addition to allowing you to interpret a raw score in relation to a larger population (and thereby allowing you to make sense of variables that lie on arbitrary scales), standard scores serve a second useful function. Standard scores can be compared to one another in situations where the raw scores can’t. Suppose, for instance, my friend also had another questionnaire that measured extroversion using a 24 items questionnaire. The overall mean for this measure turns out to be 13 with standard deviation 4; and I scored a 2. As you can imagine, it doesn’t make a lot of sense to try to compare my raw score of 2 on the extroversion questionnaire to my raw score of 35 on the grumpiness questionnaire. The raw scores for the two variables are “about” fundamentally different things, so this would be like comparing apples to oranges.\nWhat about the standard scores? Well, this is a little different. If we calculate the standard scores, we get \\(z = (35-17)/5 = 3.6\\) for grumpiness and \\(z = (2-13)/4 = -2.75\\) for extroversion. These two numbers can be compared to each other. I’m much less extroverted than most people (\\(z = -2.75\\)) and much grumpier than most people (\\(z = 3.6\\)): but the extent of my unusualness is much more extreme for grumpiness (since 3.6 is a bigger number than 2.75). Because each standardized score is a statement about where an observation falls relative to its own population, it is possible to compare standardized scores across completely different variables."
  },
  {
    "objectID": "descriptives.html#sec-correl",
    "href": "descriptives.html#sec-correl",
    "title": "9  Descriptive Statistics",
    "section": "9.6 Correlations",
    "text": "9.6 Correlations\nUp to this point we have focused entirely on how to construct descriptive statistics for a single variable. What we haven’t done is talked about how to describe the relationships between variables in the data. To do that, we want to talk mostly about the correlation between variables.\nTo give us a general sense of how closely related two variables are, we can draw scatterplots (Section 8.3). However, we might wish to say a bit more than that. For instance, let’s inspect the relationship between nba$pts and nba$ast (Figure 9.3).\n\n\n\n\n\nFigure 9.3: Scatterplot showing the relationship between pts and ast\n\n\n\n\nOr we can look at the relationship between players’ heights and weights.\n\n\n\n\n\nFigure 9.4: Scatterplot showing the relationship between player_height and player_weight\n\n\n\n\nIt’s clear that the relationship is qualitatively the same in both cases: more points means more assists (and vice versa) and taller means heavier (and vice versa). However, it’s also seems that the two relationship between height and weight is stronger than the relationship between points and assists.\n\n\nWarning: Removed 2277 rows containing missing values (`geom_point()`).\n\n\n\n\n\nFigure 9.5: Scatterplot showing the relationship between pts and draft_number\n\n\n\n\nIn contrast, let’s consider Figure 9.5). Here, the the direction of the relationship is different. As a player’s position goes up (i.e., teams apparently were not overly motivated to spend an early draft pick on him), the player also tends to average fewer points per game (a negative relationship).\n\n9.6.1 The correlation coefficient\nWe can make these ideas a bit more explicit by introducing the idea of a correlation coefficient (or, more specifically, Pearson’s correlation coefficient), which is traditionally denoted by \\(r\\). The correlation coefficient between two variables \\(X\\) and \\(Y\\) (sometimes denoted \\(r_{XY}\\)), which we’ll define more precisely in the next section, is a measure that varies from \\(-1\\) to \\(1\\). When \\(r = -1\\) it means that we have a perfect negative relationship, and when \\(r = 1\\) it means we have a perfect positive relationship. When \\(r = 0\\), there’s no relationship at all.\nThe formula for the Pearson’s correlation coefficient can be written in several different ways. I think the simplest way to write down the formula is to break it into two steps. Firstly, let’s introduce the idea of a covariance. The covariance between two variables \\(X\\) and \\(Y\\) is a generalization of the notion of the variance (Section 9.2.4); it’s a mathematically simple way of describing the relationship between two variables that isn’t terribly informative to humans:\n\\[\n\\mbox{Cov}(X,Y) = \\frac{1}{N-1} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right) \\left( Y_i - \\bar{Y} \\right)\n\\]\nBecause we’re multiplying (i.e., taking the “product” of) a quantity that depends on \\(X\\) by a quantity that depends on \\(Y\\) and then averaging (dividing by \\(N-1\\) rather than \\(N\\), just like we saw with the variance and the standard deviation), you can think of the formula for the covariance as an “average cross product” between \\(X\\) and \\(Y\\). The covariance has the nice property that, if \\(X\\) and \\(Y\\) are entirely unrelated, then the covariance is exactly zero. If the relationship between them is positive, then the covariance is also positive; and if the relationship is negative then the covariance is also negative. In other words, the covariance captures the basic qualitative idea of correlation. Unfortunately, the raw magnitude of the covariance isn’t easy to interpret: it depends on the units in which \\(X\\) and \\(Y\\) are expressed, and worse yet, the actual units that the covariance itself is expressed in are really weird. For instance, if \\(X\\) refers to the player_height variable (units: centimeters) and \\(Y\\) refers to the player_weight variable (units: kilograms), then the units for their covariance are “centimeters \\(\\times\\) kilograms”. And I have no idea what that would even mean.\nThe Pearson correlation coefficient \\(r\\) fixes this interpretation problem by standardising the covariance, in pretty much the exact same way that the \\(z\\)-score standardises a raw score: by dividing by the standard deviation. However, because we have two variables that contribute to the covariance, the standardisation only works if we divide by both standard deviations. In other words, the correlation between \\(X\\) and \\(Y\\) can be written as follows: \\[\nr_{XY}  = \\frac{\\mbox{Cov}(X,Y)}{ \\hat{\\sigma}_X \\ \\hat{\\sigma}_Y}\n\\] By doing this standardisation, not only do we keep all of the nice properties of the covariance discussed earlier, but the actual values of \\(r\\) are on a meaningful scale: \\(r= 1\\) implies a perfect positive relationship, and \\(r = -1\\) implies a perfect negative relationship. I’ll expand a little more on this point later, in Section@refsec:interpretingcorrelations. But before I do, let’s look at how to calculate correlations in R.\n\n\n9.6.2 Calculating correlations in R\nCalculating correlations in R can be done using the cor() command. The simplest way to use the command is to specify two input arguments x and y, each one corresponding to one of the variables. The following extract illustrates the basic usage of the function:\n\ncor(nba$pts, nba$ast)\n\n[1] 0.6609489\n\n\nLet’s take a quick look at the relationship between a player’s draft number and that player’s average points per game:\n\ncor(nba$draft_number, nba$pts)\n\n[1] NA\n\n\nHere we can see that appears to be a problem. cor() has returned the unhelpful result of NA. This is because there are missing values in nba$draft_number (we converted the “Undrafted” values to NA when we loaded the CSV file, Section 7.1). So the correlation between these variables is actually undefined. But this isn’t super helpful. We may wish to determine the correlation among players who have valid values for both variables, both draft_number and pts. To get this, we need to tell cor to use any draft_number-pts pairs that consist of two valid (non-NA) values. In other words, any “complete” observations.\n\ncor(nba$draft_number, nba$pts, use=\"complete.obs\")\n\n[1] -0.3733803\n\n\nThe cor() function is actually a bit more powerful than these simple examples suggest. For example, you can also calculate a complete “correlation matrix”, between all pairs of variables in a given data frame. Let’s do that for a selection of variables in our nba data frame. We have some missing values, so we will need to filter those out along the way.\n\nnba %&gt;%\n    select(pts, ast, player_height, player_weight, draft_number) %&gt;%\n    cor(use = \"complete.obs\") %&gt;%\n    round(2)\n\n                pts   ast player_height player_weight draft_number\npts            1.00  0.65         -0.10         -0.06        -0.37\nast            0.65  1.00         -0.49         -0.41        -0.24\nplayer_height -0.10 -0.49          1.00          0.82        -0.11\nplayer_weight -0.06 -0.41          0.82          1.00        -0.10\ndraft_number  -0.37 -0.24         -0.11         -0.10         1.00\n\n\nHere we can quickly see the positive relationships between pts and ast and between player_height and player_weight, as well as the negative relationship between draft_number and both pts and ast. You might also note the negative relationships between player_height (and player_weight) and both ast. This might may sense to you if you knew anything about basketball (but I don’t).\n\n\n9.6.3 Interpreting a correlation\nNaturally, in real life you don’t see many correlations of 1. So how should you interpret a correlation of, say \\(r= .4\\)? The honest answer is that it really depends on what you want to use the data for, and on how strong the correlations in your field tend to be. A friend of mine in engineering once argued that any correlation less than \\(.95\\) is completely useless (I think he was exaggerating, even for engineering). On the other hand there are real cases – even in psychology – where you should really expect correlations that strong. For instance, one of the benchmark data sets used to test theories of how people judge similarities is so clean that any theory that can’t achieve a correlation of at least \\(.9\\) really isn’t deemed to be successful. However, when looking for (say) elementary correlates of intelligence (e.g., inspection time, response time), if you get a correlation above \\(.3\\) you’re doing very very well. In short, the interpretation of a correlation depends a lot on the context.\nHowever, something that can never be stressed enough is that you should always look at the scatterplot before attaching any interpretation to a correlation. A correlation might not mean what you think it means. The classic illustration of this is “Anscombe’s Quartet” Anscombe (1973), which is a collection of four data sets. Each data set has two variables, an \\(X\\) and a \\(Y\\). For each data set, the mean value for \\(X\\) is 9 and the mean for \\(Y\\) is 7.5. The, standard deviations for all \\(X\\) variables are almost identical, as are those for the the \\(Y\\) variables. And in each case the correlation between \\(X\\) and \\(Y\\) is \\(r = 0.816\\). You can verify this yourself, since the data set comes distributed with R. The commands would be:\n\ncor( anscombe$x1, anscombe$y1 )\n\n[1] 0.8164205\n\ncor( anscombe$x2, anscombe$y2 )\n\n[1] 0.8162365\n\n\nand so on.\nYou’d think that these four data sets would look pretty similar to one another. They do not. If we draw scatterplots of \\(X\\) against \\(Y\\) for all four variables, as shown in Figure 9.6 we see that all four of these are spectacularly different to each other.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nFigure 9.6: Anscombe’s quartet. All four of these data sets have a Pearson correlation of \\(r = .816\\), but they are qualitatively different from one another.”\n\n\n\n\nThe lesson here, which so very many people seem to forget in real life is “always graph your raw data” (Chapter 8).\n\n\n9.6.4 Spearman’s rank correlations\nThe Pearson correlation coefficient is useful for a lot of things, but it does have shortcomings. One issue in particular stands out: what it actually measures is the strength of the linear relationship between two variables. In other words, what it gives you is a measure of the extent to which the data all tend to fall on a single, perfectly straight line. Often, this is a pretty good approximation to what we mean when we say “relationship”, and so the Pearson correlation is a good thing to calculation. Sometimes, it isn’t.\nOne very common situation where the Pearson correlation isn’t quite the right thing to use arises when an increase in one variable \\(X\\) really is reflected in an increase in another variable \\(Y\\), but the nature of the relationship isn’t necessarily linear. An example of this might be the relationship between effort and reward when studying for an exam. If you put in zero effort (\\(X\\)) into learning a subject, then you should expect a grade of 0% (\\(Y\\)). However, a little bit of effort will cause a massive improvement: just turning up to lectures means that you learn a fair bit, and if you just turn up to classes, and scribble a few things down so your grade might rise to 35%, all without a lot of effort. However, you just don’t get the same effect at the other end of the scale. As everyone knows, it takes a lot more effort to get a grade of 90% than it takes to get a grade of 55%. What this means is that, if I’ve got data looking at study effort and grades, there’s a pretty good chance that Pearson correlations will be misleading.\nHow should we address this? Actually, it’s really easy: if we’re looking for ordinal relationships, all we have to do is treat the data as if it were ordinal scale! So, instead of measuring effort in terms of “hours worked”, we can rank all 10 of our students in order of hours worked. That is, student 1 did the least work out of anyone (2 hours) so they get the lowest rank (rank = 1). Student 4 was the next laziest, putting in only 6 hours of work in over the whole semester, so they get the next lowest rank (rank = 2). Notice that I’m using “rank =1” to mean “low rank”. Sometimes in everyday language we talk about “rank = 1” to mean “top rank” rather than “bottom rank”. So be careful: you can rank “from smallest value to largest value” (i.e., small equals rank 1) or you can rank “from largest value to smallest value” (i.e., large equals rank 1). In this case, I’m ranking from smallest to largest, because that’s the default way that R does it. But in real life, it’s really easy to forget which way you set things up, so you have to put a bit of effort into remembering!\nWe can get R to construct these rankings using the rank() function, like this:\n\nhours_rank &lt;- rank(hours)   # rank students by hours worked\ngrade_rank &lt;- rank(grade)   # rank students by grade received\n\nAs the table above shows, these two rankings are identical, so if we now correlate them we get a perfect relationship:\n\ncor(hours_rank, grade_rank)\n\nWhat we’ve just re-invented is Spearman’s rank order correlation, usually denoted \\(\\rho\\) to distinguish it from the Pearson correlation \\(r\\). We can calculate Spearman’s \\(\\rho\\) using R in two different ways. Firstly we could do it the way I just showed, using the rank() function to construct the rankings, and then calculate the Pearson correlation on these ranks. However, that’s way too much effort to do every time. It’s much easier to just specify the method argument of the cor() function.\n\ncor(hours, grade, method = \"spearman\")\n\nThe default value of the method argument is \"pearson\", which is why we didn’t have to specify it earlier on when we were doing Pearson correlations.\n\n\n\n\nAnscombe, Francis J. 1973. “Graphs in Statistical Analysis.” The American Statistician 27 (1): 17–21."
  },
  {
    "objectID": "hypothesis.html#sec-ht-activity",
    "href": "hypothesis.html#sec-ht-activity",
    "title": "10  Hypothesis Testing",
    "section": "10.1 Promotions activity",
    "text": "10.1 Promotions activity\nLet’s start with an activity studying the effect of gender on promotions at a bank.\n\n10.1.1 Does gender affect promotions at a bank?\nSay you are working at a bank in the 1970s and you are submitting your resume to apply for a promotion. Will your gender affect your chances of getting promoted? To answer this question, we’ll focus on data from a study published in the Journal of Applied Psychology in 1974. This data is also used in the OpenIntro series of statistics textbooks.\nTo begin the study, 48 bank supervisors were asked to assume the role of a hypothetical director of a bank with multiple branches. Every one of the bank supervisors was given a resume and asked whether or not the candidate on the resume was fit to be promoted to a new position in one of their branches.\nHowever, each of these 48 resumes were identical in all respects except one: the name of the applicant at the top of the resume Of the supervisors, 24 were randomly given resumes with stereotypically “male” names, while 24 of the supervisors were randomly given resumes with stereotypically “female” names. Since only (binary) gender varied from resume to resume, researchers could isolate the effect of this variable in promotion rates.\nWhile many people today (including us, the authors) disagree with such binary views of gender, it is important to remember that this study was conducted at a time where more nuanced views of gender were not as prevalent. Despite this imperfection, we decided to still use this example as we feel it presents ideas still relevant today about how we could study discrimination in the workplace.\nWe have the data on the 48 applicants in a promotions.csv file that we can then load as a data frame. Let’s explore this data by looking at six randomly selected rows:\n\npromotions &lt;- read_csv(\"./data/promotions.csv\")\n\nRows: 48 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): decision, gender\ndbl (1): id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npromotions %&gt;% \n  sample_n(size = 6) %&gt;% \n  arrange(id)\n\n# A tibble: 6 × 3\n     id decision gender\n  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt; \n1    11 promoted male  \n2    26 promoted female\n3    28 promoted female\n4    36 not      male  \n5    37 not      male  \n6    46 not      female\n\n\nThe variable id acts as an identification variable for all 48 rows, the decision variable indicates whether the applicant was selected for promotion or not, while the gender variable indicates the gender of the name used on the resume. Recall that this data does not pertain to 24 actual men and 24 actual women, but rather 48 identical resumes of which 24 were assigned stereotypically “male” names and 24 were assigned stereotypically “female” names.\nLet’s perform an exploratory data analysis of the relationship between the two categorical variables decision and gender. Recall that we saw in Section 8.8.3 that one way we can visualize such a relationship is by using a stacked barplot.\n\n\n\n\n\nFigure 10.1: ?(caption)\n\n\n\n\nObserve in Figure 10.1 that it appears that resumes with female names were much less likely to be accepted for promotion. Let’s quantify these promotion rates by computing the proportion of resumes accepted for promotion for each group using the dplyr package for data wrangling. Note the use of the tally() function here which is a shortcut for summarize(n = n()) to get counts.\n\npromotions %&gt;% \n  group_by(gender, decision) %&gt;% \n  tally()\n\n# A tibble: 4 × 3\n# Groups:   gender [2]\n  gender decision     n\n  &lt;chr&gt;  &lt;chr&gt;    &lt;int&gt;\n1 female not         10\n2 female promoted    14\n3 male   not          3\n4 male   promoted    21\n\n\nSo of the 24 resumes with male names, 21 were selected for promotion, for a proportion of 21/24 = 0.875 = 87.5%. On the other hand, of the 24 resumes with female names, 14 were selected for promotion, for a proportion of 14/24 = 0.583 = 58.3%. Comparing these two rates of promotion, it appears that resumes with male names were selected for promotion at a rate 0.875 - 0.583 = 0.292 = 29.2% higher than resumes with female names. This is suggestive of an advantage for resumes with a male name on it.\nThe question is, however, does this provide conclusive evidence that there is gender discrimination in promotions at banks? Could a difference in promotion rates of 29.2% still occur by chance, even in a hypothetical world where no gender-based discrimination existed? In other words, what is the role of sampling variation in this hypothesized world? To answer this question, we’ll again rely on a computer to run simulations.\n\n\n10.1.2 Shuffling once\nFirst, try to imagine a hypothetical universe where no gender discrimination in promotions existed. In such a hypothetical universe, the gender of an applicant would have no bearing on their chances of promotion. Bringing things back to our promotions data frame, the gender variable would thus be an irrelevant label. If these gender labels were irrelevant, then we could randomly reassign them by “shuffling” them to no consequence!\nTo illustrate this idea, let’s narrow our focus to 6 arbitrarily chosen resumes of the 48 in Figure 10.2). The decision column shows that 3 resumes resulted in promotion while 3 didn’t. The gender column shows what the original gender of the resume name was.\nHowever, in our hypothesized universe of no gender discrimination, gender is irrelevant and thus it is of no consequence to randomly “shuffle” the values of gender. The shuffled_gender column shows one such possible random shuffling. Observe in the fourth column how the number of male and female names remains the same at 3 each, but they are now listed in a different order.\n\n\n\n\n\n\n\n\nresume number\n\n\ndecision\n\n\ngender\n\n\nshuffled gender\n\n\n\n\n\n\n1\n\n\nnot\n\n\nmale\n\n\nmale\n\n\n\n\n2\n\n\nnot\n\n\nfemale\n\n\nmale\n\n\n\n\n3\n\n\nnot\n\n\nfemale\n\n\nfemale\n\n\n\n\n4\n\n\npromoted\n\n\nmale\n\n\nfemale\n\n\n\n\n5\n\n\npromoted\n\n\nmale\n\n\nfemale\n\n\n\n\n6\n\n\npromoted\n\n\nfemale\n\n\nmale\n\n\n\n\n\nFigure 10.2: One example of shuffling gender variable\n\n\n\nAgain, such random shuffling of the gender label only makes sense in our hypothesized universe of no gender discrimination. How could we extend this shuffling of the gender variable to all 48 resumes by hand? One way would be by using standard deck of 52 playing cards, which we display in Figure 10.3.\n\n\n\nFigure 10.3: Standard deck of 52 playing cards.\n\n\nSince half the cards are red (diamonds and hearts) and the other half are black (spades and clubs), by removing two red cards and two black cards, we would end up with 24 red cards and 24 black cards. After shuffling these 48 cards, we can flip the cards over one-by-one, assigning “male” for each red card and “female” for each black card.\nWe’ve saved one such shuffling in the promotions_shuffled data frame of the moderndive package. If you compare the original promotions and the shuffled promotions_shuffled data frames, you’ll see that while the decision variable is identical, the gender variable has changed.\nLet’s repeat the same exploratory data analysis we did for the original promotions data on our promotions_shuffled data frame. Let’s create a barplot visualizing the relationship between decision and the new shuffled gender variable and compare this to the original unshuffled version in Figure 10.4.\n\nggplot(promotions_shuffled, \n       aes(x = gender, fill = decision)) +\n  geom_bar() + \n  labs(x = \"Gender of resume name\")\n\n\n\n`summarise()` has grouped output by 'gender'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'gender'. You can override using the\n`.groups` argument.\n\n\n\n\n\nFigure 10.4: Barplots of relationship of promotion with gender (left) and shuffled gender (right).\n\n\n\n\nIt appears the difference in “male names” versus “female names” promotion rates is now different. Compared to the original data in the left barplot, the new “shuffled” data in the right barplot has promotion rates that are much more similar.\nLet’s also compute the proportion of resumes accepted for promotion for each group:\n\npromotions_shuffled %&gt;% \n  group_by(gender, decision) %&gt;% \n  tally() # Same as summarize(n = n())\n\n# A tibble: 4 × 3\n# Groups:   gender [2]\n  gender decision     n\n  &lt;chr&gt;  &lt;chr&gt;    &lt;int&gt;\n1 female not          5\n2 female promoted    19\n3 male   not          8\n4 male   promoted    16\n\n\nSo in this hypothetical universe of no discrimination, \\(16/24 = 0.667 = 66.7\\%\\) of “male” resumes were selected for promotion. On the other hand, \\(19/24 = 0.792 = 79.2\\%\\) of “female” resumes were selected for promotion.\nLet’s next compare these two values. It appears that resumes with stereotypically male names were selected for promotion at a rate that was \\(0.667 - 0.792 = -0.125 = -12.5\\%\\) different than resumes with stereotypically female names.\nObserve how this difference in rates is not the same as the difference in rates of 0.292 = 29.2% we originally observed. This is once again due to sampling variation. How can we better understand the effect of this sampling variation? By repeating this shuffling several times!\n\n\n10.1.3 Shuffling 16 times\nWe recruited 16 groups of our friends to repeat this shuffling exercise. They recorded these values in a shared spreadsheet; we display a snapshot of the first 10 rows and 5 columns in Figure 10.5.\n\n\n\nFigure 10.5: Snapshot of shared spreadsheet of shuffling results (m for male, f for female).\n\n\n\n\nRows: 48 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (17): decision, Cassandra, Nox, Priya, Jenny, Eindra, Maddie, Grace, Ste...\ndbl  (1): id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nFor each of these 16 columns of shuffles, we computed the difference in promotion rates, and in Figure 10.6 we display their distribution in a histogram. We also mark the observed difference in promotion rate that occurred in real life of 0.292 = 29.2% with a dark line.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nFigure 10.6: Distribution of shuffled differences in promotions.\n\n\n\n\nBefore we discuss the distribution of the histogram, we emphasize the key thing to remember: this histogram represents differences in promotion rates that one would observe in our hypothesized universe of no gender discrimination.\nObserve first that the histogram is roughly centered at 0. Saying that the difference in promotion rates is 0 is equivalent to saying that both genders had the same promotion rate. In other words, the center of these 16 values is consistent with what we would expect in our hypothesized universe of no gender discrimination.\nHowever, while the values are centered at 0, there is variation about 0. This is because even in a hypothesized universe of no gender discrimination, you will still likely observe small differences in promotion rates because of chance sampling variation. Looking at the histogram in Figure 10.6, such differences could even be as extreme as -0.292 or 0.208.\nTurning our attention to what we observed in real life: the difference of 0.292 = 29.2% is marked with a vertical dark line. Ask yourself: in a hypothesized world of no gender discrimination, how likely would it be that we observe this difference? While opinions here may differ, in our opinion not often! Now ask yourself: what do these results say about our hypothesized universe of no gender discrimination?\n\n\n\n10.1.4 What did we just do?\nWhat we just demonstrated in this activity is the statistical procedure known as hypothesis testing using a permutation test. The term “permutation” is the mathematical term for “shuffling”: taking a series of values and reordering them randomly, as you did with the playing cards.\nIn fact, permutations are a form of resampling. Permutation methods involve resampling without replacement. In contrast, our exploration of sampling variation back in Chapter 11 was a slightly different type of resampling (known as bootstrapping) which involved resampling with replacement. After using our shovel once, we put everything back into the bowl before usign the shovel again. Now think of our deck of cards. After drawing a card, you laid it out in front of you, recorded the color, and then you did not put it back in the deck.\nIn our previous example, we tested the validity of the hypothesized universe of no gender discrimination. The evidence contained in our observed sample of 48 resumes was somewhat inconsistent with our hypothesized universe. Thus, we would be inclined to reject this hypothesized universe and declare that the evidence suggests there is gender discrimination.\nIn the case of sampling from the bowl (Chapter 11) involved an inference about an unknown population proportion. This time, it will be \\(p_{m} - p_{f}\\), where \\(p_{m}\\) is the population proportion of resumes with male names being recommended for promotion and \\(p_{f}\\) is the equivalent for resumes with female names. Recall that this is one of the scenarios for inference we’ve seen so far in Table 10.1.\n\n\nTable 10.1: Scenarios of sampling for inference\n\n\n\n\n\n\n\n\n\nScenario\nPopulation parameter\nNotation\nPoint estimate\nSymbol(s)\n\n\n\n\n1\nPopulation proportion\n\\(p\\)\nSample proportion\n\\(\\widehat{p}\\)\n\n\n2\nPopulation mean\n\\(\\mu\\)\nSample mean\n\\(\\overline{x}\\) or \\(\\widehat{\\mu}\\)\n\n\n3\nDifference in population proportions\n\\(p_1 - p_2\\)\nDifference in sample proportions\n\\(\\widehat{p}_1 - \\widehat{p}_2\\)\n\n\n\n\nSo, based on our sample of \\(n_m\\) = 24 “male” applicants and \\(n_w\\) = 24 “female” applicants, the point estimate for \\(p_{m} - p_{f}\\) is the difference in sample proportions \\(\\widehat{p}_{m} -\\widehat{p}_{f}\\) = 0.875 - 0.583 = 0.292 = 29.2%. This difference in favor of “male” resumes of 0.292 is greater than 0, suggesting discrimination in favor of men.\nHowever, the question we asked ourselves was “is this difference meaningfully greater than 0?”. In other words, is that difference indicative of true discrimination, or can we just attribute it to sampling variation? Hypothesis testing allows us to make such distinctions."
  },
  {
    "objectID": "hypothesis.html#sec-understanding-ht",
    "href": "hypothesis.html#sec-understanding-ht",
    "title": "10  Hypothesis Testing",
    "section": "10.2 Understanding hypothesis tests",
    "text": "10.2 Understanding hypothesis tests\nMuch like the terminology, notation, and definitions relating to sampling you saw in Section 11.3, there are a lot of terminology, notation, and definitions related to hypothesis testing as well. Learning these may seem like a very daunting task at first. However, with practice, practice, and more practice, anyone can master them.\nFirst, a hypothesis is a statement about the value of an unknown population parameter. In our resume activity, our population parameter of interest is the difference in population proportions \\(p_{m} - p_{f}\\).\nSecond, a hypothesis test consists of a test between two competing hypotheses: (1) a null hypothesis \\(H_0\\) (pronounced “H-naught”) versus (2) an alternative hypothesis \\(H_A\\) (also denoted \\(H_1\\)).\nGenerally the null hypothesis is a claim that there is “no effect” or “no difference of interest.” In many cases, the null hypothesis represents the status quo or a situation that nothing interesting is happening. Furthermore, generally the alternative hypothesis is the claim the experimenter or researcher wants to establish or find evidence to support. It is viewed as a “challenger” hypothesis to the null hypothesis \\(H_0\\). In our resume activity, an appropriate hypothesis test would be:\n\\[\n\\begin{aligned}\nH_0 &: \\text{men and women are promoted at the same rate}\\\\\n\\text{vs } H_A &: \\text{men are promoted at a higher rate than women}\n\\end{aligned}\n\\]\nNote some of the choices we have made. First, we set the null hypothesis \\(H_0\\) to be that there is no difference in promotion rate and the “challenger” alternative hypothesis \\(H_A\\) to be that there is a difference. While it would not be wrong in principle to reverse the two, it is a convention in statistical inference that the null hypothesis is set to reflect a “null” situation where “nothing is going on.” As we discussed earlier, in this case, \\(H_0\\) corresponds to there being no difference in promotion rates. Furthermore, we set \\(H_A\\) to be that men are promoted at a higher rate, a subjective choice reflecting a prior suspicion we have that this is the case. We call such alternative hypotheses one-sided alternatives. If someone else however does not share such suspicions and only wants to investigate that there is a difference, whether higher or lower, they would set what is known as a two-sided alternative.\nWe can re-express the formulation of our hypothesis test using the mathematical notation for our population parameter of interest, the difference in population proportions \\(p_{m} - p_{f}\\):\n\\[\n\\begin{aligned}\nH_0 &: p_{m} - p_{f} = 0\\\\\n\\text{vs } H_A&: p_{m} - p_{f} &gt; 0\n\\end{aligned}\n\\]\nObserve how the alternative hypothesis \\(H_A\\) is one-sided with \\(p_{m} - p_{f} &gt; 0\\). Had we opted for a two-sided alternative, we would have set \\(p_{m} - p_{f} \\neq 0\\). To keep things simple for now, we’ll stick with the simpler one-sided alternative. We’ll present an example of a two-sided alternative in Section 10.5.\nThird, a test statistic is a point estimate/sample statistic formula used for hypothesis testing. Note that a sample statistic is merely a summary statistic based on a sample of observations. Recall we saw in #sec-summarize that a summary statistic takes in many values and returns only one. Here, the samples would be the \\(n_m\\) = 24 resumes with male names and the \\(n_f\\) = 24 resumes with female names. Hence, the point estimate of interest is the difference in sample proportions \\(\\widehat{p}_{m} - \\widehat{p}_{f}\\).\nFourth, the observed test statistic is the value of the test statistic that we observed in real life. In our case, we computed this value using the data saved in the promotions data frame. It was the observed difference of \\(\\widehat{p}_{m} -\\widehat{p}_{f} = 0.875 - 0.583 = 0.292 = 29.2\\%\\) in favor of résumés with male names.\nFifth, the null distribution is the sampling distribution of the test statistic assuming the null hypothesis \\(H_0\\) is true. Ooof! That’s a long one! Let’s unpack it slowly. The key to understanding the null distribution is that the null hypothesis \\(H_0\\) is assumed to be true. We’re not saying that \\(H_0\\) is true at this point, we’re only assuming it to be true for hypothesis testing purposes. In our case, this corresponds to our hypothesized universe of no gender discrimination in promotion rates. Assuming the null hypothesis \\(H_0\\), also stated as “Under \\(H_0\\),” how does the test statistic vary due to sampling variation? In our case, how will the difference in sample proportions \\(\\widehat{p}_{m} - \\widehat{p}_{f}\\) vary due to sampling under \\(H_0\\)? Recall from Section 11.3.2 that distributions displaying how point estimates vary due to sampling variation are called sampling distributions. The only additional thing to keep in mind about null distributions is that they are sampling distributions assuming the null hypothesis \\(H_0\\) is true.\nIn our case, we previously visualized a null distribution in Figure 10.6, which we re-display in Figure 10.7 using our new notation and terminology. It is the distribution of the 16 differences in sample proportions our friends computed assuming a hypothetical universe of no gender discrimination. We also mark the value of the observed test statistic of 0.292 with a vertical line.\n\n\n\n\n\nFigure 10.7: Null distribution and observed test statistic.\n\n\n\n\nSixth, the \\(p\\)-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true. Double ooof! Let’s unpack this slowly as well. You can think of the \\(p\\)-value as a quantification of “surprise”: assuming \\(H_0\\) is true, how surprised are we with what we observed? Or in our case, in our hypothesized universe of no gender discrimination, how surprised are we that we observed a difference in promotion rates of 0.292 from our collected samples assuming \\(H_0\\) is true? Very surprised? Somewhat surprised?\nThe \\(p\\)-value quantifies this probability, or in the case of our 16 differences in sample proportions in Figure 10.7, what proportion had a more “extreme” result? Here, extreme is defined in terms of the alternative hypothesis \\(H_A\\) that “male” applicants are promoted at a higher rate than “female” applicants. In other words, how often was the discrimination in favor of men even more pronounced than \\(0.875 - 0.583 = 0.292 = 29.2\\%\\)?\nIn this case, 0 times out of 16, we obtained a difference in proportion greater than or equal to the observed difference of 0.292 = 29.2%. A very rare (in fact, not occurring) outcome! Given the rarity of such a pronounced difference in promotion rates in our hypothesized universe of no gender discrimination, we’re inclined to reject our hypothesized universe. Instead, we favor the hypothesis stating there is discrimination in favor of the “male” applicants. In other words, we reject \\(H_0\\) in favor of \\(H_A\\).\n\nSeventh and lastly, in many hypothesis testing procedures, it is commonly recommended to set the significance level of the test beforehand. It is denoted by the Greek letter \\(\\alpha\\) (pronounced “alpha”). This value acts as a cutoff on the \\(p\\)-value, where if the \\(p\\)-value falls below \\(\\alpha\\), we would “reject the null hypothesis \\(H_0\\).”\nAlternatively, if the \\(p\\)-value does not fall below \\(\\alpha\\), we would “fail to reject \\(H_0\\).” Note the latter statement is not quite the same as saying we “accept \\(H_0\\).” This distinction is rather subtle and not immediately obvious. So we’ll revisit it later in Section 10.4.\nThough different fields tend to use different values of \\(\\alpha\\), some commonly used values for \\(\\alpha\\) are 0.1, 0.01, and 0.05; with 0.05 being the choice people often make without putting much thought into it. We’ll talk more about \\(\\alpha\\) significance levels in Section 10.4, but first let’s fully conduct the hypothesis test corresponding to our promotions activity using the infer package (part of the tidymodels family discussed back in Section 5.3)."
  },
  {
    "objectID": "hypothesis.html#sec-ht-infer",
    "href": "hypothesis.html#sec-ht-infer",
    "title": "10  Hypothesis Testing",
    "section": "10.3 Conducting hypothesis tests",
    "text": "10.3 Conducting hypothesis tests\nThe infer package provides a workflow that emphasizes each of the steps in the overall process of conducting a hypothesis test Figure 10.8. It does so providing functions (“verbs”) that are intuitively named to indicate their role in the process:\n\nspecify() the variables of interest in your data frame.\ngenerate() replicates of bootstrap resamples with replacement.\ncalculate() the summary statistic of interest.\nvisualize() the resulting bootstrap distribution.\n\nIn this section, we’ll now show you how to use infer to conduct hypothesis tests. The workflow is illustrated in Figure 10.8.\n\n\n\nFigure 10.8: Hypothesis testing with the infer package.\n\n\n\n10.3.1 infer package workflow\n\n1. specify variables\nWe use the specify() verb to specify the response variable and, if needed, any explanatory variables for our study. In this case, because we are interested in any potential effects of gender on promotion decisions, we set decision as the response variable and gender as the explanatory variable. We do so using formula = response ~ explanatory where response is the name of the response variable in the data frame and explanatory is the name of the explanatory variable. So in our case it is decision ~ gender. The format of this, including the ~, is something we will see repeatedly as we build increasingly complex statistical models. So it may seem a bit odd now, but it is it not unique to the infer and you might as well get familiar with it now.\nGiven that we are interested in the proportion of resumes \"promoted\", and not the proportion of resumes not promoted, we set the argument success to \"promoted\".\n\npromotions %&gt;%\n    specify(formula = decision ~ gender,\n            success = \"promoted\") \n\nResponse: decision (factor)\nExplanatory: gender (factor)\n# A tibble: 48 × 2\n   decision gender\n   &lt;fct&gt;    &lt;fct&gt; \n 1 promoted male  \n 2 promoted male  \n 3 promoted male  \n 4 promoted male  \n 5 promoted male  \n 6 promoted male  \n 7 promoted male  \n 8 promoted male  \n 9 promoted male  \n10 promoted male  \n# ℹ 38 more rows\n\n\nAgain, notice how the promotions data itself doesn’t change, but the Response: decision (factor) and Explanatory: gender (factor) meta-data do. This is similar to how the group_by() verb from dplyr doesn’t change the data, but only adds “grouping” meta-data, as we saw in Section 7.5.\n\n\n10.3.1.1 2. hypothesize the null\nIn order to conduct hypothesis tests using the infer workflow, we need a new step not present for confidence intervals: hypothesize(). Recall from Section 10.2 that our hypothesis test was\n\\[\n\\begin{aligned}\nH_0 &: p_{m} - p_{f} = 0\\\\\n\\text{vs. } H_A&: p_{m} - p_{f} &gt; 0\n\\end{aligned}\n\\]\nIn other words, the null hypothesis \\(H_0\\) corresponding to our “hypothesized universe” stated that there was no difference in gender-based discrimination rates. We set this null hypothesis \\(H_0\\) in our infer workflow using the null argument of the hypothesize() function to either:\n\n\"point\" for hypotheses involving a single sample or\n\"independence\" for hypotheses involving two samples.\n\nIn our case, since we have two samples (the resumes with “male” and “female” names), we set null = \"independence\".\n\npromotions %&gt;% \n  specify(formula = decision ~ gender, success = \"promoted\") %&gt;% \n  hypothesize(null = \"independence\")\n\nResponse: decision (factor)\nExplanatory: gender (factor)\nNull Hypothesis: independence\n# A tibble: 48 × 2\n   decision gender\n   &lt;fct&gt;    &lt;fct&gt; \n 1 promoted male  \n 2 promoted male  \n 3 promoted male  \n 4 promoted male  \n 5 promoted male  \n 6 promoted male  \n 7 promoted male  \n 8 promoted male  \n 9 promoted male  \n10 promoted male  \n# ℹ 38 more rows\n\n\nAgain, the data has not changed yet. This will occur at the upcoming generate() step; we’re merely setting meta-data for now.\nWhere do the terms \"point\" and \"independence\" come from? These are two technical statistical terms. The term “point” relates from the fact that for a single group of observations, you will test the value of a single point.\nThe term “independence” relates to the fact that for two groups of observations, you are testing whether or not the response variable is independent of the explanatory variable that assigns the groups. In our case, we are testing whether the decision response variable is “independent” of the explanatory variable gender that assigns each resume to either of the two groups.\n\n\n10.3.1.2 3. generate replicates\nAfter we hypothesize() the null hypothesis, we generate() replicates of “shuffled” datasets assuming the null hypothesis is true. We do this by repeating the shuffling exercise you performed in Section 10.1 several times. Instead of merely doing it 16 times as our groups of friends did, let’s use the computer to repeat this 1000 times by setting reps = 1000 in the generate() function. However, unlike for confidence intervals where we generated replicates using type = \"bootstrap\" resampling with replacement, we’ll now perform shuffles/permutations by setting type = \"permute\". Recall that shuffles/permutations are a kind of resampling, but unlike the bootstrap method, they involve resampling without replacement.\n\npromotions_generate &lt;- promotions %&gt;% \n  specify(formula = decision ~ gender, success = \"promoted\") %&gt;% \n  hypothesize(null = \"independence\") %&gt;% \n  generate(reps = 1000, type = \"permute\")\nnrow(promotions_generate)\n\n[1] 48000\n\n\n\nObserve that the resulting data frame has 48000 rows. This is because we performed shuffles/permutations for each of the 48 rows 1000 times and \\(48000 = 1000 \\cdot 48\\). If you explore the promotions_generate data frame with View(), you’ll notice that the variable replicate indicates which resample each row belongs to. So it has the value 1 48 times, the value 2 48 times, all the way through to the value 1000 48 times.\n\n\n10.3.1.3 4. calculate summary statistics\nNow that we have generated 1000 replicates of “shuffles” assuming the null hypothesis is true, let’s calculate() the appropriate summary statistic for each of our 1000 shuffles. From Section 10.2, point estimates related to hypothesis testing have a specific name: test statistics. Since the unknown population parameter of interest is the difference in population proportions \\(p_{m} - p_{f}\\), the test statistic here is the difference in sample proportions \\(\\widehat{p}_{m} - \\widehat{p}_{f}\\).\nFor each of our 1000 shuffles, we can calculate this test statistic by setting stat = \"diff in props\". Furthermore, since we are interested in \\(\\widehat{p}_{m} - \\widehat{p}_{f}\\) we set order = c(\"male\", \"female\"). As we stated earlier, the order of the subtraction does not matter, so long as you stay consistent throughout your analysis and tailor your interpretations accordingly.\nLet’s save the result in a data frame called null_distribution:\n\nnull_distribution &lt;- promotions %&gt;% \n  specify(formula = decision ~ gender, success = \"promoted\") %&gt;% \n  hypothesize(null = \"independence\") %&gt;% \n  generate(reps = 1000, type = \"permute\") %&gt;% \n  calculate(stat = \"diff in props\", order = c(\"male\", \"female\"))\nnull_distribution\n\nResponse: decision (factor)\nExplanatory: gender (factor)\nNull Hypothesis: independence\n# A tibble: 1,000 × 2\n   replicate    stat\n       &lt;int&gt;   &lt;dbl&gt;\n 1         1 -0.125 \n 2         2 -0.0417\n 3         3 -0.0417\n 4         4 -0.0417\n 5         5 -0.0417\n 6         6  0.208 \n 7         7 -0.125 \n 8         8  0.125 \n 9         9 -0.0417\n10        10  0.208 \n# ℹ 990 more rows\n\n\nObserve that we have 1000 values of stat, each representing one instance of \\(\\widehat{p}_{m} - \\widehat{p}_{f}\\) in a hypothesized world of no gender discrimination. Observe as well that we chose the name of this data frame carefully: null_distribution. Recall once again from Section 10.2 that sampling distributions when the null hypothesis \\(H_0\\) is assumed to be true have a special name: the null distribution.\nWhat was the observed difference in promotion rates? In other words, what was the observed test statistic \\(\\widehat{p}_{m} - \\widehat{p}_{f}\\)? Recall from Section 10.1 that we computed this observed difference by hand to be 0.875 - 0.583 = 0.292 = 29.2%. We can also compute this value using the previous infer code but with the hypothesize() and generate() steps removed. Let’s save this in obs_diff_prop:\n\nobs_diff_prop &lt;- promotions %&gt;% \n  specify(decision ~ gender, success = \"promoted\") %&gt;% \n  calculate(stat = \"diff in props\", order = c(\"male\", \"female\"))\nobs_diff_prop\n\nResponse: decision (factor)\nExplanatory: gender (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1 0.292\n\n\n\n\n10.3.1.4 5. visualize the p-value\nThe final step is to measure how surprised we are by a promotion difference of 29.2% in a hypothesized universe of no gender discrimination. If the observed difference of 0.292 is highly unlikely, then we would be inclined to reject the validity of our hypothesized universe.\nWe start by visualizing the null distribution of our 1000 values of \\(\\widehat{p}_{m} - \\widehat{p}_{f}\\) using visualize() in Figure 10.9. Recall that these are values of the difference in promotion rates assuming \\(H_0\\) is true. This corresponds to being in our hypothesized universe of no gender discrimination.\n\nvisualize(null_distribution, bins = 10)\n\n\n\n\nFigure 10.9: ?(caption)\n\n\n\n\nLet’s now add what happened in real life to Figure 10.9, the observed difference in promotion rates of 0.875 - 0.583 = 0.292 = 29.2%. However, instead of merely adding a vertical line using geom_vline(), let’s use the shade_p_value() function with obs_stat set to the observed test statistic value we saved in obs_diff_prop.\nFurthermore, we’ll set the direction = \"right\" reflecting our alternative hypothesis \\(H_A: p_{m} - p_{f} &gt; 0\\). Recall our alternative hypothesis \\(H_A\\) is that \\(p_{m} - p_{f} &gt; 0\\), stating that there is a difference in promotion rates in favor of resumes with male names. “More extreme” here corresponds to differences that are “bigger” or “more positive” or “more to the right.” Hence we set the direction argument of shade_p_value() to be \"right\".\nOn the other hand, had our alternative hypothesis \\(H_A\\) been the other possible one-sided alternative \\(p_{m} - p_{f} &lt; 0\\), suggesting discrimination in favor of resumes with female names, we would’ve set direction = \"left\". Had our alternative hypothesis \\(H_A\\) been two-sided \\(p_{m} - p_{f} \\neq 0\\), suggesting discrimination in either direction, we would’ve set direction = \"both\".\n\nvisualize(null_distribution, bins = 10) + \n  shade_p_value(obs_stat = obs_diff_prop, direction = \"right\")\n\n\n\n\nFigure 10.10: Shaded histogram to show \\(p\\)-value.\n\n\n\n\nIn the resulting Figure 10.10, the solid dark line marks 0.292 = 29.2%. However, what does the shaded-region correspond to? This is the \\(p\\)-value. Recall the definition of the \\(p\\)-value from Section 10.2:\n\nA \\(p\\)-value is the probability of obtaining a test statistic just as or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\n\nSo judging by the shaded region in Figure 10.10, it seems we would somewhat rarely observe differences in promotion rates of 0.292 = 29.2% or more in a hypothesized universe of no gender discrimination. In other words, the \\(p\\)-value is somewhat small. Hence, we would be inclined to reject this hypothesized universe, or using statistical language we would “reject \\(H_0\\).”\nWhat fraction of the null distribution is shaded? In other words, what is the exact value of the \\(p\\)-value? We can compute it using the get_p_value() function with the same arguments as the previous shade_p_value() code:\n\nnull_distribution %&gt;% \n  get_p_value(obs_stat = obs_diff_prop, direction = \"right\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.024\n\n\nKeeping the definition of a \\(p\\)-value in mind, the probability of observing a difference in promotion rates as large as 0.292 = 29.2% due to sampling variation alone in the null distribution is 0.024 = 2.4%. Since this \\(p\\)-value is smaller than our pre-specified significance level \\(\\alpha\\) = 0.05, we reject the null hypothesis \\(H_0: p_{m} - p_{f} = 0\\). In other words, this \\(p\\)-value is sufficiently small to reject our hypothesized universe of no gender discrimination. We instead have enough evidence to change our mind in favor of gender discrimination being a likely culprit here. Observe that whether we reject the null hypothesis \\(H_0\\) or not depends in large part on our choice of significance level \\(\\alpha\\). We’ll discuss this more in Section 10.4.3.\n\n\n\n10.3.2 “There is only one test”\nLet’s recap the steps necessary to conduct a hypothesis test using the terminology, notation, and definitions related to sampling you saw in Section 10.2 and the infer workflow from Section 10.3.1:\n\nspecify() the variables of interest in your data frame.\nhypothesize() the null hypothesis \\(H_0\\). In other words, set a “model for the universe” assuming \\(H_0\\) is true.\ngenerate() shuffles assuming \\(H_0\\) is true. In other words, simulate data assuming \\(H_0\\) is true.\ncalculate() the test statistic of interest, both for the observed data and your simulated data.\nvisualize() the resulting null distribution and compute the \\(p\\)-value by comparing the null distribution to the observed test statistic.\n\nWhile this is a lot to digest, especially the first time you encounter hypothesis testing, the nice thing is that once you understand this general framework, then you can understand any hypothesis test. In a famous blog post, computer scientist Allen Downey called this the “There is only one test” framework, for which he created the flowchart displayed in Figure 10.11.\n\n\n\nFigure 10.11: Allen Downey’s hypothesis testing framework.\n\n\nNotice its similarity with the “hypothesis testing with infer” diagram you saw in Figure 10.8. That’s because the infer package was explicitly designed to match the “There is only one test” framework. So if you can understand the framework, you can easily generalize these ideas for all hypothesis testing scenarios. Whether for population proportions \\(p\\), population means \\(\\mu\\), differences in population proportions \\(p_1 - p_2\\), differences in population means \\(\\mu_1 - \\mu_2\\), and as you’ll see in Chapter @ref(inference-for-regression) on inference for regression, population regression slopes \\(\\beta_1\\) as well. In fact, it applies more generally even than just these examples to more complicated hypothesis tests and test statistics as well."
  },
  {
    "objectID": "hypothesis.html#sec-ht-interpretation",
    "href": "hypothesis.html#sec-ht-interpretation",
    "title": "10  Hypothesis Testing",
    "section": "10.4 Interpreting hypothesis tests",
    "text": "10.4 Interpreting hypothesis tests\nInterpreting the results of hypothesis tests is one of the more challenging aspects of this method for statistical inference. In this section, we’ll focus on ways to help with deciphering the process and address some common misconceptions.\n\n10.4.1 Two possible outcomes\nIn Section 10.2, we mentioned that given a pre-specified significance level \\(\\alpha\\) there are two possible outcomes of a hypothesis test:\n\nIf the \\(p\\)-value is less than \\(\\alpha\\), then we reject the null hypothesis \\(H_0\\) in favor of \\(H_A\\).\nIf the \\(p\\)-value is greater than or equal to \\(\\alpha\\), we fail to reject the null hypothesis \\(H_0\\).\n\nUnfortunately, the latter result is often misinterpreted as “accepting the null hypothesis \\(H_0\\).” While at first glance it may seem that the statements “failing to reject \\(H_0\\)” and “accepting \\(H_0\\)” are equivalent, there actually is a subtle difference. Saying that we “accept the null hypothesis \\(H_0\\)” is equivalent to stating that “we think the null hypothesis \\(H_0\\) is true.” However, saying that we “fail to reject the null hypothesis \\(H_0\\)” is saying something else: “While \\(H_0\\) might still be false, we don’t have enough evidence to say so.” In other words, there is an absence of enough proof. However, the absence of proof is not proof of absence.\nTo further shed light on this distinction, let’s use the United States criminal justice system as an analogy. A criminal trial in the United States is a similar situation to hypothesis tests whereby a choice between two contradictory claims must be made about a defendant who is on trial:\n\nThe defendant is truly either “innocent” or “guilty.”\nThe defendant is presumed “innocent until proven guilty.”\nThe defendant is found guilty only if there is strong evidence that the defendant is guilty. The phrase “beyond a reasonable doubt” is often used as a guideline for determining a cutoff for when enough evidence exists to find the defendant guilty.\nThe defendant is found to be either “not guilty” or “guilty” in the ultimate verdict.\n\nIn other words, not guilty verdicts are not suggesting the defendant is innocent, but instead that “while the defendant may still actually be guilty, there wasn’t enough evidence to prove this fact.” Now let’s make the connection with hypothesis tests:\n\nEither the null hypothesis \\(H_0\\) or the alternative hypothesis \\(H_A\\) is true.\nHypothesis tests are conducted assuming the null hypothesis \\(H_0\\) is true.\nWe reject the null hypothesis \\(H_0\\) in favor of \\(H_A\\) only if the evidence found in the sample suggests that \\(H_A\\) is true. The significance level \\(\\alpha\\) is used as a guideline to set the threshold on just how strong of evidence we require.\nWe ultimately decide to either “fail to reject \\(H_0\\)” or “reject \\(H_0\\).”\n\nSo while gut instinct may suggest “failing to reject \\(H_0\\)” and “accepting \\(H_0\\)” are equivalent statements, they are not. “Accepting \\(H_0\\)” is equivalent to finding a defendant innocent. However, courts do not find defendants “innocent,” but rather they find them “not guilty.” Putting things differently, defense attorneys do not need to prove that their clients are innocent, rather they only need to prove that clients are not “guilty beyond a reasonable doubt”.\nSo going back to our resumes activity in Section 10.3, recall that our hypothesis test was \\(H_0: p_{m} - p_{f} = 0\\) versus \\(H_A: p_{m} - p_{f} &gt; 0\\) and that we used a pre-specified significance level of \\(\\alpha\\) = 0.05. We found a \\(p\\)-value of 0.024. Since the \\(p\\)-value was smaller than \\(\\alpha\\) = 0.05, we rejected \\(H_0\\). In other words, we found needed levels of evidence in this particular sample to say that \\(H_0\\) is false at the \\(\\alpha\\) = 0.05 significance level. We also state this conclusion using non-statistical language: we found enough evidence in this data to suggest that there was gender discrimination at play.\n\n\n10.4.2 Types of errors\nUnfortunately, there is some chance a jury or a judge can make an incorrect decision in a criminal trial by reaching the wrong verdict. For example, finding a truly innocent defendant “guilty”. Or on the other hand, finding a truly guilty defendant “not guilty.” This can often stem from the fact that prosecutors don’t have access to all the relevant evidence, but instead are limited to whatever evidence the police can find.\nThe same holds for hypothesis tests. We can make incorrect decisions about a population parameter because we only have a sample of data from the population and thus sampling variation can lead us to incorrect conclusions.\nThere are two possible erroneous conclusions in a criminal trial: either (1) a truly innocent person is found guilty or (2) a truly guilty person is found not guilty. Similarly, there are two possible errors in a hypothesis test: either (1) rejecting \\(H_0\\) when in fact \\(H_0\\) is true, called a Type I error or (2) failing to reject \\(H_0\\) when in fact \\(H_0\\) is false, called a Type II error. Another term used for “Type I error” is “false positive,” while another term for “Type II error” is “false negative.”\nThis risk of error is the price researchers pay for basing inference on a sample instead of performing a census on the entire population. But as we’ve seen in our numerous examples and activities so far, censuses are often very expensive and other times impossible, and thus researchers have no choice but to use a sample. Thus in any hypothesis test based on a sample, we have no choice but to tolerate some chance that a Type I error will be made and some chance that a Type II error will occur.\nTo help understand the concepts of Type I error and Type II errors, we apply these terms to our criminal justice analogy in Figure 10.12.\n\n\n\nFigure 10.12: Type I and Type II errors in criminal trials\n\n\nThus a Type I error corresponds to incorrectly putting a truly innocent person in jail, whereas a Type II error corresponds to letting a truly guilty person go free. Let’s show the corresponding table in Figure 10.13 for hypothesis tests.\n\n\n\nFigure 10.13: Type I and Type II errors in hypothesis tests.\n\n\n\n\n10.4.3 How do we choose alpha?\nIf we are using a sample to make inferences about a population, we run the risk of making errors. For confidence intervals, a corresponding “error” would be constructing a confidence interval that does not contain the true value of the population parameter. For hypothesis tests, this would be making either a Type I or Type II error. Obviously, we want to minimize the probability of either error; we want a small probability of making an incorrect conclusion:\n\nThe probability of a Type I Error occurring is denoted by \\(\\alpha\\). The value of \\(\\alpha\\) is called the significance level of the hypothesis test, which we defined in Section 10.2.\nThe probability of a Type II Error is denoted by \\(\\beta\\). The value of \\(1-\\beta\\) is known as the power of the hypothesis test.\n\nIn other words, \\(\\alpha\\) corresponds to the probability of incorrectly rejecting \\(H_0\\) when in fact \\(H_0\\) is true. On the other hand, \\(\\beta\\) corresponds to the probability of incorrectly failing to reject \\(H_0\\) when in fact \\(H_0\\) is false.\nIdeally, we want \\(\\alpha = 0\\) and \\(\\beta = 0\\), meaning that the chance of making either error is 0. However, this can never be the case in any situation where we are sampling for inference. There will always be the possibility of making either error when we use sample data. Furthermore, these two error probabilities are inversely related. As the probability of a Type I error goes down, the probability of a Type II error goes up.\nWhat is typically done in practice is to fix the probability of a Type I error by pre-specifying a significance level \\(\\alpha\\) and then try to minimize \\(\\beta\\). In other words, we will tolerate a certain fraction of incorrect rejections of the null hypothesis \\(H_0\\), and then try to minimize the fraction of incorrect non-rejections of \\(H_0\\).\nSo for example if we used \\(\\alpha\\) = 0.01, we would be using a hypothesis testing procedure that in the long run would incorrectly reject the null hypothesis \\(H_0\\) one percent of the time. This is analogous to setting the confidence level of a confidence interval.\nSo what value should you use for \\(\\alpha\\)? Different fields have different conventions, but some commonly used values include 0.10, 0.05, 0.01, and 0.001. However, it is important to keep in mind that if you use a relatively small value of \\(\\alpha\\), then all things being equal, \\(p\\)-values will have a harder time being less than \\(\\alpha\\). Thus we would reject the null hypothesis less often. In other words, we would reject the null hypothesis \\(H_0\\) only if we have very strong evidence to do so. This is known as a “conservative” test.\nOn the other hand, if we used a relatively large value of \\(\\alpha\\), then all things being equal, \\(p\\)-values will have an easier time being less than \\(\\alpha\\). Thus we would reject the null hypothesis more often. In other words, we would reject the null hypothesis \\(H_0\\) even if we only have mild evidence to do so. This is known as a “liberal” test."
  },
  {
    "objectID": "hypothesis.html#sec-ht-case-study",
    "href": "hypothesis.html#sec-ht-case-study",
    "title": "10  Hypothesis Testing",
    "section": "10.5 Case study: Who’s the GOAT?",
    "text": "10.5 Case study: Who’s the GOAT?\nLet’s apply our knowledge of hypothesis testing to answer the question: “Who is better, Michael Jordan or Steph Curry?”. We’ll investigate if, on average, Michael Jordan or Steph Curry averaged more points per game.\nLet’s perform an exploratory data analysis of the relevant data. Let’s first filter our data set down so that we only have rows corresponding to either Michael Jordan or Steph Curry.\n\nmjsc &lt;- nba %&gt;%\n    filter(player_name %in% c(\"Michael Jordan\", \"Stephen Curry\"))\n\nWe can use a box plot to show the relationship between a numerical and a categorical variable.\n\nmjsc %&gt;%\nggplot(aes(x = player_name, y = pts)) +\n  geom_boxplot() +\n  labs(y = \"Points per Game\")\n\n\n\n\nFigure 10.14: Boxplot of pts vs. player_name.\n\n\n\n\nEyeballing Figure 10.14, Jordan has a higher median. Do we have reason to believe, however, that there is a significant difference between the mean pts for Michael Jordan compared to Steph Curry? It’s hard to say just based on this plot. The boxplot does show that the median of Jordan’s pts is higher than Curry’s median.\nHowever, there is a large amount of overlap between the boxes. Recall that the median isn’t necessarily the same as the mean either, depending on whether the distribution is skewed.\nLet’s calculate some summary statistics split by the binary categorical variable player_name: the number of seasons for which we have observations, the mean of pts, and the standard deviation split by player_name. We’ll do this using dplyr data wrangling verbs. Notice in particular how we count the number of rows associated with each player using the n() summary function.\n\nmjsc %&gt;%\n    group_by(player_name) %&gt;%\n    summarize(n = n(),\n              mean_pts = mean(pts),\n              std_dev = sd(pts))\n\n# A tibble: 2 × 4\n  player_name        n mean_pts std_dev\n  &lt;chr&gt;          &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 Michael Jordan     4     25.3    4.62\n2 Stephen Curry     13     23.8    4.92\n\n\nObserve that Jordan has an average pts value of 25.3 and Curry has an average pts value of 23.762. The difference in these is thus 25.3 - 23.762 = 1.538. So there appears to be an edge of 1.538 points in favor of Jordan. The question is, however, are these results indicative of a true difference for all Jordan’s and Curry’s seasons? Or could we attribute this difference to chance sampling variation?\n\n10.5.1 Sampling scenario\nLet’s now revisit this study in terms of terminology and notation related to sampling we studied in Section 11.3.1. The study population includes all the seasons that Michael Jordan played and all the seasons that Steph Curry played. However, we do not have all of Michael Jordan’s season averages (the data set begins in 1996, but Jordan’s rookie season was 1984) nor do we have all of Steph Curry’s season averages (he is currently continuing to play). Thus, we have a sample from the population consisting of 17 rows in the nba dataset corresponding to these two players (the fact that we do not have a random sample from either player will be discussed in Section 10.6.1).\nWhen conducting a hypothesis test, our goal is to generalize from our sample to the entire population. What are the relevant population parameter and point estimates? We introduce the fourth sampling scenario in Table 10.2.\n\n\n\n\nTable 10.2: Scenarios of sampling for inference\n\n\nScenario\nPopulation parameter\nNotation\nPoint estimate\nSymbol(s)\n\n\n\n\n1\nPopulation proportion\n$p$\nSample proportion\n$\\widehat{p}$\n\n\n2\nPopulation mean\n$\\mu$\nSample mean\n$\\overline{x}$ or $\\widehat{\\mu}$\n\n\n3\nDifference in population proportions\n$p_1 - p_2$\nDifference in sample proportions\n$\\widehat{p}_1 - \\widehat{p}_2$\n\n\n4\nDifference in population means\n$\\mu_1 - \\mu_2$\nDifference in sample means\n$\\overline{x}_1 - \\overline{x}_2$ or $\\widehat{\\mu}_1 - \\widehat{\\mu}_2$\n\n\n\n\n\n\n\n\nSo, whereas the sampling bowl exercise in Section 11.1 concerned proportions, we are now concerned with differences in means.\nIn other words, the population parameter of interest is the difference in population mean ratings \\(\\mu_{MJ} - \\mu_{SC}\\), where \\(\\mu_{MJ}\\) is the mean pts of all of Jordan’s observations and \\(\\mu_{SC}\\) is the mean pts of all of Curry’s observations. Additionally the point estimate/sample statistic of interest is the difference in sample means \\(\\overline{x}_{MJ} - \\overline{x}_{SC}\\), where \\(\\overline{x}_{MJ}\\) is the mean pts of the \\(n_{MJ}\\) = 4 movies in our sample and \\(\\overline{x}_{SC}\\) is the mean rating of the \\(n_{SC}\\) = 13 in our sample. Based on our earlier exploratory data analysis, our estimate \\(\\overline{x}_{SC} - \\overline{x}_{SC}\\) is \\(25.3 - 23.762 = 1.538\\).\nSo there appears to be a slight difference of 1.538 in favor of Jordan. The question is, however, could this difference of 1.538 be merely due to chance and sampling variation? Or are these results indicative of a true difference in points scored for all of Jordan’s and Curry’s seasons? To answer this question, we’ll use hypothesis testing.\n\n\n10.5.2 Conducting the hypothesis test\nWe’ll be testing:\n\\[\n\\begin{aligned}\nH_0 &: \\mu_{MJ} - \\mu_{SC} = 0\\\\\n\\text{vs } H_A&: \\mu_{MJ} - \\mu_{SC} \\neq 0\n\\end{aligned}\n\\]\nIn other words, the null hypothesis \\(H_0\\) suggests that both Jordan and Curry have the same mean pts. This is the “hypothesized universe” we’ll assume is true. On the other hand, the alternative hypothesis \\(H_A\\) suggests that there is a difference. Unlike the one-sided alternative we used in the promotions exercise \\(H_A: p_m - p_f &gt; 0\\), we are now considering a two-sided alternative of \\(H_A: \\mu_{MJ} - \\mu_{SC} \\neq 0\\).\n\n10.5.2.1 1. specify variables\nLet’s now perform all the steps of the infer workflow. We first specify() the variables of interest in our data using the formula pts ~ player_name. This tells infer that the numerical variable pts is the outcome variable, while the variable player_name (which is now dichotomous because we filtered out any rows that didn’t correspond to either Jordan or Curry) is the explanatory variable. Note that unlike previously when we were interested in proportions, since we are now interested in the mean of a numerical variable, we do not need to set the success argument.\n\nmjsc %&gt;% \n  specify(formula = pts ~ player_name)\n\nResponse: pts (numeric)\nExplanatory: player_name (factor)\n# A tibble: 17 × 2\n     pts player_name   \n   &lt;dbl&gt; &lt;fct&gt;         \n 1  29.6 Michael Jordan\n 2  28.7 Michael Jordan\n 3  22.9 Michael Jordan\n 4  20   Michael Jordan\n 5  17.5 Stephen Curry \n 6  18.6 Stephen Curry \n 7  14.7 Stephen Curry \n 8  22.9 Stephen Curry \n 9  24   Stephen Curry \n10  23.8 Stephen Curry \n11  30.1 Stephen Curry \n12  25.3 Stephen Curry \n13  26.4 Stephen Curry \n14  27.3 Stephen Curry \n15  20.8 Stephen Curry \n16  32   Stephen Curry \n17  25.5 Stephen Curry \n\n\nObserve at this point that the data in mjsc has not changed. The only change so far is the newly defined Response: pts (numeric) and Explanatory: player_name (factor) meta-data.\n\n\n10.5.2.2 2. hypothesize the null\nWe set the null hypothesis \\(H_0: \\mu_{MJ} - \\mu_{SC} = 0\\) by using the hypothesize() function. Since we have two samples, action and romance movies, we set null to be \"independence\" as we described in Section 10.3.\n\nmjsc %&gt;% \n  specify(formula = pts ~ player_name) %&gt;% \n  hypothesize(null = \"independence\")\n\nResponse: pts (numeric)\nExplanatory: player_name (factor)\nNull Hypothesis: independence\n# A tibble: 17 × 2\n     pts player_name   \n   &lt;dbl&gt; &lt;fct&gt;         \n 1  29.6 Michael Jordan\n 2  28.7 Michael Jordan\n 3  22.9 Michael Jordan\n 4  20   Michael Jordan\n 5  17.5 Stephen Curry \n 6  18.6 Stephen Curry \n 7  14.7 Stephen Curry \n 8  22.9 Stephen Curry \n 9  24   Stephen Curry \n10  23.8 Stephen Curry \n11  30.1 Stephen Curry \n12  25.3 Stephen Curry \n13  26.4 Stephen Curry \n14  27.3 Stephen Curry \n15  20.8 Stephen Curry \n16  32   Stephen Curry \n17  25.5 Stephen Curry \n\n\n\n\n10.5.2.3 3. generate replicates\nAfter we have set the null hypothesis, we generate “shuffled” replicates assuming the null hypothesis is true by repeating the shuffling/permutation exercise you performed in Section 10.1.\nWe’ll repeat this resampling without replacement of type = \"permute\" a total of reps = 1000 times. Feel free to run the code below to check out what the generate() step produces.\n\nmjsc %&gt;% \n  specify(formula = pts ~ player_name) %&gt;% \n  hypothesize(null = \"independence\") %&gt;% \n  generate(reps = 1000, type = \"permute\") %&gt;% \n  View()\n\n\n\n10.5.2.4 4. calculate summary statistics\nNow that we have 1000 replicated “shuffles” assuming the null hypothesis \\(H_0\\) that both Jordan and Curry average the same numebr of points per game, let’s calculate() the appropriate summary statistic for these 1000 replicated shuffles. From Section 10.2, summary statistics relating to hypothesis testing have a specific name: test statistics. Since the unknown population parameter of interest is the difference in population means \\(\\mu_{MJ} - \\mu_{SC}\\), the test statistic of interest here is the difference in sample means \\(\\overline{x}_{MJ} - \\overline{x}_{SC}\\).\nFor each of our 1000 shuffles, we can calculate this test statistic by setting stat = \"diff in means\". Furthermore, since we are interested in \\(\\overline{x}_{MJ} - \\overline{x}_{SC}\\), we set order = c(\"Michael Jordan\", \"Stephen Curry\"). Let’s save the results in a data frame called null_distribution_movies:\n\nnull_distribution_mjsc &lt;- mjsc %&gt;%\n    specify(formula = pts ~ player_name) %&gt;%\n    hypothesize(null = \"independence\") %&gt;%\n    generate(reps = 1000, type = \"permute\") %&gt;%\n    calculate(stat = \"diff in means\",\n              order = c(\"Michael Jordan\",\n                        \"Stephen Curry\"))\nnull_distribution_mjsc\n\nResponse: pts (numeric)\nExplanatory: player_name (factor)\nNull Hypothesis: independence\n# A tibble: 1,000 × 2\n   replicate   stat\n       &lt;int&gt;  &lt;dbl&gt;\n 1         1  1.70 \n 2         2 -2.55 \n 3         3 -3.59 \n 4         4 -2.12 \n 5         5  3.57 \n 6         6 -2.42 \n 7         7 -0.292\n 8         8  0.688\n 9         9  2.68 \n10        10 -1.11 \n# ℹ 990 more rows\n\n\nObserve that we have 1000 values of stat, each representing one instance of \\(\\overline{x}_{MJ} - \\overline{x}_{SC}\\). The 1000 values form the null distribution, which is the technical term for the sampling distribution of the difference in sample means \\(\\overline{x}_{MJ} - \\overline{x}_{SC}\\) assuming \\(H_0\\) is true. What happened in real life? What was the observed difference in promotion rates? What was the observed test statistic \\(\\overline{x}_{MJ} - \\overline{x}_{SC}\\)? Recall from our earlier data wrangling, this observed difference in means was \\(25.3 - 23.762 = 1.538\\). We can also achieve this using the code that constructed the null distribution null_distribution_mjsc but with the hypothesize() and generate() steps removed. Let’s save this in obs_diff_means:\n\nobs_diff_means &lt;- mjsc %&gt;%\n    specify(formula = pts ~ player_name) %&gt;%\n    calculate(stat = \"diff in means\",\n              order = c(\"Michael Jordan\",\n                        \"Stephen Curry\"))\nobs_diff_means\n\nResponse: pts (numeric)\nExplanatory: player_name (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  1.54\n\n\n\n\n10.5.2.5 5. visualize the p-value\nLastly, in order to compute the \\(p\\)-value, we have to assess how “extreme” the observed difference in means of 1.538 is. We do this by comparing 1.538 to our null distribution, which was constructed in a hypothesized universe of no true difference between Jordan and Curry. Let’s visualize both the null distribution and the \\(p\\)-value in Figure 10.15. Unlike our example in Section 10.3.1 involving promotions, since we have a two-sided \\(H_A: \\mu_{MJ} - \\mu_{SC} \\neq 0\\), we have to allow for both possibilities for more extreme, so we set direction = \"both\".\n\n\n\n\n\nFigure 10.15: Null distribution, observed test statistic, and \\(p\\)-value.\n\n\n\n\nLet’s go over the elements of this plot. First, the histogram is the null distribution. Second, the solid line is the observed test statistic, or the difference in sample means we observed in real life of \\(25.3 - 23.762 = 1.538\\). Third, the two shaded areas of the histogram form the \\(p\\)-value, or the probability of obtaining a test statistic just as or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\nWhat proportion of the null distribution is shaded? In other words, what is the numerical value of the \\(p\\)-value? We use the get_p_value() function to compute this value:\n\np_value_mjsc &lt;- null_distribution_mjsc %&gt;%\n  get_p_value(obs_stat = obs_diff_means, direction = \"both\") %&gt;%\n  mutate(p_value = round(p_value, 3))\n\nThis \\(p\\)-value of 0.612 is extremely modest. In other words, there is a very good chance that we’d observe a difference of 25.3 - 23.762 = 1.538 in a hypothesized universe where there was truly no difference in ratings.\nThis \\(p\\)-value is larger than our \\(\\alpha\\) of 0.05. Thus, we fail to reject the null hypothesis \\(H_0: \\mu_{MJ} - \\mu_{SC} = 0\\). In non-statistical language, the conclusion is: we do not have the evidence needed in this sample of data to suggest that we should reject the hypothesis that there is no difference in points per game between Michael Jordan and Steph Curry."
  },
  {
    "objectID": "hypothesis.html#sec-nhst-conclusion",
    "href": "hypothesis.html#sec-nhst-conclusion",
    "title": "10  Hypothesis Testing",
    "section": "10.6 Conclusion",
    "text": "10.6 Conclusion\n\n10.6.1 Theory-based hypothesis tests\nMost of the time researchers conduct hypothesis tests, they are using traditional, theory-based methods (also known as “parametric” test). These methods rely on probability models, probability distributions, and a few assumptions to construct the null distribution. This stands in stark contrast to the approach we’ve just seen where we relied on computer simulations to construct the null distribution.\nThese traditional theory-based methods have been used for decades mostly because researchers didn’t have access to computers that could run thousands of calculations quickly and efficiently. Now that computing power is much cheaper and more accessible, simulation-based methods are much more feasible. However, researchers in many fields continue to use theory-based methods. Hence, we make it a point to include an example here.\n\n10.6.1.1 Two-sample t-statistic\nLet’s reconsider the question we asked in Section 10.5: is Michael Jordan better than Steph Curry? We’ll again investigate if, on average, Michael Jordan or Steph Curry averaged more points per game. Recall that we can construct the distribution of the test statistic \\(\\bar{x}_{MJ} - \\bar{x}_{SC}\\) assuming that the null hypothesis is true, just as we did in Section 10.5.\n\n# Construct null distribution of t:\nnull_distribution_mjsc &lt;- mjsc %&gt;% \n  specify(formula = pts ~ player_name) %&gt;% \n  hypothesize(null = \"independence\") %&gt;% \n  generate(reps = 1000, type = \"permute\") %&gt;% \n  # Notice we switched stat from \"diff in means\" to \"t\"\n  calculate(stat = \"diff in means\", order = c(\"Michael Jordan\", \"Stephen Curry\"))\n\nThe infer package also includes some built-in theory-based test statistics as well. So instead of calculating the test statistic of interest as the \"diff in means\" \\(\\bar{x}_{MJ} - \\bar{x}_{SC}\\), we can calculate this defined two-sample \\(t\\)-statistic by setting stat = \"t\".\n\n# Construct null distribution of t:\nnull_distribution_mjsc_t &lt;- mjsc %&gt;% \n  specify(formula = pts ~ player_name) %&gt;% \n  hypothesize(null = \"independence\") %&gt;% \n  generate(reps = 1000, type = \"permute\") %&gt;% \n  # Notice we switched stat from \"diff in means\" to \"t\"\n  calculate(stat = \"t\", order = c(\"Michael Jordan\", \"Stephen Curry\"))\n\nWe can then visualize how these two compare in Figure 10.16.\n\nvisualize(null_distribution_mjsc_t, bins = 10, method = \"both\")\n\n\n\n\nFigure 10.16: Null distribution using t-statistic and t-distribution.\n\n\n\n\nObserve that the curve does a good job of approximating the histogram here. To calculate the \\(p\\)-value in this case, we need to figure out how much of the total area under the \\(t\\)-distribution curve is at or “more extreme” than our observed two-sample \\(t\\)-statistic. Since \\(H_A: \\mu_a - \\mu_r \\neq 0\\) is a two-sided alternative, we need to add up the areas in both tails.\nWe first compute the observed two-sample \\(t\\)-statistic using infer verbs. This shortcut calculation further assumes that the null hypothesis is true: that the population of action and romance movies have an equal average rating.\n\nobs_two_sample_t &lt;- mjsc %&gt;% \n  specify(formula = pts ~ player_name) %&gt;% \n  calculate(stat = \"t\", order = c(\"Michael Jordan\", \"Stephen Curry\"))\nobs_two_sample_t\n\nResponse: pts (numeric)\nExplanatory: player_name (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1 0.574\n\n\nWe want to find the percentage of values that are at or below obs_two_sample_t \\(= 0.574\\) or at or above -obs_two_sample_t \\(= -0.574\\). We use the shade_p_value() function with the direction argument set to \"both\" to do this:\n\nvisualize(null_distribution_mjsc_t, method = \"both\") +\n  shade_p_value(obs_stat = obs_two_sample_t, direction = \"both\")\n\n\n\n\nNull distribution using t-statistic and t-distribution with \\(p\\)-value shaded.\n\n\n\n\n(We’ll discuss this warning message shortly.) What is the \\(p\\)-value? We apply get_p_value() to our null distribution saved in null_distribution_movies_t:\n\nnull_distribution_mjsc_t %&gt;%\n  get_p_value(obs_stat = obs_two_sample_t, direction = \"both\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1    0.58\n\n\nOur \\(p\\)-value is even closer to 1.0 than it was in Section 10.5, and thus we are again reject \\(H_0\\).\nLet’s come back to that earlier warning message: Check to make sure the conditions have been met for the theoretical method. {infer} currently does not check these for you. To be able to use the \\(t\\)-test and other such theoretical methods, there are always a few conditions to check. The infer package does not automatically check these conditions, hence the warning message we received. These conditions are necessary so that the underlying mathematical theory holds. In order for the results of our two-sample \\(t\\)-test to be valid, three conditions must be met:\n\nNearly normal populations or large sample sizes. A general rule of thumb that works in many (but not all) situations is that the sample size \\(n\\) should be greater than 30.\nBoth samples are selected independently of each other.\nAll observations are independent from each other.\n\nLet’s see if these conditions hold for our mjsc data:\n\nThis is unlikely to be met because \\(n_a\\) = 4 and \\(n_r\\) = 13 are both smaller than than 30, violating our rule of thumb.\nThis is not met because we didn’t sampled the seasons (neither Jordan’s nor Curry’s) at random and in an unbiased fashion from all seasons each player has played.\nObservations are not independent. For example, Curry had very little playing time early in his career (he was not a starter) and thus had fewer opportunities to score. As his skill increased, he was given more and more playing time and scored more. For all players, one would expect that pts taken from two consecutive seasons would be more similar than pts taken from two seasons that are far apart.\n\nIf we assumed that all three conditions are met, then we can be reasonably certain that the theory-based \\(t\\)-test results are valid. If any of the conditions are not met, we cannot put as much trust into the results of our hypothesis test. On the other hand, the only assumption that needs to be met in the simulation-based method is that the sample is selected at random. Thus, there are many reasons to prefer simulation-based methods: they have fewer assumptions, they are conceptually easier to understand, and because computing power has become easily accessible, they can be run quickly. That being said, because much of the world’s research still relies on traditional theory-based methods, we also believe it is important to understand them.\nYou may be wondering why we chose reps = 1000 for these simulation-based methods. We’ve noticed that after around 1000 replicates for the null distribution. You can change this value to something like 10,000 if you would like even finer detail, but this will take more time to compute. Feel free to iterate on this as you like to get an even better idea about the shape of the distributions as you wish.\n\n\n\n10.6.2 When inference is not needed\nWe’ve now walked through several different examples of how to use the infer package to perform statistical inference: constructing confidence intervals and conducting hypothesis tests. For each of these examples, we made it a point to always perform an exploratory data analysis (EDA) first; specifically, by looking at the raw data values, by using data visualization with ggplot2, and by data wrangling with dplyr beforehand. We highly encourage you to always do the same. As a beginner to statistics, EDA helps you develop intuition as to what statistical methods like confidence intervals and hypothesis tests can tell us. Even as a seasoned practitioner of statistics, EDA helps guide your statistical investigations. In particular, is statistical inference even needed?\nLet’s consider an example. Say we’re interested in another pair of players. Let’s say you wanted to know whether Steph Curry’s average points per game was higher than that of his father, Dell Curry. Let’s filter this data frame to only include Hawaiian and Alaska Airlines using their carrier codes HA and AS:\n\nscdc &lt;- nba %&gt;% \n  filter(player_name %in% c(\"Stephen Curry\", \"Dell Curry\"))\n\nThere are two possible statistical inference methods we could use to answer such questions. First, we could construct a 95% confidence interval for the difference in population means \\(\\mu_{SC} - \\mu_{DC}\\), where \\(\\mu_{SC}\\) is the mean of all of Steph’s rows and \\(\\mu_{DC}\\) is the mean air time of all of Dell’s. We could then check if the entirety of the interval is greater than 0, suggesting that \\(\\mu_{SC} - \\mu_{DC} &gt; 0\\), or, in other words suggesting that \\(\\mu_{SC} &gt; \\mu_{DC}\\). Second, we could perform a hypothesis test of the null hypothesis \\(H_0: \\mu_{SC} - \\mu_{DC} = 0\\) versus the alternative hypothesis \\(H_A: \\mu_{SC} - \\mu_{DC} &gt; 0\\).\nHowever, let’s first construct an exploratory visualization as we suggested earlier. Since pts is numerical and player_name is categorical, a boxplot can display the relationship between these two variables, which we display in Figure 10.17.\n\nscdc %&gt;%\n    ggplot(mapping = aes(x = player_name, y = pts)) +\n    geom_boxplot() +\n    labs(x = \"Player\", y = \"Points per Game\")\n\n\n\n\nFigure 10.17: Air time for Hawaiian and Alaska Airlines flights departing NYC in 2013.\n\n\n\n\nThis is what we like to call “no PhD in Statistics needed” moments. You don’t have to be an expert in statistics to know that Alaska Airlines and Hawaiian Airlines have significantly different air times. The two boxplots don’t even overlap! Constructing a confidence interval or conducting a hypothesis test would frankly not provide much more insight than Figure 10.17.\nLet’s investigate why we observe such a clear cut difference between these two players using data wrangling. Let’s first group by the rows of our data, not only by carrier but also by destination dest. Subsequently, we’ll compute two summary statistics: the number of observations using n() and the mean airtime:\n\nscdc %&gt;% \n  group_by(player_name) %&gt;% \n  summarize(n = n(), mean_pts = mean(pts))\n\n# A tibble: 2 × 3\n  player_name       n mean_pts\n  &lt;chr&gt;         &lt;int&gt;    &lt;dbl&gt;\n1 Dell Curry        6     9.05\n2 Stephen Curry    13    23.8 \n\n\nIt turns out that we only have a few seasons Dell Curry and his scoring is consistently low. Given the clear difference between the two players, it will not be surprising when we observe a difference (statistically significantly different, in fact) between these two players’ pts.\nThis is a clear example of not needing to do anything more than a simple exploratory data analysis using data visualization and descriptive statistics to get an appropriate conclusion. This is why we highly recommend you perform an EDA of any sample data before running statistical inference methods like confidence intervals and hypothesis tests.\n\n\n10.6.3 Problems with p-values\nOn top of the many common misunderstandings about hypothesis testing and \\(p\\)-values we listed in Section 10.4), another unfortunate consequence of the expanded use of \\(p\\)-values and hypothesis testing is a phenomenon known as “p-hacking.” p-hacking is the act of “cherry-picking” only results that are “statistically significant” while dismissing those that aren’t, even if at the expense of the scientific ideas. There are lots of articles written recently about misunderstandings and the problems with \\(p\\)-values. We encourage you to check some of them out:\n\nMisunderstandings of \\(p\\)-values\nWhat a nerdy debate about \\(p\\)-values shows about science - and how to fix it\nStatisticians issue warning over misuse of \\(P\\) values\nYou Can’t Trust What You Read About Nutrition\nA Litany of Problems with p-values\n\nSuch issues were getting so problematic that the American Statistical Association (ASA) put out a statement in 2016 titled, “The ASA Statement on Statistical Significance and \\(P\\)-Values,” with six principles underlying the proper use and interpretation of \\(p\\)-values. The ASA released this guidance on \\(p\\)-values to improve the conduct and interpretation of quantitative science and to inform the growing emphasis on reproducibility of science research."
  },
  {
    "objectID": "sampling.html#sec-sampling-activity",
    "href": "sampling.html#sec-sampling-activity",
    "title": "11  Sampling",
    "section": "11.1 Sampling bowl activity",
    "text": "11.1 Sampling bowl activity\nLet’s start with a hands-on activity.\n\n11.1.1 What proportion of this bowl’s balls are red?\nTake a look at the bowl in Figure 11.1. It has a certain number of red and a certain number of white balls all of equal size. Furthermore, it appears the bowl has been mixed beforehand, as there does not seem to be any coherent pattern to the spatial distribution of the red and white balls.\nLet’s now ask ourselves, what proportion of this bowl’s balls are red?\n\n\n\nFigure 11.1: A bowl with red and white balls.\n\n\nOne way to answer this question would be to perform an exhaustive count: remove each ball individually, count the number of red balls and the number of white balls, and divide the number of red balls by the total number of balls. However, this would be a long and tedious process.\n\n\n11.1.2 Using the shovel once\nInstead of performing an exhaustive count, let’s insert a shovel into the bowl as seen in Figure 11.2. Using the shovel, let’s remove \\(5 \\cdot 10 = 50\\) balls, as seen in Figure 11.3.\n\n\n\nFigure 11.2: Inserting a shovel into the bowl.\n\n\n\n\n\nFigure 11.3: Removing 50 balls from the bowl.\n\n\nObserve that 17 of the balls are red and thus 0.34 = 34% of the shovel’s balls are red. We can view the proportion of balls that are red in this shovel as a guess of the proportion of balls that are red in the entire bowl. While not as exact as doing an exhaustive count of all the balls in the bowl, our guess of 34% took much less time and energy to make.\nHowever, say, we started this activity over from the beginning. In other words, we replace the 50 balls back into the bowl and start over. Would we remove exactly 17 red balls again? In other words, would our guess at the proportion of the bowl’s balls that are red be exactly 34% again? Maybe?\nWhat if we repeated this activity several times following the process shown in Figure 11.3? Would we obtain exactly 17 red balls each time? In other words, would our guess at the proportion of the bowl’s balls that are red be exactly 34% every time? Surely not. Let’s repeat this exercise several times with the help of 33 groups of friends to understand how the value differs with repetition.\n\n\n11.1.3 Using the shovel 33 times\nEach of our 33 groups of friends will do the following:\n\nUse the shovel to remove 50 balls each.\nCount the number of red balls and thus compute the proportion of the 50 balls that are red.\nReturn the balls into the bowl.\nMix the contents of the bowl a little to not let a previous group’s results influence the next group’s.\n\nEach of our 33 groups of friends make note of their proportion of red balls from their sample collected. Each group then notes the proportion of their 50 balls that were red. If we entered each group’s measurements into a dta frame, it would look something like this:\n\n\nRows: 33 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): group\ndbl (3): replicate, red_balls, prop_red\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 33 × 4\n   group            replicate red_balls prop_red\n   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 Ilyas, Yohan             1        21     0.42\n 2 Morgan, Terrance         2        17     0.34\n 3 Martin, Thomas           3        21     0.42\n 4 Clark, Frank             4        21     0.42\n 5 Riddhi, Karina           5        18     0.36\n 6 Andrew, Tyler            6        19     0.38\n 7 Julia                    7        19     0.38\n 8 Rachel, Lauren           8        11     0.22\n 9 Daniel, Caroline         9        15     0.3 \n10 Josh, Maeve             10        17     0.34\n# ℹ 23 more rows\n\n\nObserve for each group that we have their names, the number of red_balls they obtained, and the corresponding proportion out of 50 balls that were red named prop_red. We also have a replicate variable enumerating each of the 33 groups. We chose this name because each row can be viewed as one instance of a replicated (in other words repeated) activity: using the shovel to remove 50 balls and computing the proportion of those balls that are red.\nFigure 11.4 visualizes the distribution of these 33 proportions using a histogram (see Section 8.5 for a refresher on histograms).\n\n\n\n\n\nFigure 11.4: ?(caption)\n\n\n\n\nObserve the following:\n\nAt the low end, two groups removed 50 balls from the bowl with proportion red between 0.20 and 0.25.\nAt the high end, a single group removed 50 balls from the bowl with proportion between 0.45 and 0.5 red.\nHowever, the most frequently occurring proportions are between 0.30 and 0.35 red, right in the middle of the distribution.\nThe shape of this distribution is approximately bell-shaped.\n\n\n\n11.1.4 What did we just do?\nWhat we just demonstrated in this activity is the statistical concept of sampling. We would like to know the proportion of the bowl’s balls that are red. Because the bowl has a large number of balls, performing an exhaustive count of the red and white balls would be time-consuming. We thus extracted a sample of 50 balls using the shovel to make an estimate. Using this sample of 50 balls, we estimated the proportion of the bowl’s balls that are red to be 34%.\nMoreover, because we mixed the balls before each use of the shovel, the samples were randomly drawn. Because each sample was drawn at random, the samples were different from each other. Because the samples were different from each other, we obtained the different proportions red observed in Figure 11.4. This is known as the concept of sampling variation.\nThe purpose of this sampling activity was to develop an understanding of two key concepts relating to sampling:\n\nUnderstanding the effect of sampling variation.\nUnderstanding the effect of sample size on sampling variation."
  },
  {
    "objectID": "sampling.html#sec-sampling-simulation",
    "href": "sampling.html#sec-sampling-simulation",
    "title": "11  Sampling",
    "section": "11.2 Virtual sampling",
    "text": "11.2 Virtual sampling\nIn the exercise above, we performed a tactile sampling activity by hand. In other words, we used a physical bowl of balls and a physical shovel. We performed this sampling activity by hand first so that we could develop a firm understanding of the root ideas behind sampling. In this section, we’ll mimic this tactile sampling activity with a virtual sampling activity using a computer. In other words, we’ll use a virtual analog to the bowl of balls and a virtual analog to the shovel.\n\n11.2.1 Using the virtual shovel once\nLet’s start by performing the virtual analog of the tactile sampling exercise. We first need a virtual analog of the bowl seen in Figure 11.1. To this end, we have created a CSV file named bowl.csv The rows of bowl correspond exactly with the contents of the actual bowl.\n\nbowl&lt;-read_csv(\"data/bowl.csv\")\n\nRows: 2400 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): color\ndbl (1): ball_ID\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(bowl)\n\n# A tibble: 6 × 2\n  ball_ID color\n    &lt;dbl&gt; &lt;chr&gt;\n1       1 white\n2       2 white\n3       3 white\n4       4 red  \n5       5 white\n6       6 white\n\n\nObserve that bowl has 2400 rows, telling us that the bowl contains 2400 equally sized balls. The first variable ball_ID is used as an identification variable; none of the balls in the actual bowl are marked with numbers. The second variable color indicates whether a particular virtual ball is red or white.\nNow that we have a virtual analog of our bowl, we next need a virtual analog to the shovel seen in Figure 11.2 to generate virtual samples of 50 balls. We’re going to put together a function called rep_sample_n() (a very similar function is available in the infer package). This function allows us to take repeated, or replicated, samples of size n.\n\nrep_sample_n &lt;- function(tbl,\n                         size,\n                         replace = FALSE,\n                         reps = 1) {\n    n &lt;- nrow(tbl)\n    i &lt;- unlist(replicate(reps,\n                          sample.int(n, size, replace = replace),\n                          simplify = FALSE))\n    \n    rep_tbl &lt;- cbind(replicate = rep(1:reps,\n                                     rep(size, reps)),\n                     tbl[i, ])\n    \n    group_by(rep_tbl, replicate)\n}\n\nLet’s show an example of this function in action. Let’s first use the tibble() function to manually create a data frame of five fruit called fruit_basket.\n\nfruit_basket &lt;- tibble(\n  fruit = c(\"Mango\", \"Tangerine\", \"Apricot\", \"Pamplemousse\", \"Lime\")\n)\n\nNow let’s try out rep_sample_n() by asking for a single sample from our fruit basket.\n\nrep_sample_n(fruit_basket, size=3)\n\n# A tibble: 3 × 2\n# Groups:   replicate [1]\n  replicate fruit       \n      &lt;int&gt; &lt;chr&gt;       \n1         1 Apricot     \n2         1 Tangerine   \n3         1 Pamplemousse\n\n\nNow let’s try asking for 4 different samples.\n\nrep_sample_n(fruit_basket, size=3, reps=4)\n\n# A tibble: 12 × 2\n# Groups:   replicate [4]\n   replicate fruit       \n       &lt;int&gt; &lt;chr&gt;       \n 1         1 Apricot     \n 2         1 Tangerine   \n 3         1 Pamplemousse\n 4         2 Tangerine   \n 5         2 Mango       \n 6         2 Pamplemousse\n 7         3 Apricot     \n 8         3 Mango       \n 9         3 Tangerine   \n10         4 Apricot     \n11         4 Mango       \n12         4 Lime        \n\n\nNote that the replicate variable indicates which “use” of our virtual shovel that row corresponds to. So rows where replicate=1 correspond to our first “scoop” of fruit, rows where replicate=12 correspond to our second “scoop” of fruit, and so on.\nNow let’s return to our bowl and take a single scoop.\n\nvirtual_shovel &lt;- bowl %&gt;% \n  rep_sample_n(size = 50)\nvirtual_shovel\n\n# A tibble: 50 × 3\n# Groups:   replicate [1]\n   replicate ball_ID color\n       &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;\n 1         1    1255 red  \n 2         1     326 white\n 3         1     875 white\n 4         1    1553 white\n 5         1     383 red  \n 6         1    1203 red  \n 7         1     750 white\n 8         1     486 white\n 9         1    1221 white\n10         1    1870 white\n# ℹ 40 more rows\n\n\nObserve that virtual_shovel has 50 rows corresponding to our virtual sample of size 50. The ball_ID variable identifies which of the 2400 balls from bowl are included in our sample of 50 balls while color denotes its color, and replicate again indicates which “scoop” the row came from.\nLet’s compute the proportion of balls in our virtual sample that are red using the dplyr data wrangling verbs you learned in Chapter 7 First, for each of our 50 sampled balls, let’s identify if it is red or not using a test for equality with ==. Let’s create a new Boolean variable is_red using the mutate() function from Section 7.6:\n\nvirtual_shovel %&gt;% \n  mutate(is_red = (color == \"red\"))\n\n# A tibble: 50 × 4\n# Groups:   replicate [1]\n   replicate ball_ID color is_red\n       &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; \n 1         1    1255 red   TRUE  \n 2         1     326 white FALSE \n 3         1     875 white FALSE \n 4         1    1553 white FALSE \n 5         1     383 red   TRUE  \n 6         1    1203 red   TRUE  \n 7         1     750 white FALSE \n 8         1     486 white FALSE \n 9         1    1221 white FALSE \n10         1    1870 white FALSE \n# ℹ 40 more rows\n\n\nObserve that for every row where color == \"red\", the Boolean (logical) value TRUE is returned and for every row where color is not equal to \"red\", the Boolean FALSE is returned.\nSecond, let’s compute the number of balls out of 50 that are red using the summarize() function. Recall from Section 7.4 that summarize() takes a data frame with many rows and returns a data frame with a single row containing summary statistics, like the mean() or median(). In this case, we use the sum():\n\nvirtual_shovel %&gt;% \n  mutate(is_red = (color == \"red\")) %&gt;% \n  summarize(num_red = sum(is_red))\n\n# A tibble: 1 × 2\n  replicate num_red\n      &lt;int&gt;   &lt;int&gt;\n1         1      19\n\n\nWhy does this work? Because R treats TRUE like the number 1 and FALSE like the number 0. So summing the number of TRUEs and FALSEs is equivalent to summing 1’s and 0’s. In the end, this operation counts the number of balls where color is red. In our case, 19 of the 50 balls were red. However, you might have gotten a different number red because of the randomness of the virtual sampling.\nThird and lastly, let’s compute the proportion of the 50 sampled balls that are red by dividing num_red by 50:\n\nvirtual_shovel %&gt;% \n  mutate(is_red = color == \"red\") %&gt;% \n  summarize(num_red = sum(is_red)) %&gt;% \n  mutate(prop_red = num_red / 50)\n\n# A tibble: 1 × 3\n  replicate num_red prop_red\n      &lt;int&gt;   &lt;int&gt;    &lt;dbl&gt;\n1         1      19     0.38\n\n\nIn other words, 38% of this virtual sample’s balls were red. Let’s make this code a little more compact and succinct by combining the first mutate() and the summarize() as follows:\n\nvirtual_shovel %&gt;% \n  summarize(num_red = sum(color == \"red\")) %&gt;% \n  mutate(prop_red = num_red / 50)\n\n# A tibble: 1 × 3\n  replicate num_red prop_red\n      &lt;int&gt;   &lt;int&gt;    &lt;dbl&gt;\n1         1      19     0.38\n\n\nGreat! 38% of virtual_shovel’s 50 balls were red! So based on this particular sample of 50 balls, our guess at the proportion of the bowl’s balls that are red is 38%. But remember from our earlier tactile sampling activity that if we repeat this sampling, we will not necessarily obtain the same value of 38% again. There will likely be some variation. In fact, our 33 groups of friends computed 33 such proportions whose distribution we visualized in Figure 11.4. We saw that these estimates varied. Let’s now perform the virtual analog of having 33 groups of students use the sampling shovel!\n\n\n11.2.2 Using the virtual shovel 33 times\nRecall that in our tactile sampling exercise in Section 11.1, we had 33 groups of students each use the shovel, yielding 33 samples of size 50 balls. We then used these 33 samples to compute 33 proportions. In other words, we repeated/replicated using the shovel 33 times. We can perform this repeated/replicated sampling virtually by once again using our virtual shovel function rep_sample_n(), but by adding the reps = 33 argument. This is telling R that we want to repeat the sampling 33 times.\nWe’ll save these results in a data frame called virtual_samples. Though we provide a preview of the first 10 rows of virtual_samples in what follows, we highly suggest you scroll through its contents using RStudio’s spreadsheet viewer by running View(virtual_samples).\n\nvirtual_samples &lt;- bowl %&gt;% \n  rep_sample_n(size = 50, reps = 33)\nvirtual_samples\n\n# A tibble: 1,650 × 3\n# Groups:   replicate [33]\n   replicate ball_ID color\n       &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;\n 1         1    1873 red  \n 2         1    1313 red  \n 3         1    1023 white\n 4         1    1085 white\n 5         1     102 red  \n 6         1    1983 white\n 7         1     855 red  \n 8         1     554 red  \n 9         1      81 white\n10         1    1566 white\n# ℹ 1,640 more rows\n\n\nObserve in the spreadsheet viewer that the first 50 rows of replicate are equal to 1 while the next 50 rows of replicate are equal to 2. This is telling us that the first 50 rows correspond to the first sample of 50 balls while the next 50 rows correspond to the second sample of 50 balls. This pattern continues for all reps = 33 replicates and thus virtual_samples has 33 \\(\\cdot\\) 50 = 1650 rows.\nLet’s now take virtual_samples and compute the resulting 33 proportions red. We’ll use the same dplyr verbs as before, but this time with an additional group_by() of the replicate variable. Recall from Section Section 7.5 that by assigning the grouping variable “meta-data” before we summarize(), we’ll obtain 33 different proportions red. We display a preview of the first 10 out of 33 rows:\n\nvirtual_prop_red &lt;- virtual_samples %&gt;% \n  group_by(replicate) %&gt;% \n  summarize(red = sum(color == \"red\")) %&gt;% \n  mutate(prop_red = red / 50)\nvirtual_prop_red\n\n# A tibble: 33 × 3\n   replicate   red prop_red\n       &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;\n 1         1    20     0.4 \n 2         2    24     0.48\n 3         3    17     0.34\n 4         4    23     0.46\n 5         5    20     0.4 \n 6         6    17     0.34\n 7         7    24     0.48\n 8         8    18     0.36\n 9         9    22     0.44\n10        10    17     0.34\n# ℹ 23 more rows\n\n\nAs with our 33 groups of friends’ tactile samples, there is variation in the resulting 33 virtual proportions red. Let’s visualize this variation in a histogram in Figure 11.5. Note that we add binwidth = 0.05 and boundary = 0.4 arguments as well. Recall that setting boundary = 0.4 ensures a binning scheme with one of the bins’ boundaries at 0.4. Since the binwidth = 0.05 is also set, this will create bins with boundaries at 0.30, 0.35, 0.45, 0.5, etc. as well.\n\n\n\n\n\nFigure 11.5: Distribution of 33 proportions based on 33 samples of size 50.\n\n\n\n\nObserve that we occasionally obtained proportions red that are less than 30%. On the other hand, we occasionally obtained proportions that are greater than 45%. However, the most frequently occurring proportions were between 35% and 40%. Why do we have these differences in proportions red? Because of sampling variation.\nLet’s now compare our virtual results with our tactile results from the previous section in Figure 11.6. Observe that both histograms are somewhat similar in their center and variation, although not identical. These slight differences are again due to random sampling variation. Furthermore, observe that both distributions are somewhat bell-shaped.\n\nbind_rows(\n    virtual_prop_red %&gt;%\n        mutate(type = \"Virtual sampling\"),\n    tactile_prop_red %&gt;%\n        select(replicate, red = red_balls, prop_red) %&gt;%\n        mutate(type = \"Tactile sampling\")\n) %&gt;%\n    mutate(type = factor(type, levels = c(\"Virtual sampling\", \"Tactile sampling\"))) %&gt;%\n    ggplot(aes(x = prop_red)) +\n    geom_histogram(binwidth = 0.05,\n                   boundary = 0.4,\n                   color = \"white\") +\n    facet_wrap( ~ type) +\n    labs(x = \"Proportion of 50 balls that were red\",\n         title = \"Comparing distributions\")\n\n\n\n\nFigure 11.6: Comparing 33 virtual and 33 tactile proportions red.\n\n\n\n\n\n\n11.2.3 Using the virtual shovel 1000 times\nNow say we want to study the effects of sampling variation not for 33 samples, but rather for a larger number of samples, say 1000. We have two choices at this point. We could have our groups of friends manually take 1000 samples of 50 balls and compute the corresponding 1000 proportions. However, this would be a tedious and time-consuming task. This is where computers excel: automating long and repetitive tasks while performing them quite quickly. Thus, at this point we will abandon tactile sampling in favor of only virtual sampling. Let’s once again use the rep_sample_n() function with sample size set to be 50 once again, but this time with the number of replicates reps set to 1000. Be sure to scroll through the contents of virtual_samples in RStudio’s viewer.\n\nvirtual_samples &lt;- bowl %&gt;% \n  rep_sample_n(size = 50, reps = 1000)\nvirtual_samples\n\n# A tibble: 50,000 × 3\n# Groups:   replicate [1,000]\n   replicate ball_ID color\n       &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;\n 1         1    2071 red  \n 2         1     830 red  \n 3         1      52 red  \n 4         1     511 white\n 5         1     580 white\n 6         1     418 red  \n 7         1    1038 red  \n 8         1     701 red  \n 9         1    1405 red  \n10         1    2045 white\n# ℹ 49,990 more rows\n\n\nObserve that now virtual_samples has 1000 \\(\\cdot\\) 50 = 50,000 rows, instead of the 33 \\(\\cdot\\) 50 = 1650 rows from earlier. Using the same data wrangling code as earlier, let’s take the data frame virtual_samples with 1000 \\(\\cdot\\) 50 = 50,000 rows and compute the resulting 1000 proportions of red balls.\n\nvirtual_prop_red &lt;- virtual_samples %&gt;% \n  group_by(replicate) %&gt;% \n  summarize(red = sum(color == \"red\")) %&gt;% \n  mutate(prop_red = red / 50)\nvirtual_prop_red\n\n# A tibble: 1,000 × 3\n   replicate   red prop_red\n       &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;\n 1         1    24     0.48\n 2         2    17     0.34\n 3         3    20     0.4 \n 4         4    13     0.26\n 5         5    21     0.42\n 6         6    24     0.48\n 7         7    20     0.4 \n 8         8    22     0.44\n 9         9    19     0.38\n10        10    23     0.46\n# ℹ 990 more rows\n\n\nObserve that we now have 1000 replicates of prop_red, the proportion of 50 balls that are red. Using the same code as earlier, let’s now visualize the distribution of these 1000 replicates of prop_red in a histogram in Figure 11.7.\n\nggplot(virtual_prop_red, aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of 50 balls that were red\", \n       title = \"Distribution of 1000 proportions red\") \n\n\n\n\nFigure 11.7: Distribution of 1000 proportions based on 1000 samples of size 50.\n\n\n\n\nOnce again, the most frequently occurring proportions of red balls occur between 35% and 40%. Every now and then, we obtain proportions as less than 25% and others greater than 55%. These are rare, however. Furthermore, observe that we now have a much more symmetric and smoother bell-shaped distribution. This distribution is, in fact, well approximated by a normal distribution.\n\n\n11.2.4 Using different shovels\nNow say instead of just one shovel, you have three choices of shovels to extract a sample of balls with: shovels of size 25, 50, and 100.\n\n\n\nFigure 11.8: Three shovels to extract three different sample sizes.\n\n\nIf your goal is still to estimate the proportion of the bowl’s balls that are red, which shovel would you choose? In our experience, most people would choose the largest shovel with 100 slots because it would yield the “best” guess of the proportion of the bowl’s balls that are red. Let’s define some criteria for “best” in this subsection.\nUsing our newly developed tools for virtual sampling, let’s unpack the effect of having different sample sizes! In other words, let’s use rep_sample_n() with size set to 25, 50, and 100, respectively, while keeping the number of repeated/replicated samples at 1000:\n\nVirtually use the appropriate shovel to generate 1000 samples with size balls.\nCompute the resulting 1000 replicates of the proportion of the shovel’s balls that are red.\nVisualize the distribution of these 1000 proportions red using a histogram.\n\nRun each of the following code segments individually and then compare the three resulting histograms.\n\n# Segment 1: sample size = 25 ------------------------------\n# 1.a) Virtually use shovel 1000 times\nvirtual_samples_25 &lt;- bowl %&gt;% \n  rep_sample_n(size = 25, reps = 1000)\n\n# 1.b) Compute resulting 1000 replicates of proportion red\nvirtual_prop_red_25 &lt;- virtual_samples_25 %&gt;% \n  group_by(replicate) %&gt;% \n  summarize(red = sum(color == \"red\")) %&gt;% \n  mutate(prop_red = red / 25)\n\n# 1.c) Plot distribution via a histogram\nggplot(virtual_prop_red_25, aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of 25 balls that were red\", title = \"25\") \n\n\n# Segment 2: sample size = 50 ------------------------------\n# 2.a) Virtually use shovel 1000 times\nvirtual_samples_50 &lt;- bowl %&gt;% \n  rep_sample_n(size = 50, reps = 1000)\n\n# 2.b) Compute resulting 1000 replicates of proportion red\nvirtual_prop_red_50 &lt;- virtual_samples_50 %&gt;% \n  group_by(replicate) %&gt;% \n  summarize(red = sum(color == \"red\")) %&gt;% \n  mutate(prop_red = red / 50)\n\n# 2.c) Plot distribution via a histogram\nggplot(virtual_prop_red_50, aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of 50 balls that were red\", title = \"50\")  \n\n\n# Segment 3: sample size = 100 ------------------------------\n# 3.a) Virtually using shovel with 100 slots 1000 times\nvirtual_samples_100 &lt;- bowl %&gt;% \n  rep_sample_n(size = 100, reps = 1000)\n\n# 3.b) Compute resulting 1000 replicates of proportion red\nvirtual_prop_red_100 &lt;- virtual_samples_100 %&gt;% \n  group_by(replicate) %&gt;% \n  summarize(red = sum(color == \"red\")) %&gt;% \n  mutate(prop_red = red / 100)\n\n# 3.c) Plot distribution via a histogram\nggplot(virtual_prop_red_100, aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of 100 balls that were red\", title = \"100\") \n\nFor easy comparison, we present the three resulting histograms in a single row with matching x and y axes in Figure 11.9.\n\n\n\n\n\nFigure 11.9: Comparing the distributions of proportion red for different sample sizes.\n\n\n\n\nObserve that as the sample size increases, the variation of the 1000 replicates of the proportion of red decreases. In other words, as the sample size increases, there are fewer differences due to sampling variation and the distribution centers more tightly around the same value. Eyeballing Figure 11.9, all three histograms appear to center around roughly 40%.\nWe can be numerically explicit about the amount of variability in our three sets of 1000 values of prop_red using the standard deviation (Section 9.2.5). For all three sample sizes, let’s compute the standard deviation of the 1000 proportions red by running the following data wrangling code that uses the sd() summary function.\n\n# n = 25\nvirtual_prop_red_25 %&gt;% \n  summarize(sd = sd(prop_red))\n\n# n = 50\nvirtual_prop_red_50 %&gt;% \n  summarize(sd = sd(prop_red))\n\n# n = 100\nvirtual_prop_red_100 %&gt;% \n  summarize(sd = sd(prop_red))\n\nLet’s compare these three measures of distributional variation in Table 11.1.\n\n\n\n\nTable 11.1: Comparing standard deviations of proportions red for three different shovels\n\n\nNumber of slots in shovel\nStandard deviation of proportions red\n\n\n\n\n25\n0.096\n\n\n50\n0.069\n\n\n100\n0.047\n\n\n\n\n\n\n\n\nAs we observed in Figure 11.9, as the sample size increases, the variation decreases. In other words, there is less variation in the 1000 values of the proportion red. So as the sample size increases, our guesses at the true proportion of the bowl’s balls that are red get more precise."
  },
  {
    "objectID": "sampling.html#sec-sampling-framework",
    "href": "sampling.html#sec-sampling-framework",
    "title": "11  Sampling",
    "section": "11.3 Sampling framework",
    "text": "11.3 Sampling framework\nIn both our tactile and our virtual sampling activities, we used sampling for the purpose of estimation. We extracted samples in order to estimate the proportion of the bowl’s balls that are red. We used sampling as a less time-consuming approach than performing an exhaustive count of all the balls. Our virtual sampling activity built up to the results shown in Figure 11.9 and Table 11.1: comparing 1000 proportions red based on samples of size 25, 50, and 100. This was our first attempt at understanding two key concepts relating to sampling for estimation:\n\nThe effect of sampling variation on our estimates.\nThe effect of sample size on sampling variation.\n\nNow that you have built some intuition relating to sampling, let’s now attach words and labels to the various concepts we’ve explored so far. Specifically in the next section, we’ll introduce terminology and notation as well as statistical definitions related to sampling. This will allow us to succinctly summarize and refer to the ideas behind sampling for the rest of this book.\n\n11.3.1 Terminology and notation\nLet’s now attach words and labels to the various sampling concepts we’ve seen so far by introducing some terminology and mathematical notation. While they may seem daunting at first, we’ll make sure to tie each of them to sampling bowl activities you performed earlier. Furthermore, throughout this book we’ll give you plenty of opportunity for practice, as the best method for mastering these terms is repetition.\nThe first set of terms and notation relate to populations:\n\nA population is a collection of individuals or observations we are interested in. This is also commonly denoted as a study population. We mathematically denote the population’s size using upper-case \\(N\\).\n\nA population parameter is some numerical summary about the population that is unknown but you wish you knew. For example, when this quantity is a mean like the average height of all Canadians, the population parameter of interest is the population mean.\nA census is an exhaustive enumeration or counting of all \\(N\\) individuals in the population. We do this in order to compute the population parameter’s value exactly. Of note is that as the number \\(N\\) of individuals in our population increases, conducting a census gets more expensive (in terms of time, energy, and money).\n\nSo in our sampling activities, the population is the collection of \\(N\\) = 2400 identically sized red and white balls in the bowl shown in Figure 11.1. Recall that we also represented the bowl “virtually” in the data frame bowl:\n\nhead(bowl)\n\n# A tibble: 6 × 2\n  ball_ID color\n    &lt;dbl&gt; &lt;chr&gt;\n1       1 white\n2       2 white\n3       3 white\n4       4 red  \n5       5 white\n6       6 white\n\n\nThe population parameter here is the proportion of the bowl’s balls that are red. Whenever we’re interested in a proportion of some value in a population, the population parameter has a specific name: the population proportion. We denote population proportions with the letter \\(p\\). We’ll see later on that we can also consider other types of population parameters, like population means and population regression slopes.\nIn order to compute this population proportion \\(p\\) exactly, we need to first conduct a census by going through all \\(N\\) = 2400 and counting the number that are red. We then divide this count by 2400 to obtain the proportion red.\nYou might be now asking yourself: “Wait. I understand that performing a census on the actual bowl would take a long time. But can’t we conduct a ‘virtual’ census using the virtual bowl?” You are absolutely correct! In fact when the authors of this book created the bowl data frame, they made its contents match the contents of actual bowl not by doing a census, but by reading the contents written on the box the bowl came in!\nLet’s conduct this “virtual” census by using the same dplyr verbs you used earlier to count the number of balls that are red:\n\nbowl %&gt;% \n  summarize(red = sum(color == \"red\")) \n\n# A tibble: 1 × 1\n    red\n  &lt;int&gt;\n1   900\n\n\nSince 900 of the 2400 are red, the proportion is 900/2400 = 0.375 = 37.5. So we know the value of the population parameter: in our case, the population proportion \\(p\\) is equal to 0.375.\nAt this point, you might be further asking yourself: “If we had a way of knowing that the proportion of the balls that are red is 37.5, then why did we do any sampling?” Great question! Normally, you wouldn’t do any sampling! However, the sampling activities we did this chapter are merely simulations of how sampling is done in real-life! We perform these simulations in order to study:\n\nThe effect of sampling variation on our estimates.\nThe effect of sample size on sampling variation.\n\nIn real-life sampling, not only will the population size \\(N\\) be very large making a census expensive, but sometimes we won’t even know how big the population is! For now however, we press on with our next set of terms and notation.\nThe second set of terms and notation relate to samples:\n\nSampling is the act of collecting a sample from the population, which we generally only do when we can’t perform a census. We mathematically denote the sample size using lower case \\(n\\), as opposed to upper case \\(N\\) which denotes the population’s size. Typically the sample size \\(n\\) is much smaller than the population size \\(N\\). Thus sampling is a much cheaper alternative than performing a census.\nA point estimate, also known as a sample statistic, is a summary statistic computed from a sample that estimates the unknown population parameter.\n\nSo previously we conducted sampling using a shovel with 50 slots to extract samples of size \\(n\\) = 50. To perform the virtual analog of this sampling, recall that we used the rep_sample_n() function as follows:\n\nvirtual_shovel &lt;- bowl %&gt;% \n  rep_sample_n(size = 50)\nvirtual_shovel\n\n# A tibble: 50 × 3\n# Groups:   replicate [1]\n   replicate ball_ID color\n       &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;\n 1         1    1061 red  \n 2         1     174 red  \n 3         1    1185 red  \n 4         1    1400 white\n 5         1    1039 white\n 6         1     219 white\n 7         1    1715 white\n 8         1    2226 red  \n 9         1      52 red  \n10         1     214 white\n# ℹ 40 more rows\n\n\nUsing the sample of 50 balls contained in virtual_shovel, we generated an estimate of the proportion of the bowl’s balls that are red prop_red\n\nvirtual_shovel %&gt;% \n  summarize(num_red = sum(color == \"red\")) %&gt;% \n  mutate(prop_red = num_red / 50)\n\n# A tibble: 1 × 3\n  replicate num_red prop_red\n      &lt;int&gt;   &lt;int&gt;    &lt;dbl&gt;\n1         1      23     0.46\n\n\nSo in our case, the value of prop_red is the point estimate of the population proportion \\(p\\) since it estimates the latter’s value. Furthermore, this point estimate has a specific name when considering proportions: the sample proportion. It is denoted using \\(\\widehat{p}\\) because it is a common convention in statistics to use a “hat” symbol to denote point estimates.\nThe third set of terms relate to sampling methodology: the method used to collect samples. You’ll see here and throughout the rest of your book that the way you collect samples directly influences their quality.\n\nA sample is said to be representative if it roughly “looks like” the population. In other words, if the sample’s characteristics are a “good” representation of the population’s characteristics.\nWe say a sample is generalizable if any results based on the sample can generalize to the population. In other words, if we can make “good” guesses about the population using the sample.\nWe say a sampling procedure is biased if certain individuals in a population have a higher chance of being included in a sample than others. We say a sampling procedure is unbiased if every individual in a population has an equal chance of being sampled.\n\nWe say a sample of \\(n\\) balls extracted using our shovel is representative of the population if it’s contents “roughly resemble” the contents of the bowl. If so, then the proportion of the shovel’s balls that are red can generalize to the proportion of the bowl’s \\(N\\) = 2400 balls that are red. Or expressed differently, \\(\\widehat{p}\\) is a “good guess” of \\(p\\). Now say we cheated when using the shovel and removed a number of white balls in favor of red balls. Then this sample would be biased towards red balls, and thus the sample would no longer be representative of the bowl.\nThe fourth and final set of terms and notation relate to the goal of sampling:\n\nOne way to ensure that a sample is unbiased and representative of the population is by using random sampling\nInference is the act of “making a guess” about some unknown. Statistical inference is the act of making a guess about a population using a sample.\n\nIn our case, since the rep_sample_n() function uses your computer’s random number generator, we were in fact performing random sampling.\nLet’s now put all four sets of terms and notation together, keeping our sampling activities in mind:\n\nSince we extracted a sample of \\(n\\) = 50 balls at random, we mixed all of the equally sized balls before using the shovel, then\nthe contents of the shovel are unbiased and representative of the contents of the bowl, thus\nany result based on the shovel can generalize to the bowl, thus\nthe sample proportion \\(\\widehat{p}\\) of the \\(n\\) = 50 balls in the shovel that are red is a “good guess” of the population proportion \\(p\\) of the bowl’s \\(N\\) = 2400 balls that are red, thus\ninstead of conducting a census of the 2400 balls in the bowl, we can infer about the bowl using the sample from the shovel.\n\nWhat you have been performing is statistical inference. This is one of the most important concepts in all of statistics. So much so, we included this term in the title of our book: “Statistical Inference via Data Science”. More generally speaking,\n\nIf the sampling of a sample of size \\(n\\) is done at random, then\nthe sample is unbiased and representative of the population of size \\(N\\), thus\nany result based on the sample can generalize to the population, thus\nthe point estimate is a “good guess” of the unknown population parameter, thus\ninstead of performing a census, we can infer about the population using sampling.\n\n\n\n11.3.2 Statistical definitions\nTo further attach words and labels to the various sampling concepts we’ve seen so far, we also introduce some important statistical definitions related to sampling. As a refresher of our 1000 repeated/replicated virtual samples of size \\(n\\) = 25, \\(n\\) = 50, and \\(n\\) = 100 in Section Section 11.2, let’s display Figure 11.9 again as Figure 11.10.\n\n\n\n\n\nFigure 11.10: Previously seen three distributions of the sample proportion \\(\\widehat{p}\\).\n\n\n\n\nThese types of distributions have a special name: sampling distributions of point estimates. Their visualization displays the effect of sampling variation on the distribution of any point estimate, in this case, the sample proportion \\(\\widehat{p}\\). Using these sampling distributions, for a given sample size \\(n\\), we can make statements about what values we can typically expect. Unfortunately, the term sampling distribution is often confused with a sample’s distribution which is merely the distribution of the values in a single sample.\nFor example, observe the centers of all three sampling distributions: they are all roughly centered around 0.4 = 40%. Furthermore, observe that while we are somewhat likely to observe sample proportions of red balls of 0.2 = 20% when using the shovel with 25 slots, we will almost never observe a proportion of 20% when using the shovel with 100 slots. Observe also the effect of sample size on the sampling variation. As the sample size \\(n\\) increases from 25 to 50 to 100, the variation of the sampling distribution decreases and thus the values cluster more and more tightly around the same center of around 40%. We quantified this variation using the standard deviation of our sample proportions in Table 11.1, which we display again as Table 11.2:\n\n\n\n\nTable 11.2: Previously seen comparing standard deviations of proportions red for three different shovels\n\n\nNumber of slots in shovel\nStandard deviation of proportions red\n\n\n\n\n25\n0.096\n\n\n50\n0.069\n\n\n100\n0.047\n\n\n\n\n\n\n\n\nSo as the sample size increases, the standard deviation of the proportion of red balls decreases. This type of standard deviation has another special name: standard error of a point estimate. Standard errors quantify the effect of sampling variation induced on our estimates. In other words, they quantify how much we can expect different proportions of a shovel’s balls that are red to vary from one sample to another sample to another sample, and so on. As a general rule, as sample size increases, the standard error decreases.\nSimilarly to confusion between sampling distributions with a sample’s distribution, people often confuse the standard error with the standard deviation. This is especially the case since a standard error is itself a kind of standard deviation. The best advice we can give is that a standard error is merely a kind of standard deviation: the standard deviation of any point estimate from sampling. In other words, all standard errors are standard deviations, but not every standard deviation is necessarily a standard error.\nTo help reinforce these concepts, let’s re-display Figure 11.9 but using our new terminology, notation, and definitions relating to sampling in Figure 11.11.\n\n\n\n\n\nFigure 11.11: Three sampling distributions of the sample proportion \\(\\widehat{p}\\).\n\n\n\n\nFurthermore, let’s re-display Table 11.1 but using our new terminology, notation, and definitions relating to sampling in Table 11.3.\n\n\n\n\nTable 11.3: Standard errors of the sample proportion based on sample sizes of 25, 50, and 100\n\n\nSample size (n)\nStandard error of $\\widehat{p}$\n\n\n\n\nn = 25\n0.096\n\n\nn = 50\n0.069\n\n\nn = 100\n0.047\n\n\n\n\n\n\n\n\nRemember the key message of this last table: that as the sample size \\(n\\) goes up, the “typical” error of your point estimate will go down, as quantified by the standard error.\n\n\n11.3.3 The moral of the story\nLet’s recap this section so far. We’ve seen that if a sample is generated at random, then the resulting point estimate is a “good guess” of the true unknown population parameter. In our sampling activities, since we made sure to mix the balls first before extracting a sample with the shovel, the resulting sample proportion \\(\\widehat{p}\\) of the shovel’s balls that were red was a “good guess” of the population proportion \\(p\\) of the bowl’s balls that were red.\nHowever, what do we mean by our point estimate being a “good guess”? Sometimes, we’ll get an estimate that is less than the true value of the population parameter, while at other times we’ll get an estimate that is greater. This is due to sampling variation. However, despite this sampling variation, our estimates will “on average” be correct and thus will be centered at the true value. This is because our sampling was done at random and thus in an unbiased fashion.\nIn our sampling activities, sometimes our sample proportion \\(\\widehat{p}\\) was less than the true population proportion \\(p\\), while at other times it was greater. This was due to the sampling variability. However, despite this sampling variation, our sample proportions \\(\\widehat{p}\\) were “on average” correct and thus were centered at the true value of the population proportion \\(p\\). This is because we mixed our bowl before taking samples and thus the sampling was done at random and thus in an unbiased fashion. This is also known as having an accurate estimate.\nRecall from earlier that the value of the population proportion \\(p\\) of the \\(N\\) = 2400 balls in the bowl was 900/2400 = 0.375 = 37.5. We computed this value by performing a virtual census of bowl. Let’s re-display our sampling distributions from Figure 11.9 and Figure 11.11, but now with a vertical red line marking the true population proportion \\(p\\) of balls that are red = 37.5 in Figure 11.12. We see that while there is a certain amount of error in the sample proportions \\(\\widehat{p}\\) for all three sampling distributions, on average the \\(\\widehat{p}\\) are centered at the true population proportion red \\(p\\).\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nFigure 11.12: Three sampling distributions with population proportion \\(p\\) marked by vertical line.\n\n\n\n\nWe also saw in this section that as your sample size \\(n\\) increases, your point estimates will vary less and less and be more and more concentrated around the true population parameter. This variation is quantified by the decreasing standard error. In other words, the typical error of your point estimates will decrease. In our sampling exercise, as the sample size increased, the variation of our sample proportions \\(\\widehat{p}\\) decreased. You can observe this behavior in Figure 11.12. This is also known as having a precise estimate.\nSo random sampling ensures our point estimates are accurate, while on the other hand having a large sample size ensures our point estimates are precise. While the terms “accuracy” and “precision” may sound like they mean the same thing, there is a subtle difference. Accuracy describes how “on target” our estimates are, whereas precision describes how “consistent” our estimates are. Figure 11.13 illustrates the difference.\n\n\n\nFigure 11.13: Comparing accuracy and precision.\n\n\nAt this point, you might be asking yourself: “Why did we take 1000 repeated samples of size n = 25, 50, and 100? Shouldn’t we be taking only one sample that’s as large as possible?”. If you did ask yourself these questions, your suspicion is correct! Recall from earlier when we asked ourselves “If we had a way of knowing that the proportion of the balls that are red is 37.5, then why did we do any sampling?” Similarly, we took 1000 repeated samples as a simulation of how sampling is done in real-life! We used these simulations to study:\n\nThe effect of sampling variation on our estimates.\nThe effect of sample size on sampling variation.\n\nThis is not how sampling is done in real life! In a real-life scenario, we wouldn’t take 1000 repeated/replicated samples, but rather a single sample that’s as large as we can afford."
  },
  {
    "objectID": "sampling.html#sec-sampling-conclusion-central-limit-theorem",
    "href": "sampling.html#sec-sampling-conclusion-central-limit-theorem",
    "title": "11  Sampling",
    "section": "11.4 Central Limit Theorem",
    "text": "11.4 Central Limit Theorem\nThis chapter began with (virtual) access to a large bowl of balls (our population) and a desire to figure out the proportion of red balls. Despite having access to this population, in reality, you almost never will have access to the population, either because the population is too large, ever changing, or too expensive to take a census of. Accepting this reality means accepting that we need to use statistical inference.\nIn Section 11.3.1, we stated that “statistical inference is the act of making a guess about a population using a sample.” But how do we do this inference? In the previous section, we defined the sampling framework only to state that in reality we take one large sample, instead of many samples as done in the sampling framework (which we modeled physically by taking many samples from the bowl).\nIn reality, we take only one sample and use that one sample to make statements about the population parameter. This ability of making statements about the population is allowable by a famous theorem, or mathematically proven truth, called the Central Limit Theorem. What you visualized in Figure 11.9 and Figure 11.11 and summarized in Table 11.1 and Table 11.3 was a demonstration of this theorem. It loosely states that when sample means are based on larger and larger sample sizes, the sampling distribution of these sample means becomes both more and more normally shaped and more and more narrow.\nIn other words, as our sample size gets larger (1) the sampling distribution of a point estimate (like a sample proportion) increasingly follows a normal distribution and (2) the variation of these sampling distributions gets smaller, as quantified by their standard errors.\nHere’s what is so surprising about the Central Limit Theorem: regardless of the shape of the underlying population distribution, the sampling distribution of means (such as the sample mean of bunny weights or the sample mean of the length of dragon wings) and proportions (such as the sample proportion red in our shovels) will be normal. Normal distributions are defined by where they are centered and how wide they are, and the Central Limit Theorem gives us both:\n\nThe sampling distribution of the point estimate is centered at the true population parameter\nWe have an estimate for how wide the sampling distribution of the point estimate is, given by the standard error\n\nWhat the Central Limit Theorem creates for us is a ladder between a single sample and the population. By the Central Limit Theorem, we can say that (1) our sample’s point estimate is drawn from a normal distribution centered at the true population parameter and (2)that the width of that normal distribution is governed by the standard error of our point estimate. Relating this to our bowl, if we pull one sample and get the sample proportion of red balls \\(\\widehat{p}\\), this value of \\(\\widehat{p}\\) is drawn from the normal curve centered at the true population proportion of red balls \\(p\\) with the computed standard error."
  },
  {
    "objectID": "sampling.html#sec-sampling-conclusion",
    "href": "sampling.html#sec-sampling-conclusion",
    "title": "11  Sampling",
    "section": "11.5 Conclusion",
    "text": "11.5 Conclusion\n\n11.5.1 Sampling scenarios\nIn this chapter, we performed both tactile and virtual sampling exercises to infer about an unknown proportion. We also presented a case study of sampling in real life with polls. In each case, we used the sample proportion \\(\\widehat{p}\\) to estimate the population proportion \\(p\\). However, we are not just limited to scenarios related to proportions. In other words, we can use sampling to estimate other population parameters using other point estimates as well. We present four more such scenarios in Table 11.4).\n\n\nRows: 6 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): Population parameter, Notation, Point estimate, Symbol(s)\ndbl (1): Scenario\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nTable 11.4: Scenarios of sampling for inference\n\n\nScenario\nPopulation parameter\nNotation\nPoint estimate\nSymbol(s)\n\n\n\n\n1\nPopulation proportion\n$p$\nSample proportion\n$\\widehat{p}$\n\n\n2\nPopulation mean\n$\\mu$\nSample mean\n$\\overline{x}$ or $\\widehat{\\mu}$\n\n\n3\nDifference in population proportions\n$p_1 - p_2$\nDifference in sample proportions\n$\\widehat{p}_1 - \\widehat{p}_2$\n\n\n4\nDifference in population means\n$\\mu_1 - \\mu_2$\nDifference in sample means\n$\\overline{x}_1 - \\overline{x}_2$ or $\\widehat{\\mu}_1 - \\widehat{\\mu}_2$\n\n\n5\nPopulation regression slope\n$\\beta_1$\nFitted regression slope\n$b_1$ or $\\widehat{\\beta}_1$\n\n\n\n\n\n\n\n\n\n\n11.5.2 Theory-based standard-errors\nThere exists in many cases a formula that approximates the standard error! In the case of our bowl where we used the sample proportion red \\(\\widehat{p}\\) to estimate the proportion of the bowl’s balls that are red, the formula that approximates the standard error for the sample proportion \\(\\widehat{p}\\) is:\n\\[\\text{SE}_{\\widehat{p}} \\approx \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}}\\]\nFor example, say you sampled \\(n = 50\\) balls and observed 21 red balls. This equals a sample proportion \\(\\widehat{p}\\) of 21/50 = 0.42. So, using the formula, an approximation of the standard error of \\(\\widehat{p}\\) is\n\\[\\text{SE}_{\\widehat{p}} \\approx \\sqrt{\\frac{0.42(1-0.42)}{50}} = \\sqrt{0.004872} = 0.0698 \\approx 0.070\\]\nSay instead you sampled \\(n = 100\\) balls and observed 42 red balls. This once again equals a sample proportion \\(\\widehat{p}\\) of 42/100 = 0.42. However using the formula, an approximation of the standard error of \\(\\widehat{p}\\) is now\n\\[\\text{SE}_{\\widehat{p}} \\approx \\sqrt{\\frac{0.42(1-0.42)}{100}} = \\sqrt{0.002436} = 0.0494\\] Observe that the standard error has gone down from 0.0698 to 0.0494. In other words, the “typical” error of our estimates using \\(n\\) = 100 will go down relative to \\(n\\) = 50 and hence be more precise. Recall that we illustrated the difference between accuracy and precision of estimates in Figure 11.13.\nThe key observation to make in the formula is that there is an \\(n\\) in the denominator. As the sample size \\(n\\) increases, the standard error decreases. We’ve demonstrated this fact using our virtual shovels in Section 11.3.3.\nFurthermore, this is one of the key messages of the Central Limit Theorem we saw in Section 11.4: as the sample size \\(n\\) increases, the distribution of averages gets narrower as quantified by the standard deviation of the sampling distribution of the sample mean. This standard deviation of the sampling distribution of the sample means in turn has a special name: the standard error of the sample mean.\nWhy is this formula true? Unfortunately, we don’t have the tools at this point to prove this; you’ll need to take a more advanced course in probability and statistics. (It is related to the concepts of Bernoulli and Binomial Distributions. You can read more about its derivation here if you like.)"
  },
  {
    "objectID": "samplepvalues.html#philosophical-approaches-to-p-values",
    "href": "samplepvalues.html#philosophical-approaches-to-p-values",
    "title": "12  Using p-values to test a hypothesis",
    "section": "12.1 Philosophical approaches to p-values",
    "text": "12.1 Philosophical approaches to p-values\nBefore we look at how p-values are computed, it is important to examine how they are supposed to help us make ordinal claims when testing hypotheses. The definition of a p-value is the probability of observing the sample data, or more extreme data, assuming the null hypothesis is true. But this definition does not tell us much about how we should interpret a p-value.\nThe interpretation of a p-value depends on the statistical philosophy one subscribes to. Ronald Fisher published ‘Statistical Methods for Research Workers’ in 1925 which popularized the use of p-values. In a Fisherian framework a p-value is interpreted as a descriptive continuous measure of compatibility between the observed data and the null hypothesis. According to Fisher (1956), p-values “do not generally lead to any probability statement about the real world, but”a rational and well-defined measure of reluctance to the acceptance of the hypotheses they test” (p. 44). The lower the p-value, the greater the reluctance to accept the null hypothesis. Spanos (2019) refers to the Fisherian approach as misspecification testing, as used when testing model assumptions. In a misspecification test (such as measures of goodness of fit) the null hypothesis is tested against any unspecified alternative hypothesis. Without a specified alternative hypothesis, the p value can only be used to describe the probability of the observed or more extreme data under the null model.\nFisher tried to formalize his philosophy in an approach called ‘fiducial inference’, but this has not received the same widespread adoption of other approaches, such as decision theory, likelihoods, and Bayesian inference. Indeed, Zabell (1992) writes “The fiducial argument stands as Fisher’s one great failure”, although others have expressed the hope that it might be developed into a useful approach in the future (Schweder and Hjort 2016). A Fisherian p-value describes the incompatibility of the data with a single hypothesis, and is known as significance testing. The main reason a significance test is limited is because researchers only specify a null hypothesis (\\(H_0\\)), but not the alternative hypothesis (\\(H_1\\)). According to Spanos (2019), after the model assumptions have been checked, when “testing theoretical restrictions within a statistically adequate model, however, the Neyman–Pearson approach becomes the procedure of choice” (p. xxiii).\nNeyman and Pearson (1933) built on insights about p-values by William Gosset and Ronald Fisher, and developed an approach called statistical hypothesis testing. The main difference with the significance testing approach developed by Fisher was that in a statistical hypothesis test both a null hypothesis and an alternative hypothesis is specified. In a Neyman-Pearson framework, the goal of statistical tests is to guide the behavior of researchers with respect to these two hypotheses. Based on the results of a statistical test, and without ever knowing whether the hypothesis is true or not, researchers choose to tentatively act as if the null hypothesis or the alternative hypothesis is true. In psychology, researchers often use an imperfect hybrid of the Fisherian and Neyman-Pearson frameworks, but the Neyman-Pearson approach is, according to Dienes (2008) “the logic underlying all the statistics you see in the professional journals of psychology”.\nWhen a Neyman-Pearson hypothesis test is performed, the observed p-value is only used to check if it is smaller than the chosen alpha level, but it does not matter how much smaller it is. For example, if an alpha level of 0.01 is used, both a p = 0.006 and a p = 0.000001 will lead researchers to decide to act as if the state of the world is best described by the alternative hypothesis. This differs from a Fisherian approach to p-values, where the lower the p-value, the greater the psychological reluctance of a researcher to accept the null hypothesis they are testing. A Neyman-Pearson hypothesis test does not see the goal of an inference as quantifying a continuous measure of compatibility or evidence. Instead, as Neyman (1957) writes:\n\nThe content of the concept of inductive behavior is the recognition that the purpose of every piece of serious research is to provide grounds for the selection of one of several contemplated courses of action.\n\nIntuitively, one might feel that decisions about how to act should not be based on the results of a single statistical test, and this point is often raised as a criticism of a Neyman-Pearson approach to statistical inferences. However, such criticisms rarely use the same definition of an ‘act’ as Neyman used. It is true that, for example, the decision to implement a new government policy should not be based on a single study result. However, Neyman considered making a scientific claim an “act” as well, and wrote (1957, p. 10) that the concluding phase of a study involves:\n\nan act of will or a decision to take a particular action, perhaps to assume a particular attitude towards the various sets of hypotheses\n\nCox (1958) writes:\n\nit might be argued that in making an inference we are ‘deciding’ to make a statement of a certain type about the populations and that therefore, provided that the word decision is not interpreted too narrowly, the study of statistical decisions embraces that of inference. The point here is that one of the main general problems of statistical inference consists in deciding what types of statement can usefully be made and exactly what they mean.\n\nThus, in a Neyman-Pearson approach, p-values form the basis of decisions about which claims to make. In science, such claims underlie most novel experiments in the form of auxiliary hypotheses, or the assumptions about underlying hypotheses that are assumed to be accurate in order for a test to work as planned (Hempel 1967). For example, if it is important that participants can see color in a planned experiment, we assume it is true that the Ishihara test successfully identifies which participants are colorblind."
  },
  {
    "objectID": "samplepvalues.html#creating-a-null-model",
    "href": "samplepvalues.html#creating-a-null-model",
    "title": "12  Using p-values to test a hypothesis",
    "section": "12.2 Creating a null model",
    "text": "12.2 Creating a null model\nAssume I ask two groups of 10 people how much they liked the extended directors cut of the Lord of the Rings (LOTR) trilogy. This means our total sample size (N) is 20, and the sample size in each group (n) is 10. The first group consists of my friends, and the second groups consists of friends of my wife. Our friends rate the trilogy on a score from 1 to 10. We can calculate the average rating by my friends, which is 8.7, and the average rating by my wife’s friends, which is 7.7. We can compare the scores in both groups by looking at the raw data, and by plotting the data.\n\n\n\n\n\n\nRatings for the Lord of the Rings extended trilogy by two groups of friends.\n\n\n\n\n\n\nFriends Daniel\n\n\nFriends Kyra\n\n\n\n\n\n\nfriend_1\n\n\n9\n\n\n9\n\n\n\n\nfriend_2\n\n\n7\n\n\n6\n\n\n\n\nfriend_3\n\n\n8\n\n\n7\n\n\n\n\nfriend_4\n\n\n9\n\n\n8\n\n\n\n\nfriend_5\n\n\n8\n\n\n7\n\n\n\n\nfriend_6\n\n\n9\n\n\n9\n\n\n\n\nfriend_7\n\n\n9\n\n\n8\n\n\n\n\nfriend_8\n\n\n10\n\n\n8\n\n\n\n\nfriend_9\n\n\n9\n\n\n8\n\n\n\n\nfriend_10\n\n\n9\n\n\n7\n\n\n\n\n\nFigure 12.1: Ratings for the Lord of the Rings extended trilogy by two groups of friends.\n\n\n\n\n\n\n\n\nWe can see the groups overlap but the mean ratings differ by 1 whole point. The question we are now faced with is the following: Is the difference between the two groups just random variation, or can we claim that my friends like the extended directors cut of the Lord of the Rings (LOTR) trilogy more than my wife’s friends?\nIn a null hypothesis significance test we try to answer this question by calculating the probability of the observed difference (in this case, a mean difference of 1) or a more extreme difference, under the assumption that there is no real difference between how much my friends and my wife’s friends like the extended directors cut of LOTR, and we are just looking at random noise. This probability is called the p-value. If this probability is low enough, we decide to claim there is a difference. If this probability is not low enough, we refrain from making a claim about a difference.\nThe null hypothesis assumes that if we would ask an infinite number of my friends and an infinite number of my wife’s friends how much they like LOTR, the difference between these huge groups is exactly 0. However, in any sample drawn from the population, random variation is very likely to lead to a difference somewhat larger or smaller than 0. We can create a null model that quantifies the expected variation in the observed data, just due to random noise, to tell us what constitutes a reasonable expectation about how much the differences between groups can vary if there is no difference in the population.\nIt is practical to create a null model in terms of a standardized distribution, as this makes it easier to calculate the probability that specific values will occur, regardless of the scale that is used to collect the measurements. One version of a null model for differences is the t-distribution, which can be used to describe which differences should be expected when drawing samples from a population. Such a null model is built on assumptions. In the case of the t-distribution, the assumption is that scores are normally distributed. In reality, the assumptions upon which statistical methods are built are never met perfectly, which is why statisticians examine the impact of violations of assumptions on methodological procedures. Statistical tests are still useful in practice when the impact of violations on statistical inferences is small enough.\nWe can quantify the distribution of t-values that is expected when there is no difference in the population by a probability density function. Below is a plot of the probability density function for a t-distribution with 18 degrees of freedom (df), which corresponds to our example where we collect data from 20 friends (df = N - 2 for two independent groups). For a continuous distribution, where probabilities are defined for an infinite number of points, the probability of observing any single point (e.g., t = 2.5) is always zero. Probabilities are measured over intervals. For this reason, when a p-value is computed, it is not defined as ‘the probability of observing the data’, but as ‘the probability of observing the data, or more extreme data’. This creates an interval (a tail of a distribution) for which a probability can be calculated."
  },
  {
    "objectID": "samplepvalues.html#calculating-a-p-value",
    "href": "samplepvalues.html#calculating-a-p-value",
    "title": "12  Using p-values to test a hypothesis",
    "section": "12.3 Calculating a p-value",
    "text": "12.3 Calculating a p-value\nA t-value can be computed from the mean in the sample, the mean in the population, the standard deviation in the sample, and the sample size. By then computing the probability of observing a t-value as extreme or more extreme as the one observed, we get a p-value. For the comparison of the movie ratings for the two groups of friends above, performing a two-sided Student’s t-test yields a t-value of 2.5175 and a p-value of 0.02151.\n\nt.test(df_long$rating ~ df_long$`Friend Group`, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  df_long$rating by df_long$`Friend Group`\nt = 2.5175, df = 18, p-value = 0.02151\nalternative hypothesis: true difference in means between group Friends Daniel and group Friends Kyra is not equal to 0\n95 percent confidence interval:\n 0.1654875 1.8345125\nsample estimates:\nmean in group Friends Daniel   mean in group Friends Kyra \n                         8.7                          7.7 \n\n\nWe can graph the t-distribution (for df = 18) and highlight the two tail areas that start at the t-values of 2.5175 and -2.5175.\n\n\n\n\n\nFigure 12.2: A t-distribution with 18 degrees of freedom."
  },
  {
    "objectID": "samplepvalues.html#sec-whichpexpect",
    "href": "samplepvalues.html#sec-whichpexpect",
    "title": "12  Using p-values to test a hypothesis",
    "section": "12.4 Which p-values can you expect?",
    "text": "12.4 Which p-values can you expect?\nIn a very educational video about the ‘Dance of the p-values’, Geoff Cumming explains that p-values vary from experiment to experiment. However, this is not a reason to ‘not trust p’ as he mentions in the video. Instead, it is important to clearly understand p-value distributions to prevent misconceptions. Because p-values are part of frequentist statistics, we need to examine what we can expect in the long run. Because we never do the same experiment hundreds of times, and we do only a very limited number of studies in our lifetime, the best way to learn about what we should expect in the long run is through computer simulations.\nTake a moment to try to answer the following two questions. Which p-values can you expect to observe if there is a true effect, and you repeat the same study one-hundred thousand times? And which p-values can you expect if there is no true effect, and you repeat the same study one-hundred thousand times? If you don’t know the answer, don’t worry - you will learn it now. But if you don’t know the answer, it is worth reflecting on why you don’t know the answer about such an essential aspect of p-values. If you are like me, you were simply never taught this. But as we will see, it is essential to a solid understanding of how to interpret p-values.\nWhich p-values you can expect is completely determined by the statistical power of the study, or the probability that you will observe a significant effect, if there is a true effect. The statistical power ranges from 0 to 1. We can illustrate this by simulating independent t-tests. The idea is that we simulate IQ scores for a group of people. We know the standard deviation of IQ scores is 15. For now, we will set the mean IQ score in one simulated group to 100, and in the other simulated group to 105. We are testing if the people in one group have an IQ that differs from the other group (and we know the correct answer is ‘yes’, because we made it so in the simulation).\n\np &lt;- numeric(100000) # store all simulated *p*-values\n\nfor (i in 1:100000) { # for each simulated experiment\n  x &lt;- rnorm(n = 71, mean = 100, sd = 15) # Simulate data\n  y &lt;- rnorm(n = 71, mean = 105, sd = 15) # Simulate data\n  p[i] &lt;- t.test(x, y)$p.value # store the *p*-value\n}\n\n(sum(p &lt; 0.05) / 100000) # compute power\n\nhist(p, breaks = 20) # plot a histogram\n\nIn the simulation, we generate n = 71 normally distributed IQ scores with means of M (100 and 105 by default) and a standard deviation of 15. We then perform an independent t-test, store the p-value, and generate a plot of the p-value distribution.\n\n\n\n\n\nFigure 12.3: Distribution of p-values when power = 50%.\n\n\n\n\nOn the x-axis we see p-values from 0 to 1 in 20 bars, and on the y-axis we see how frequently these p-values were observed. There is a horizontal red dotted line that indicates an alpha of 5% (located at a frequency of 100.000*0.05 = 5000) – but you can ignore this line for now. In the title of the graph, the statistical power that is achieved in the simulated studies is given (assuming an alpha of 0.05): The studies have 50% power.\nThe simulation result illustrates the probability density function of p-values fot a t-test. A probability density function provides the probability that a random variable has a specific value (such as Figure 12.2) of the t-distribution). Because the p-value is a random variable, we can use its probability density function to plot the p-value distribution (hung_behavior_1997?; ulrich_properties_2018?), as in Figure 12.4). In this online Shiny app you can vary the sample size, effect size, and alpha level to examine the effect on the p-value distribution. Increasing the sample size or the effect size will increase the steepness of the p-value distribution, which means that the probability to observe small p-values increases. The p-value distribution is a function of the statistical power of the test.\n\n\n\n\n\nFigure 12.4: Probability density function for p-values from a two-sided t-test.\n\n\n\n\nWhen there is no true effect, and test assumptions are met, p-values for a t-test are uniformly distributed. This means that every p-value is equally likely to be observed when the null hypothesis is true. In other words, when there is no true effect, a p-value of 0.08 is just as likely as a p-value of 0.98. I remember thinking this was very counterintuitive when I first learned about uniform p-value distributions (well after completing my PhD). But it makes sense that p-values are uniformly distributed when we think about the goal to guarantee that when \\(H_0\\) is true, alpha % of the p-values should fall below the alpha level. If we set alpha to 0.01, 1% of the observed p-values should fall below 0.01, and if we set alpha to 0.12, 12% of the observed p-values should fall below 0.12. This can only happen if p-values are uniformly distributed when the null hypothesis is true. Note that the uniform p-value distribution only emerges if the distribution of the test statistic is continuous (e.g., such as for the t-test), and not if the distribution of the test statistic is discrete (e.g., in the case of the chi-squared test, see (wang_p-value_2019?)).\n\n\n\n\n\nFigure 12.5: Distribution of p-values when the null hypothesis is true."
  },
  {
    "objectID": "samplepvalues.html#sec-lindley",
    "href": "samplepvalues.html#sec-lindley",
    "title": "12  Using p-values to test a hypothesis",
    "section": "12.5 Lindley’s paradox",
    "text": "12.5 Lindley’s paradox\nAs the statistical power increases, some p-values below 0.05 (e.g., p = 0.04) can be more likely when there is no effect than when there is an effect. This is known as Lindley’s paradox (lindley_statistical_1957?), or sometimes the Jeffreys-Lindley paradox (spanos_who_2013?). Because the distribution of p-values is a function of the statistical power (cumming_replication_2008?), the higher the power, the more right-skewed the p-value distribution becomes (i.e., the more likely it becomes that small p-values are observed). When there is no true effect, p-values are uniformly distributed, and 1% of observed p-values fall between 0.04 and 0.05. When the statistical power is extremely high, not only will most p-values fall below 0.05, most p-values will fall below 0.01. In Figure 12.6) we see that with high power very small p-values (e.g., 0.001) are more likely to be observed when there is an effect than when there is no effect (e.g., the dotted black curve representing 99% power falls above the grey horizontal line representing the uniform distribution when the null is true for a p-value of 0.01).\nYet perhaps surprisingly, observing a p-value of 0.04 is more likely when the null hypothesis (\\(H_0\\)) is true than when the alternative hypothesis (\\(H_1\\)) is true and we have very high power, as illustrated by the fact that in Figure 12.6) the density of the p-value distribution is higher when the null is true, than when a test has 99% power, at 0.04. Lindley’s paradox shows that a p-value of for example 0.04 can be statistically significant, but at the same time provides evidence for the null hypothesis. From a Neyman-Pearson approach we have made a claim that has a maximum error rate of 5%, but from a likelihood or Bayesian approach, we should conclude our data provides evidence in favor of the null hypothesis, relative to the alternative hypothesis. Lindley’s paradox illustrates when different statistical philosophies would reach different conclusions, and why a p-value cannot directly be interpreted as a measure of evidence, without taking the power of the test into account. Although it is not necessary, researchers might desire to prevent situations where a frequentist rejects the null hypothesis based on p &lt; 0.05, when the evidence in the test favors the null hypothesis over the alternative hypothesis. This can be achieved by lowering the alpha level as a function of the sample size (leamer_specification_1978?; maier_justify_2022?; good_bayesnon-bayes_1992?), as explained in the chapter on error control.\n\n\n\n\n\nFigure 12.6: P-value distribution for 0 (grey horizontal line, 50 percent power (black solid curve), and 99 percent power (black dotted curve, where p-values just below 0.05 are more likely when \\(H_0\\) is true than when \\(H_1\\) is true)."
  },
  {
    "objectID": "samplepvalues.html#sec-correctlyinterpreting",
    "href": "samplepvalues.html#sec-correctlyinterpreting",
    "title": "12  Using p-values to test a hypothesis",
    "section": "12.6 Correctly reporting and interpreting p-values",
    "text": "12.6 Correctly reporting and interpreting p-values\nAlthough from a strict Neyman-Pearson perspective it is sufficient to report that p &lt; \\(\\alpha\\) or that p &gt; \\(\\alpha\\), researchers should report exact p-values. This facilitates the re-use of results for secondary analyses (appelbaum_journal_2018?), and allows other researchers to compare the p-value to an alpha level they would have preferred to use (lehmann_testing_2005?). Because claims are made using a methodological procedure with known maximum error rates, a p-value never allows you state anything with certainty. Even if we set the alpha level to 0.000001 any single claim can be an error, Fisher (1936) reminds us, “for the ‘one chance in a million’ will undoubtedly occur, with no less and no more than its appropriate frequency, however surprised we may be that it should occur to us”. This is the reason that replication studies are important in science. Any single finding could be a fluke, but this probability quickly becomes very small if several replication studies observe the same finding. This uncertainty is sometimes not reflected in academic writing, where researchers can be seen using words as ‘prove’, ‘show’, or ‘it is known’. A slightly longer but more accurate statement after a statistically significant null hypothesis test would read:\n\nWe claim there is a non-zero effect, while acknowledging that if scientists make claims using this methodological procedure, they will be misled, in the long run, at most alpha % of the time, which we deem acceptable. We will for the foreseeable future, until new data or information emerges that proves us wrong, assume this claim is correct.\n\nAfter a statistically non-significant null hypothesis test one could write:\n\n\nWe can not claim there is a non-zero effect, while acknowledging that if scientists refrain from making claims using this methodological procedure, they will be misled, in the long run, at most beta % of the time, which we deem acceptable.\n\n\nNote that after a non-significant null hypothesis test we cannot claim the absence of an effect.\nRemember that in a Neyman-Pearson framework researchers make claims, but do not necessarily believe in the truth of these claims. For example, the OPERA collaboration reported in 2011 that they had observed data that seemed to suggest neutrinos traveled faster than the speed of light. This claim was made with a with a 0.2-in-a-million Type 1 error rate, assuming the error was purely due to random noise. However, none of the researchers actually believed this claim was true, because it is theoretically impossible for neutrinos to move faster than the speed of light. Indeed, it was later confirmed that equipment failures were the cause of the anomalous data: a fiber optic cable had been attached improperly, and a clock oscillator was ticking too fast. Nevertheless, the claim was made with the explicit invitation to the scientific community to provide new data or information that would prove this claim wrong.\nWhen researchers “accept” or “reject” a hypothesis in a Neyman-Pearson approach to statistical inferences, they do not communicate any belief or conclusion about the substantive hypothesis. Instead, they utter a Popperian basic statement based on a prespecified decision rule that the observed data reflect a certain state of the world. Basic statements describe an observation that has been made (e.g., “I have observed a black swan”) or an event that has occurred (e.g., “students performed better at the exam when being trained in spaced practice, than when not”).\nThe claim is about the data we have observed, but not about the theory we used to make predictions. The claim is about observed data, as it is a statistical inference, and not about the theory, which requires a theoretical inference. Data never ‘proves’ a theory is true or false. A basic statement can corroborate a prediction derived from a theory, or not. If many predictions deduced from a theory are corroborated, we can become increasingly convinced the theory is close to the truth. This ‘truth-likeness’ of theories is called verisimilitude (Niiniluoto 1998; Popper 2005). A shorter statement when a hypothesis test is presented would therefore read ‘p = .xx, which corroborates our prediction, at an alpha level of y%’, or ‘p = .xx, which does not corroborate our prediction, at a statistical power of y% for our effect size of interest’. Often, the alpha level or the statistical power is only mentioned in the experimental design section of an article, but repeating them in the results section might remind readers of the error rates associated with your claims.\nEven when we have made correct claims, the underlying theory can be false. Popper (2005) reminds us that “The empirical basis of objective science has thus nothing ‘absolute’ basis about it”. He argues science is not built on a solid bedrock, but on piles driven in a swamp and notes that “We simply stop when we are satisfied that the piles are firm enough to carry the structure, at least for the time being.” As Hacking (2016) writes: “Rejection is not refutation. Plenty of rejections must be only tentative”. So when we reject the null model, we do so tentatively, aware of the fact we might have done so in error, without necessarily believing the null model is false, and without believing the theory we have used to make predictions is true. For Neyman (1957) inferential behavior is an: “act of will to behave in the future (perhaps until new experiments are performed) in a particular manner, conforming with the outcome of the experiment”. All knowledge in science is provisional.\nSome statisticians recommend interpreting p-values as measures of evidence. For example, Bland (bland_introduction_2015?) teaches that p-values can be interpreted as a ‘rough and ready’ guide for the strength of evidence, and that p &gt; 0.1 indicates ‘little or no evidence’, 0.01 &lt; p &lt; 0.05 indicates ‘evidence’, p &lt; 0.001 is ‘very strong evidence’. This is incorrect (Johansson 2011), as is clear from the previous discussions of Lindley’s paradox and uniform p-value distributions. If you want to quantify evidence, other tools are required."
  },
  {
    "objectID": "samplepvalues.html#sec-misconceptions",
    "href": "samplepvalues.html#sec-misconceptions",
    "title": "12  Using p-values to test a hypothesis",
    "section": "12.7 Preventing common misconceptions about p-values",
    "text": "12.7 Preventing common misconceptions about p-values\nA p-value is the probability of the observed data, or more extreme data, under the assumption that the null hypothesis is true. To understand what this means, it might be especially useful to know what this doesn’t mean. First, we need to know what ‘the assumption that the null hypothesis is true’ looks like, and which data we should expect if the null hypothesis is true. Although the null hypothesis can be any value, in this assignment we will assume the null hypothesis is specified as a mean difference of 0. For example, we might be interested in calculating the difference between a control condition and an experimental condition on a dependent variable.\nIt is useful to distinguish the null hypothesis (the prediction that the mean difference in the population is exactly 0) and the null model (a model of the data we should expect when we collect data when the null hypothesis is true). The null hypothesis is a point at 0, but the null model is a distribution. It is visualized in textbooks or power analysis software using pictures as you can see below, with t-values on the horizontal axis, and a critical t-value somewhere between 1.96 – 2.00 (depending on the sample size). This is done because the statistical test when comparing two groups is based on the t-distribution, and the p-value is statistically significant when the t-value is larger than a critical t-value.\nI personally find things become a lot clearer if you plot the null model as mean differences instead of t-values. So below, you can see a null model for the mean differences we can expect when comparing two groups of 50 observations where the true difference between the two groups is 0, and the standard deviation in each group is 1. Because the standard deviation is 1, you can also interpret the mean differences as a Cohen’s d effect size. So this is also the distribution you can expect for a Cohen’s d of 0, when collecting 50 observations per group in an independent t-test.\n\n\n\n\n\nFigure 12.7: Distribution of observed Cohen’s d effect sizes when collecting 50 observations per group in an independent t-test.\n\n\n\n\nThe first thing to notice is that we expect that the mean of the null model is 0. Looking at the x-axis, we see the plotted distribution is centered on 0. But even if the mean difference in the population is 0 that does not imply every sample we draw from the population will give a mean difference of exactly zero. There is variation around the population value, as a function of the standard deviation and the sample size.\nThe y-axis of the graph represents the density, which provides an indication of the relative likelihood of measuring a particular value of a continuous distribution. We can see that the most likely mean difference is the true population value of zero, and that larger differences from zero become increasingly less likely. The graph has two areas that are colored red. These areas represent 2.5% of the most extreme values in the left tail of the distribution, and 2.5% of the most extreme values in the right tail of the distribution. Together, they make up 5% of the most extreme mean differences we would expect to observe, given our number of observations, when the true mean difference is exactly 0. When a mean difference in the red area is observed, the corresponding statistical test will be statistically significant at a 5% alpha level. In other words, not more than 5% of the observed mean differences will be far enough away from 0 to be considered surprising. Because the null hypothesis is true, observing a ‘surprising’ mean difference in the red areas is a Type 1 error.\nLet’s assume that the null model in the Figure above is true, and that we observe a mean difference of 0.5 between the two groups. This observed difference falls in the red area in the right tail of the distribution. This means that the observed mean difference is relatively surprising, under the assumption that the true mean difference is 0. If the true mean difference is 0, the probability density functions shows that we should not expect a mean difference of 0.5 very often. If we calculate a p-value for this observation, it would be lower than 5%. The probability of observing a mean difference that is at least as far away from 0 as 0.5 (either to the left from the mean, or to the right, when we do a two-tailed test) is less than 5%.\nOne reason why I prefer to plot the null model in raw scores instead of t-values is that you can see how the null model changes when the sample size increases. When we collect 5000 instead of 50 observations, we see the null model is still centered on 0 – but in our null model we now expect most values will fall very close around 0.\n\n\n\n\n\nFigure 12.8: Distribution of observed Cohen’s d effect sizes when collecting 5000 observations per group in an independent t-test when d = 0.\n\n\n\n\nThe distribution is much narrower because the distribution of mean differences is based on the standard error of the difference between means. This value is calculated based on the standard deviation and the sample size, as follows:\n\\[\\sqrt{\\frac{\\sigma_{1}^{2}}{n_{1}}+\\frac{\\sigma_{2}^{2}}{n_{2}}}\\]\nThis formula shows that the standard deviations of each group (σ) are squared and divided by the sample size of that group, added together, after which the square root is taken. The larger the sample size the bigger the number we divide by, and thus the smaller standard error of the difference between means. In our n = 50 example this is:\n\\[\\sqrt{\\frac{1^{2}}{50}+\\frac{1^{2}}{50}}\\]\nThe standard error of the differences between means is thus 0.2 for n = 50 in each group, and for n = 5000 it is 0.02. Assuming a normal distribution, 95% of the observations fall between 1.96 SE. So for 50 samples per group, the mean differences should fall between -1.96 * 0.2 = -0.392, and +1.96 * 0.2 = 0.392, and we can see the red areas start from approximately -0.392 to 0.392 for n = 50. For 5000 samples per group, the mean differences should fall between -1.96 * 0.02, and +1.96 * 0.02; in other words between -0.0392 to 0.0392 for n = 5000. Due to the larger sample size with n = 5000 observations per group, we should expect to observe mean differences in our sample closer to 0 compared to our null model when we had only 50 observations.\nIf we collected n = 5000, and we would again observe a mean difference of 0.5, it should be clear that this same difference is even more surprising than it was when we collected 50 observations. We are now almost ready to address common misconceptions about p-values, but before we can do this, we need to introduce a model of the data when the null is not true. If we are not sampling data from a model where the true mean difference is 0, what does our alternative model look like? Some software (such as G*power, see Figure 12.9) will visualize both the null model (red curve) and the alternative model (blue curve) in their output:\n\n\n\nFigure 12.9: “Screenshot from G\\Power software visualizing the null model (red distribution) and alternative model (blue distribution) and the critical t*-value (1.66055) that is the threshold distinguishing significant and non-significant results.”\n\n\nWhen we do a study, we rarely already know what the true mean difference is (if we already knew, why would we do the study?). But let’s assume there is an all-knowing entity. Following Paul Meehl, we will call this all-knowing entity ‘Omniscient Jones’. Before we collect our sample of 50 observations, Omniscient Jones already knows that the true mean difference in the population is 0.5. Again, we should expect some variation around 0.5 in this alternative model. The figure below shows the expected data pattern when the null hypothesis is true (now indicated by a grey line) and it shows an alternative model, assuming a true mean difference of 0.5 exists in the population (indicated by a black line).\n\n\n\n\n\nFigure 12.10: Distribution of observed Cohen’s d effect sizes when collecting 50 observations per group in an independent t-test when d = 0.\n\n\n\n\nBut Omniscient Jones could have said the true difference was much larger. Let’s assume we do another study, but now before we collect our 50 observations, Omniscient Jones tells us that the true mean difference is 1.5. The null model does not change, but the alternative model now moves over to the right.\n\n\n\n\n\nFigure 12.11: Distribution of observed Cohen’s d effect sizes when collecting 50 observations per group in an independent t-test for d = 0 and d = 0.5.\n\n\n\n\nYou can play around with the alternative and null models in this online app. The app allows you to specify the sample size in each group of an independent t-test (from 2 to infinity), the mean difference (from 0 to 2), and the alpha level. In the plot, the red areas visualize Type 1 errors. The blue area visualizes the Type 2 error rate (which we will discuss below). The app also tells you the critical value: There is a vertical line (with n = 50 this line falls at a mean difference of 0.4) and a sentence that says: “Effects larger than 0.4 will be statistically significant”. Note that the same is true for effects smaller than -0.4, even though there is no second label there, but the app shows the situation for a two-sided independent t-test.\nYou can see that on the left of the vertical line that indicates the critical mean difference there is a blue area that is part of the alternative model. This is the Type 2 error rate (or 1 - the power of the study). If a study has 80% power, 80% of the mean differences we will observe should fall on the right of the critical value indicated by the line. If the alternative model is true, but we observe an effect smaller than the critical value, the observed p-value will be larger than 0.05, even when there is a true effect. You can check in the app that the larger the sample size, the further to the right the entire alternative distribution falls, and thus the higher the power. You can also see that the larger the sample size, the narrower the distribution, and the less of the distribution will fall below the critical value (as long as the true population mean is larger than the critical value). Finally, the larger the alpha level, the further to the left the critical mean difference moves, and the smaller the area of the alternative distribution that falls below the critical value.\nThe app also plots 3 graphs that illustrate the power curves as a function of different alpha levels, sample sizes, or true mean differences. Play around in the app by changing the values. Get a feel for how each variable impacts the null and alternative models, the mean difference that will be statistically significant, and the Type 1 and Type 2 error rates.\nSo far, several aspects of null models should have become clear. First of all, the population value in a traditional null hypothesis is a value of 0, but in any sample you draw, the observed difference falls in a distribution centered on 0, and will thus most often be slightly larger or smaller than 0. Second, the width of this distribution depends on the sample size and the standard deviation. The larger the sample size in the study, the narrower the distribution will be around 0. Finally, when a mean difference is observed that falls in the tails of the null model, this can be considered surprising. The further away from the null value, the more surprising this result is. But when the null model is true, these surprising values will happen with a probability specified by the alpha level (and are called Type 1 errors). Remember that a Type 1 error occurs when a researcher concludes there is a difference in the population, while the true mean difference in the population is zero.\nWe are now finally ready to address some common misconceptions about p-values. Let’s go through a list of common misconceptions that have been reported in the scientific literature. Some of these examples might sound like semantics. It is easy to at first glance think that the statement communicates the right idea, even if the written version is not formally correct. However, when a statement is not formally correct, it is wrong. And exactly because people so often misunderstand p-values, it is worth it to be formally correct about how they should be interpreted.\n\n12.7.1 Misunderstanding 1: A non-significant p-value means that the null hypothesis is true.\nA common version of this misconception is reading a sentence such as ‘because p &gt; 0.05 we can conclude that there is no effect’. Another version of such a sentence is ‘there was no difference, (p &gt; 0.05)’.\nBefore we look at this misconception in some detail, I want to remind you of one fact that is easy to remember, and will enable you to recognize many misconceptions about p-values: p-values are a statement about the probability of data, not a statement about the probability of a hypothesis or the probability of a theory. Whenever you see p-values interpreted as a probability of a theory or a hypothesis, you know something is not right. Examples of statements about a hypothesis are ‘The null hypothesis is true’, or ‘The alternative hypothesis is true’, because both these statements say that the probability that the null or alternative model is true is 100%. A subtler version is a statement such as ‘the observed difference is not due to chance’. The observed difference is only ‘due to chance’ (instead of due to the presence of a real difference) when the null hypothesis is true, and as before, this statement implies it is 100% probable that the null hypothesis is true.\nWhen you conclude that ‘there is no effect’ or that ‘there is no difference’ you are similarly claiming that it is 100% probable that the null hypothesis is true. But since p-values are statements about the probability of data, you should refrain from making statements about the probability of a theory solely based on a p-value. That’s ok. p-values were designed to help you identify surprising results from a noisy data generation process (aka the real world). They were not designed to quantify the probability that a hypothesis is true.\nLet’s take a concrete example that will illustrate why a non-significant result does not mean that the null hypothesis is true. In the figure below, Omniscient Jones tells us the true mean difference is 0.5. We can see this, because the alternative distribution which visualizes the probability of the mean differences we should expect when the alternative hypothesis is true is centered on 0.5. We have observed a mean difference of 0.35. This value is not extreme enough to be statistically different from 0. We can see this, because the value does not fall within the red area of the null model (and hence, the p-value is not smaller than our alpha level).\nNevertheless, we see that observing a mean difference of 0.35 is not only quite likely given that the true mean difference is 0.5, but observing a mean difference of 0.35 is much more likely under the alternative model than under the null model. You can see this by comparing the height of the density curve at a difference of 0.35 for the null model, which is approximately 0.5, and the height of the density curve for the alternative model, which is approximately 1.5. See the chapter on likelihoods for further details.\n\n\n\n\n\nDistribution of observed Cohen’s d effect sizes when collecting 50 observations per group in an independent t-test for d = 0 and d = 0.5 when observing d = 0.35.\n\n\n\n\nAll the p-value tells us is that a mean difference of 0.35 is not extremely surprising, if we assume the null hypothesis is true. There can be many reasons for this. In the real world, where we have no Omniscient Jones to tell us about the true mean difference, it is possible that there is a true effect, as illustrated in the figure above.\nSo what should we say instead? The solution is subtle, but important. Let’s revisit the two examples of incorrect statements we made earlier. First, ‘because p &gt; 0.05 we can conclude that there is no effect’ is incorrect, because there might very well be an effect (and remember p-values are statements about data, not about the probability that there is an effect or is no effect). Fisher’s interpretation of a p-value was that we can conclude a rare event has happened, or that the null hypothesis is false (he writes literally: “Either an exceptionally rare chance has occurred, or the theory of random distribution is not true”). This might sound like it is a statement about the probability of a theory, but it is really just stating the two possible scenarios under which low p-values occur (when you have made a Type 1 error, or when the alternative hypothesis is true). Both a true positive as a false positive remain possible, and we do not quantify the probability of either possible reality (e.g., we are not saying it is 95% probable that the null hypothesis is false). From a Neyman-Pearson perspective, a p &gt; .05 means that we cannot act as if the null hypothesis can be rejected, without maintaining our desired error rate of 5%.\nIf you are interested in concluding an effect is absent, null hypothesis testing is not the tool to use. A null hypothesis test answers the question ‘can I reject the null hypothesis with a desired error rate?’. If you cannot do this, and p &gt; 0.05, no conclusion can be drawn based only on the p-value (remember the concept of 無 ‘mu’: the answer is neither yes nor no).\nThe second incorrect statement was ‘there was no difference’. This statement is somewhat easier to correct. You can instead write ‘there was no statistically significant difference’. Granted, this is a bit tautological, because you are basically saying that the p-value was larger than the alpha level in two different ways, but at least this statement is formally correct. The difference between ‘there was no difference’ and ‘there was no statistically significant difference’ might sound like semantics, but in the first case you are formally saying ‘the difference was 0’ while in the second you are saying ‘there was no difference large enough to yield a p &lt; .05’. Although I have never seen anyone do this, a more informative message might be ‘because given our sample size of 50 per group, and our alpha level of 0.05, only observed differences more extreme than 0.4 could be statistically significant, and our observed mean difference was 0.35 thus we could not reject the null hypothesis’. If this feels like a very unsatisfactory conclusion, remember that a null hypothesis test was not designed to draw interesting conclusions about the absence of effects – you will need to learn about equivalence tests to get a more satisfactory answers about null effects.\n\n\n12.7.2 Misunderstanding 2: A significant p-value means that the null hypothesis is false.\nThis is the opposite misconception from the one we discussed previously. Examples of incorrect statements based on this misconception are “p &lt; .05, therefore there is an effect”, or “there is a difference between the two groups, p &lt; .05”. As before, both these statements imply it is 100% probable that the null model is false, and an alternative model is true.\nAs a simple example of why such extreme statements are incorrect, imagine we generate a series of numbers in R using the following command:\n\nrnorm(n = 50, mean = 0, sd = 1)\n\n [1] -0.37702287  0.71656312 -1.93299023 -2.37123766  0.30642922 -0.02036956\n [7] -2.07097645 -0.18141399 -1.78210064 -2.21064465 -0.81756594 -0.51232930\n[13]  0.77361268  0.07190125 -1.13285368  0.34888087  0.46773136  0.82713289\n[19] -1.12878666  0.59104850 -1.14795925  0.50612271  0.58893219 -1.07108241\n[25]  0.80231208  0.79017509 -2.59499719 -0.81032820 -0.64508961 -2.35201893\n[31] -0.71651907  0.35546000 -0.64469709 -0.27161641 -0.50700681 -0.61840925\n[37] -0.98193650 -0.37659341 -0.98259105 -0.16172414 -0.13503477  0.33440552\n[43]  0.02759189  1.29132258  1.25315291  0.30609552 -0.94249339 -0.08486031\n[49] -0.27643332 -0.26567939\n\n\nThis command generates 50 random observations from a distribution with a mean of 0 and a standard deviation of 1 (in the long run – the mean and standard deviation will vary in each sample that is generated). Imagine we run this command once, and we observe a mean of 0.5. The figure below visualizes this scenario. We can perform a one-sample t-test against 0, and this test tells us, with a p &lt; .05, that the data we have observed is surprisingly different from 0, assuming the random number generator in R functions as it should and generates data with a true mean of 0.\n\n\n\n\n\nFigure 12.12: Distribution of observed Cohen’s d effect sizes when collecting 50 observations per group in an independent t-test when d = 0 and observing d = 0.5.\n\n\n\n\nThe significant p-value does not allow us to conclude that the null hypothesis (“the random number generator works”) is false. It is true that the mean of the 50 samples we generated was surprisingly extreme. But a low p-value simply tells us that an observation is surprising. We should observe such surprising observations with a low probability when the null hypothesis is true – when the null is true, they still happen. Therefore, a significant result does not mean an alternative hypothesis is true – the result can also be a Type 1 error, and in the example above, Omniscient Jones knows that this is the case.\nLet’s revisit the incorrect statement “p &lt; .05, therefore there is an effect”. A correct interpretation of a significant p-value requires us to acknowledge the possibility that our significant result might be a Type 1 error. Remember that Fisher would conclude that “Either an exceptionally rare chance has occurred, or the theory of random distribution is not true”. A correct interpretation in terms of Neyman-Pearson statistics would be: “we can act as if the null hypothesis is false, and we would not be wrong more than 5% of the time in the long run”. Note the specific use of the word ‘act’, which does not imply anything about whether this specific hypothesis is true or false, but merely states that if we act as if the null hypothesis is false any time we observe p &lt; alpha, we will not make an error more than alpha percent of the time.\nBoth these formally correct statements are a bit long. In scientific articles, we often read a shorter statement such as: ‘we can reject the null hypothesis’, or ‘we can accept the alternative hypothesis’. These statements might be made with the assumption that readers will themselves add ‘with a 5% probability of being wrong, in the long run’. But it might be useful to add ‘with a 5% long run error rate’ at least the first time you make such a statement in your article to remind readers.\nIn the example above we have a very strong subjective prior probability that the random number generator in R works. Alternative statistical procedures to incorporate such prior beliefs are Bayesian statistics or false positive report probabilities. In frequentist statistics, the idea is that you need to replicate your study several times. You will observe a Type 1 error every now and then, but you are unlikely to observe a Type 1 error three times in a row. Alternatively, you can lower the alpha level in a single study to reduce the probability of a Type 1 error rate.\n\n\n12.7.3 Misunderstanding 3: A significant p-value means that a practically important effect has been discovered.\nA common concern when interpreting p-values is that ‘significant’ in normal language implies ‘important’, and thus a ‘significant’ effect is interpreted as an ‘important’ effect. However, the question whether an effect is important is completely orthogonal to the question whether it is different from zero, or even how large the effect is. Not all effects have practical impact. The smaller the effect, the less likely such effects will be noticed by individuals, but such effects might still have a large impact on a societal level. Therefore, the general take home message is that statistical significance does not answer the question whether an effect matters in practice, or is ‘practically important’. To answer the question whether an effect matters, you need to present a cost-benefit analysis.\nThis issue of practical significance most often comes up in studies with a very large sample size. As we have seen before, with an increasing sample size, the width of the density distribution around the null value becomes more and more narrow, and the values that are considered surprising fall closer and closer to zero.\nIf we plot the null model for a very large sample size (e.g., n = 10000 per group) we see that even very small mean differences (differences more extreme than a mean difference of 0.04) will be considered ‘surprising’. This still means that if there really is no difference in the population, you will observe differences larger than 0.04 less than 5% of the time, in the long run, and 95% of the observed differences will be smaller than a mean difference of 0.04. But it becomes more difficult to argue for the practical significance of such effects. Imagine that a specific intervention is successful in changing people’s spending behavior, and when implementing some intervention people save 12 cents per year. It is difficult to argue how this effect will make any individual happier. However, if this money is combined, it will yield over 2 million, which could be used to treat diseases in developing countries, where it would have a real impact. The cost of the intervention might be considered too high if the goal is to make individuals happier, but it might be consider worthwhile if the goal is to raise 2 million for charity.\nNot all effects in psychology are additive (we cannot combine or transfer an increase in happiness of 0.04 scale points), so it is often more difficult to argue for the importance of small effects in subjective feelings (anvari_not_2021?). A cost-benefit analysis might show small effects matter a lot, but whether or not this is the case cannot be inferred from a p-value.\nNote that nothing about this is a problem with the interpretation of a p-value per se: A p &lt; 0.05 still correctly indicates that, if the null hypothesis is true, we have observed data that should be considered surprising. However, just because data is surprising, does not mean we need to care about it. It is mainly the verbal label ‘significant’ that causes confusion here – it is perhaps less confusing to think of a ‘significant’ effect as a ‘surprising’ effect, but not necessarily as an ‘important’ effect.\n\n\n12.7.4 Misunderstanding 4: If you have observed a significant finding, the probability that you have made a Type 1 error (a false positive) is 5%.\nThis misinterpretation is one possible explanation of the incorrect statement that a p-value is ‘the probability that the data are observed by chance.’ Assume we collect 20 observations, and Omniscient Jones tells us the null hypothesis is true (as in the example above where we generated random numbers in R). This means we are sampling from the distribution in the figure below.\n\n\n\n\n\nFigure 12.13: Distribution of observed Cohen’s d effect sizes when collecting 20 observations per group in an independent t-test when d = 0.\n\n\n\n\nIf this is our reality, it means that 100% of the time that we observe a significant result, it is a false positive (or Type I error). Thus, 100% of our significant results are Type 1 errors.\nIt is important to distinguish probabilities before collecting the data and analyzing the result, and probabilities after collecting data and analyzing the results. What the Type 1 error rate controls is that from all studies we will perform in the future where the null hypothesis is true, not more than 5% of our observed mean differences will fall in the red tail areas. But after we have seen that our data falls in the tail areas with p &lt; alpha, and we know that the null hypothesis is true, these observed significant effects are always a Type 1 error. If you read carefully, you will notice that this misunderstanding is cause by differences in the question that is asked. “If I have observed a p &lt; .05, what is the probability that the null hypothesis is true?” is a different question than “If the null hypothesis is true, what is the probability of observing this (or more extreme) data?”. Only the latter question is answered by a p-value. The first question cannot be answered without making a subjective judgment about the probability that the null hypothesis is true prior to collecting the data.\n\n\n12.7.5 Misunderstanding 5: One minus the p-value is the probability that the effect will replicate when repeated.\nIt is impossible to calculate the probability that an effect will replicate (Miller 2009). There are too many unknown factors to accurately predict the replication probability, and one of the main factors is the true mean difference. If we were Omniscient Jones, and we knew the true mean difference (e.g., a difference between the two groups of 0.5 scale points) we would know the statistical power of our test. The statistical power is the probability that we will find a significant result, if the alternative model is true (i.e. if there is a true effect). For example, reading the text in the left bar in the app, we see that with N = 50 per group, alpha level of 0.05, and a true mean difference of 0.5, the probability of finding a significant result (or the statistical power) is 69.69%. If we would observe a significant effect in this scenario (e.g., p = 0.03) it is not true that there is a 97% probability that an exact replication of the study (with the same sample size) would again yield a significant effect. The probability that a study yields a significant effect is determined by the statistical power - not by the p-value in a previous study.\nWhat we can generally take away from this last misunderstanding is the fact that the probability of replication depends on the presence versus the absence of a true effect. In other words, as stated above, if a true effect exists then the level of statistical power informs us about how frequently we should observe a significant result (e.g., 80% power means we should observe significant result 80% of the time). On the other hand, if the null hypothesis is true (e.g., the effect is 0) then significant results will be observed only with a frequency approaching the chosen alpha level in the long run (i.e. a 5% Type 1 error rate if an an alpha of 0.05 is chosen). Therefore, if the original study correctly observed an effect, the probability of a significant result in a replication study is determined by the statistical power, and if the original study correctly observed no significant effect, the probability of a significant effect in a replication study is determined by the alpha level. In practice, many other factors determine if an effect will replicate. The only way to know if an effect will replicate, is to replicate it. If you want to explore how difficult it is to predict if findings in the literature will replicate you can perform this test by 80.000 Hours.\n\n\n\n\nBenjamini, Yoav. 2022. “It’s Not the p-Value’s Fault.” In 2022 Annual Meeting. AAAS.\n\n\nCox, David R. 1958. “Some Problems Connected with Statistical Inference.” Ann. Math. Statist 29 (2): 357–72.\n\n\nDienes, Zoltan. 2008. Understanding Psychology as a Science: An Introduction to Scientific and Statistical Inference. Springer.\n\n\nFisher, Ronald A. 1956. “Statistical Methods and Scientific Inference.”\n\n\nFisher, Ronald Aylmer. 1936. “Design of Experiments.” British Medical Journal 1 (3923): 554.\n\n\nGossett, W. 1904. “The Application of the ‘Law of Error’to the Work of the Brewery.” Guinness Internal Note 36.\n\n\nHacking, Ian. 2016. Logic of Statistical Inference. Cambridge University Press.\n\n\nHempel, Carl G. 1967. “Philosophy of Natural Science.” British Journal for the Philosophy of Science 18 (1).\n\n\nJohansson, Tobias. 2011. “Hail the Impossible: P-Values, Evidence, and Likelihood.” Scandinavian Journal of Psychology 52 (2): 113–25.\n\n\nMiller, Jeff. 2009. “What Is the Probability of Replicating a Statistically Significant Effect?” Psychonomic Bulletin & Review 16: 617–40.\n\n\nNeyman, Jerzy. 1957. “\" Inductive Behavior\" as a Basic Concept of Philosophy of Science.” Revue De L’Institut International De Statistique, 7–22.\n\n\nNeyman, Jerzy, and Egon Sharpe Pearson. 1933. “IX. On the Problem of the Most Efficient Tests of Statistical Hypotheses.” Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character 231 (694-706): 289–337.\n\n\nNiiniluoto, Ilkka. 1998. “Verisimilitude: The Third Period.” The British Journal for the Philosophy of Science 49 (1): 1–29.\n\n\nPopper, Karl. 2005. The Logic of Scientific Discovery. Routledge.\n\n\nSchweder, Tore, and Nils Lid Hjort. 2016. Confidence, Likelihood, Probability. Vol. 41. Cambridge University Press.\n\n\nSpanos, Aris. 2019. Probability Theory and Statistical Inference: Empirical Modeling with Observational Data. Cambridge University Press.\n\n\nStudent. 1908. “The Probable Error of a Mean.” Biometrika 6 (1): 1–25.\n\n\nZabell, Sandy L. 1992. “RA Fisher and Fiducial Argument.” Statistical Science, 369–87."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Appendix A — Bibliography",
    "section": "",
    "text": "Anscombe, Francis J. 1973. “Graphs in Statistical\nAnalysis.” The American Statistician 27 (1): 17–21.\n\n\nBenjamini, Yoav. 2022. “It’s Not the p-Value’s Fault.” In\n2022 Annual Meeting. AAAS.\n\n\nCox, David R. 1958. “Some Problems Connected with Statistical\nInference.” Ann. Math. Statist 29 (2): 357–72.\n\n\nDienes, Zoltan. 2008. Understanding Psychology as a Science: An\nIntroduction to Scientific and Statistical Inference. Springer.\n\n\nFisher, Ronald A. 1956. “Statistical Methods and Scientific\nInference.”\n\n\nFisher, Ronald Aylmer. 1936. “Design of Experiments.”\nBritish Medical Journal 1 (3923): 554.\n\n\nGossett, W. 1904. “The Application of the ‘Law of Error’to the\nWork of the Brewery.” Guinness Internal Note 36.\n\n\nHacking, Ian. 2016. Logic of Statistical Inference. Cambridge\nUniversity Press.\n\n\nHempel, Carl G. 1967. “Philosophy of Natural Science.”\nBritish Journal for the Philosophy of Science 18 (1).\n\n\nJohansson, Tobias. 2011. “Hail the Impossible: P-Values, Evidence,\nand Likelihood.” Scandinavian Journal of Psychology 52\n(2): 113–25.\n\n\nMiller, Jeff. 2009. “What Is the Probability of Replicating a\nStatistically Significant Effect?” Psychonomic Bulletin &\nReview 16: 617–40.\n\n\nNeyman, Jerzy. 1957. “\" Inductive Behavior\" as a Basic Concept of\nPhilosophy of Science.” Revue De L’Institut International De\nStatistique, 7–22.\n\n\nNeyman, Jerzy, and Egon Sharpe Pearson. 1933. “IX. On the Problem\nof the Most Efficient Tests of Statistical Hypotheses.”\nPhilosophical Transactions of the Royal Society of London. Series A,\nContaining Papers of a Mathematical or Physical Character 231\n(694-706): 289–337.\n\n\nNiiniluoto, Ilkka. 1998. “Verisimilitude: The Third\nPeriod.” The British Journal for the Philosophy of\nScience 49 (1): 1–29.\n\n\nPopper, Karl. 2005. The Logic of Scientific Discovery.\nRoutledge.\n\n\nSchweder, Tore, and Nils Lid Hjort. 2016. Confidence, Likelihood,\nProbability. Vol. 41. Cambridge University Press.\n\n\nSpanos, Aris. 2019. Probability Theory and Statistical Inference:\nEmpirical Modeling with Observational Data. Cambridge University\nPress.\n\n\nWilkinson, Leland. 2012. The Grammar of Graphics. Springer.\n\n\nZabell, Sandy L. 1992. “RA Fisher and Fiducial Argument.”\nStatistical Science, 369–87."
  },
  {
    "objectID": "rannoyances.html#sec-poordocs",
    "href": "rannoyances.html#sec-poordocs",
    "title": "Appendix B — A Running List of Annoying R Things",
    "section": "B.1 Consistently poor documentation",
    "text": "B.1 Consistently poor documentation\n\ne.g., CRAN reference manuals come in PDF form.\ne.g., CRAN reference manuals covers package functions/data in alphabetical order with no guidance about which functions might be more or less important/useful/central to the package.\ne.g., Documentation for the tidymodels package parsnip does not tell you what engines it supports (despite engines being the heart of what parsnip does). It does, however, tell you that you can call the function parsnip::show_engines() if you want to know that information. Of course, this requires you to first install the package."
  },
  {
    "objectID": "rannoyances.html#sec-dispatching",
    "href": "rannoyances.html#sec-dispatching",
    "title": "Appendix B — A Running List of Annoying R Things",
    "section": "B.2 Generic functions & dispatching",
    "text": "B.2 Generic functions & dispatching\n\nWhenever you call a R function, you never know what code you are about to invoke with certainty. Part of this ambiguity is intentional. R uses what I called “dispatching” to decide which of the many print() functions or which of the many describe() functions to actually execute based on the class of the first argument passed to the function. One of the major benefits of dispatching is that package authors may tweak standard functions (e.g., print()) to do special things for the objects their package instantiates and users never have to know that there are new functions available and never have to worry about which one(s) to use when.\nBut ignoring whose code you are invoking can also lead to problems. When I call the function filter() what can I expect to happen? It entirely depends what packages happen to be loaded at the time. Worse than this, it depends on the order in which those packages were loaded because R defaults to selecting functions from more-recently loaded packages.\nR knows that there are multiple functions with the name “filter” and even knows that there are multiple filter() functions that share a common first argument type. R could tell you when it detected such a scenario and warn you that it was making a choice and let you decide if R is making that choice the way you wish. But it doesn’t. It silently makes a selection and never lets you know that anything funny might be going on.\nPackages such as conflicted seek to address this, by allowing for greater control over how conflicts are resolved (e.g., specifying that you want filter() to invoke stats::filter() rather than the more-recently loaded dplyr::filter(). However, this places all responsibility for handling all relevant conflicts to the user. Furthermore, how these conflicts are resolved is still implicit when the functions are actually being called (my code still looks likes filter() and you need to go hunting to see if there’s a conflict and if/when/how I asked the conflict to be resolved)."
  },
  {
    "objectID": "rannoyances.html#inconsistent-acknowledgement-of-namespaces",
    "href": "rannoyances.html#inconsistent-acknowledgement-of-namespaces",
    "title": "Appendix B — A Running List of Annoying R Things",
    "section": "B.3 Inconsistent acknowledgement of namespaces",
    "text": "B.3 Inconsistent acknowledgement of namespaces\n\nOne way to solve the ambiguity described in Section B.2 is to be explicit when calling functions (e.g., dplyr::filter() instead of filter()). However, the complaint is that this causes your code to be “pretty busy”. Of course, there is a healthy conversation to be had about clean-and-mysterious vs. busy-by-transparent.\nBut ignoring the fact that functions are “attached” to specific packages is mirrored elsewhere. For example, there is a tendency to ignore the fact that data sets are also “attached” to specific packages. When one loads a package into the R environment, (e.g., library(fivethirtyeight)), a variety of data sets may become immediately available (e.g., drinks). I would encourage you to try and find evidence that the drinks dataframe is in any way associated with the fivethirtyeight package. RStudio will tell you that your environment is empty, which is a bit odd given that you now have access to a data frame that you didn’t when you first started up RStudio. If you check immediately before and after loading fivethirtyeight, you might see that drinks is available after, but not before, doing so. You might also see that the documentation for drinks (e.g., ?drinks) tells you that it is part of the fivethirtyeight package. But this is all a bit mysterious. When one runs a series of library() statements and a variety of new dataframes pop into existence, how does one determine where they all came from? You wade through lots of (poor) documentation. Or, in RStudio, you can step through the individual package environments, one by one."
  },
  {
    "objectID": "rannoyances.html#everything-is-a-package",
    "href": "rannoyances.html#everything-is-a-package",
    "title": "Appendix B — A Running List of Annoying R Things",
    "section": "B.4 Everything is a package",
    "text": "B.4 Everything is a package\n\nWhen you update R (not RStudio) it will remove all the packages you have installed. This is undesirable.\nIf you poke around for a solution, you may come upon the installr package. The fact that this is a package and not a feature of R itself is odd (why not contribute this code/functionality to the R project itself?).\nThe R ecosystem is extremely fragmented and weirdly territorial. Do we have an R package that provides generalized linear modeling functionality? No. We have 13 of them! Or 37 of them? It’s not clear. How is this sort of situation can be maintained as an equilibrium is unclear. Why is all this effort sunk into “one more GLM package” as opposed to adding features to existing packages? Are the maintainers not open to such additions? If so, why is everyone using their packages and not starting a more open, more contributor-friendly project?"
  },
  {
    "objectID": "rannoyances.html#how-does-r-work-lazy-evaluation",
    "href": "rannoyances.html#how-does-r-work-lazy-evaluation",
    "title": "Appendix B — A Running List of Annoying R Things",
    "section": "B.5 How does R work? & “lazy evaluation”",
    "text": "B.5 How does R work? & “lazy evaluation”\n\nR code such as mpg %&gt;% filter(model == \"mustang\") is not valid because model is not the name of an R variable or anything else that R might be aware of. To see this, type model at the console and you will get an Error: object 'model' not found message. But this code becomes valid once you load the tidyverse package (or dyplyr). How this happens it mysterious, but the short version is that R allows packages to decide and change how R itself works."
  }
]